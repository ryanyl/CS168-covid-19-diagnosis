{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"colab":{"name":"COVID_CT_Dense121.ipynb","provenance":[{"file_id":"1HEG8up-0EX_ar7GneN7sU97Q23uhQhXz","timestamp":1591962554003},{"file_id":"1zsDOvX0Fl7UjBZ9s5ElsE26t9aJVlF8v","timestamp":1590718183337}]},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"d28a480b64834b0eb8c65a9958e44d2c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_2b78ff04832640b89bff09fc64cb67ee","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_9e43fc920901471f94292011c46d096a","IPY_MODEL_f39265f81fb64aea8185a0236f72e2b7"]}},"2b78ff04832640b89bff09fc64cb67ee":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"9e43fc920901471f94292011c46d096a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_8c037a303d3d41c1934fb32a7110db81","_dom_classes":[],"description":"100%","_model_name":"FloatProgressModel","bar_style":"success","max":32342954,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":32342954,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_65ce4899d9c349e29345977bae6616da"}},"f39265f81fb64aea8185a0236f72e2b7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_456ad074240b41c1950e3012a242d82a","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 30.8M/30.8M [07:23&lt;00:00, 72.8kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_1fe0553c09ae4ac08a2cefaf1d2042ed"}},"8c037a303d3d41c1934fb32a7110db81":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"65ce4899d9c349e29345977bae6616da":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"456ad074240b41c1950e3012a242d82a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"1fe0553c09ae4ac08a2cefaf1d2042ed":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"0e7b22cf3fc34d1092e8a20ebc0bc8ed":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_451df4eb96dd489083ba443e64b933cf","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_6991ef1f02bc4b79b47e629d3b9a28d5","IPY_MODEL_a6a0df30003647cea57b3190bcd43f4e"]}},"451df4eb96dd489083ba443e64b933cf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"6991ef1f02bc4b79b47e629d3b9a28d5":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_0b4f1f52fd57424f876cacb7db27f9f3","_dom_classes":[],"description":"100%","_model_name":"FloatProgressModel","bar_style":"success","max":46827520,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":46827520,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_48e3b99c3aef4ef7b29e4287dc16e058"}},"a6a0df30003647cea57b3190bcd43f4e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_229ca44ab9f24ca9a2da5dab253d8c31","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 44.7M/44.7M [00:00&lt;00:00, 109MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_e5a5cf1c5a31480aba863ec793f9971c"}},"0b4f1f52fd57424f876cacb7db27f9f3":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"48e3b99c3aef4ef7b29e4287dc16e058":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"229ca44ab9f24ca9a2da5dab253d8c31":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"e5a5cf1c5a31480aba863ec793f9971c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"59d77db376c34e84b1bffe36191149b6":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_46917d7883ba4390b531c3a8ffff453c","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_6e7fe052e5df4bf18912b1b305cdecf1","IPY_MODEL_742cc3a9fda946299febe183933a2f4a"]}},"46917d7883ba4390b531c3a8ffff453c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"6e7fe052e5df4bf18912b1b305cdecf1":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_0f7bac4ee30740dda409d172604b4eba","_dom_classes":[],"description":"100%","_model_name":"FloatProgressModel","bar_style":"success","max":32342954,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":32342954,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_d89198a8366145b78e545d3ac2c3f9fd"}},"742cc3a9fda946299febe183933a2f4a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_3f941ce50e924625b28040298df258e1","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 30.8M/30.8M [00:00&lt;00:00, 92.9MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_d313dcefa17b45ed80df9519f2185a40"}},"0f7bac4ee30740dda409d172604b4eba":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"d89198a8366145b78e545d3ac2c3f9fd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"3f941ce50e924625b28040298df258e1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"d313dcefa17b45ed80df9519f2185a40":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"08a58f2836cc4e2fa9789fb7f192a568":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_85413331a5e742518f1f4b5fc88dd54c","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_130f7c0a8937453b84b7a65f38fc6c07","IPY_MODEL_5db21e4a23ad4309844a98e709897f91"]}},"85413331a5e742518f1f4b5fc88dd54c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"130f7c0a8937453b84b7a65f38fc6c07":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_688287011a914f978c33342774601f50","_dom_classes":[],"description":"100%","_model_name":"FloatProgressModel","bar_style":"success","max":81131730,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":81131730,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_f990d94a70d14276a3ead029d36029a0"}},"5db21e4a23ad4309844a98e709897f91":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_57ca52da09e8460b8eeebffecda7a024","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 77.4M/77.4M [00:01&lt;00:00, 43.3MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_b5540b7bb5834c6f956fba6888fb0d93"}},"688287011a914f978c33342774601f50":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"f990d94a70d14276a3ead029d36029a0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"57ca52da09e8460b8eeebffecda7a024":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"b5540b7bb5834c6f956fba6888fb0d93":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"621f13b4cd7a421b8e81e223ec942aa8":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_9fdc74bc2c094193ae44ddbe2a1eb334","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_157d3cced68341029cc9d2e23e37e01f","IPY_MODEL_a2d69a8b649d4943a9a4c3aaa6bcf2b3"]}},"9fdc74bc2c094193ae44ddbe2a1eb334":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"157d3cced68341029cc9d2e23e37e01f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_9a1f91f1d0fe4a758274a88c80e85093","_dom_classes":[],"description":"100%","_model_name":"FloatProgressModel","bar_style":"success","max":102502400,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":102502400,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_3aec6db1a8824126aa15cc91b49aa6ea"}},"a2d69a8b649d4943a9a4c3aaa6bcf2b3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_f006c0fb37a344ea94be2695bef43a87","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 97.8M/97.8M [01:24&lt;00:00, 1.21MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_9a2559f356e8449a9c096bf7dbbb4aa2"}},"9a1f91f1d0fe4a758274a88c80e85093":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"3aec6db1a8824126aa15cc91b49aa6ea":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"f006c0fb37a344ea94be2695bef43a87":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"9a2559f356e8449a9c096bf7dbbb4aa2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"845916cc78b7488b89c4b8c3306f5152":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_2f5b92f08064498bb3e6a4e9fcf53884","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_48896e214d0e4819bca8a4e3259e1e9f","IPY_MODEL_863bbb9069fa497caa28038813342aec"]}},"2f5b92f08064498bb3e6a4e9fcf53884":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"48896e214d0e4819bca8a4e3259e1e9f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_5225fec20ec444f1932a3b0708341ebd","_dom_classes":[],"description":"100%","_model_name":"FloatProgressModel","bar_style":"success","max":553433881,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":553433881,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_acf692d822664458ada09d7e40d7473a"}},"863bbb9069fa497caa28038813342aec":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_46d5aaf4147d45da97e5c33d2e6755cf","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 528M/528M [02:18&lt;00:00, 3.99MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_405d48cc97934647a01cf1485ac5ed68"}},"5225fec20ec444f1932a3b0708341ebd":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"acf692d822664458ada09d7e40d7473a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"46d5aaf4147d45da97e5c33d2e6755cf":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"405d48cc97934647a01cf1485ac5ed68":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"code","metadata":{"id":"VzD3AQrWHBaE","colab_type":"code","outputId":"a82318db-b5e7-4016-8cc8-249dd6d55023","executionInfo":{"status":"ok","timestamp":1592187760366,"user_tz":420,"elapsed":13406,"user":{"displayName":"HELEN HUANG","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgPwfdcR39k2-MsYXu8TmO-R41dE5DWynTaQ89_=s64","userId":"15309384205789813260"}},"colab":{"base_uri":"https://localhost:8080/","height":581}},"source":["import torch\n","import torchvision\n","import torchvision.transforms as transforms\n","import torchvision.datasets as datasets\n","import torch.nn.functional as F\n","import torch.nn as nn\n","import torch.optim as optim\n","import os\n","import matplotlib.pyplot as plt\n","from torch.optim.lr_scheduler import StepLR\n","import numpy as np\n","from datetime import datetime\n","import pandas as pd\n","import random \n","from torchvision.datasets import ImageFolder\n","import re\n","from torch.utils.data import Dataset, DataLoader\n","import matplotlib.pyplot as plt\n","from PIL import Image\n","from torch.optim.lr_scheduler import StepLR\n","from sklearn.metrics import roc_auc_score\n","from skimage.io import imread, imsave\n","import skimage\n","from PIL import ImageFile\n","from PIL import Image\n","!pip install torchxrayvision\n","import torchxrayvision as xrv\n","\n","torch.cuda.empty_cache()"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Collecting torchxrayvision\n","  Downloading https://files.pythonhosted.org/packages/4d/b7/d9206ee1863477a144629a41b5ae8172b100d60957002adf0e7538fecf20/torchxrayvision-0.0.9-py3-none-any.whl\n","Requirement already satisfied: numpy>=1 in /usr/local/lib/python3.6/dist-packages (from torchxrayvision) (1.18.5)\n","Requirement already satisfied: tqdm>=4 in /usr/local/lib/python3.6/dist-packages (from torchxrayvision) (4.41.1)\n","Requirement already satisfied: torchvision>=0.5 in /usr/local/lib/python3.6/dist-packages (from torchxrayvision) (0.6.0+cu101)\n","Requirement already satisfied: pandas>=1 in /usr/local/lib/python3.6/dist-packages (from torchxrayvision) (1.0.4)\n","Requirement already satisfied: requests>=1 in /usr/local/lib/python3.6/dist-packages (from torchxrayvision) (2.23.0)\n","Collecting pydicom>=1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d3/56/342e1f8ce5afe63bf65c23d0b2c1cd5a05600caad1c211c39725d3a4cc56/pydicom-2.0.0-py3-none-any.whl (35.4MB)\n","\u001b[K     |████████████████████████████████| 35.5MB 99kB/s \n","\u001b[?25hRequirement already satisfied: torch>=1 in /usr/local/lib/python3.6/dist-packages (from torchxrayvision) (1.5.0+cu101)\n","Requirement already satisfied: scikit-image>=0.16 in /usr/local/lib/python3.6/dist-packages (from torchxrayvision) (0.16.2)\n","Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision>=0.5->torchxrayvision) (7.0.0)\n","Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=1->torchxrayvision) (2018.9)\n","Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas>=1->torchxrayvision) (2.8.1)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=1->torchxrayvision) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=1->torchxrayvision) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=1->torchxrayvision) (2.9)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=1->torchxrayvision) (2020.4.5.1)\n","Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1->torchxrayvision) (0.16.0)\n","Requirement already satisfied: scipy>=0.19.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.16->torchxrayvision) (1.4.1)\n","Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.16->torchxrayvision) (1.1.1)\n","Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.16->torchxrayvision) (2.4.1)\n","Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.16->torchxrayvision) (2.4)\n","Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.16->torchxrayvision) (3.2.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.6.1->pandas>=1->torchxrayvision) (1.12.0)\n","Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx>=2.0->scikit-image>=0.16->torchxrayvision) (4.4.2)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16->torchxrayvision) (0.10.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16->torchxrayvision) (1.2.0)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16->torchxrayvision) (2.4.7)\n","Installing collected packages: pydicom, torchxrayvision\n","Successfully installed pydicom-2.0.0 torchxrayvision-0.0.9\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"9Z0XZzS5_C-w","colab_type":"code","outputId":"2b718bf8-bf66-4915-e9bb-9a678cf3bf11","executionInfo":{"status":"ok","timestamp":1592187790576,"user_tz":420,"elapsed":25016,"user":{"displayName":"HELEN HUANG","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgPwfdcR39k2-MsYXu8TmO-R41dE5DWynTaQ89_=s64","userId":"15309384205789813260"}},"colab":{"base_uri":"https://localhost:8080/","height":122}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"esY-0BBJAHvC","colab_type":"code","colab":{}},"source":["# link google drive\n","os.chdir('/content/drive/My Drive/CS 168')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_1RKZCMMHBaH","colab_type":"code","colab":{}},"source":["# normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","\n","########## Mean and std are calculated from the train dataset\n","normalize = transforms.Normalize(mean=[0.45271412, 0.45271412, 0.45271412],\n","                                     std=[0.33165374, 0.33165374, 0.33165374])\n","train_transformer = transforms.Compose([\n","    transforms.Resize(256),\n","    transforms.RandomResizedCrop((224),scale=(0.5,1.0)),\n","    transforms.RandomHorizontalFlip(),\n","#     transforms.RandomRotation(90),\n","    # random brightness and random contrast\n","    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n","    transforms.ToTensor(),\n","    normalize\n","])\n","\n","val_transformer = transforms.Compose([\n","#     transforms.Resize(224),\n","#     transforms.CenterCrop(224),\n","    transforms.Resize((224,224)),\n","    transforms.ToTensor(),\n","    normalize\n","])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"XXNu1YoWHBaN","colab_type":"code","outputId":"273ab062-f196-4501-d20c-0c02f3943d96","executionInfo":{"status":"ok","timestamp":1592187808580,"user_tz":420,"elapsed":4454,"user":{"displayName":"HELEN HUANG","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgPwfdcR39k2-MsYXu8TmO-R41dE5DWynTaQ89_=s64","userId":"15309384205789813260"}},"colab":{"base_uri":"https://localhost:8080/","height":68}},"source":["batchsize=4\n","def read_txt(txt_path):\n","    with open(txt_path) as f:\n","        lines = f.readlines()\n","    txt_data = [line.strip() for line in lines]\n","    return txt_data\n","\n","class CovidCTDataset(Dataset):\n","    def __init__(self, root_dir, txt_COVID, txt_NonCOVID, transform=None):\n","        \"\"\"\n","        Args:\n","            txt_path (string): Path to the txt file with annotations.\n","            root_dir (string): Directory with all the images.\n","            transform (callable, optional): Optional transform to be applied\n","                on a sample.\n","        File structure:\n","        - root_dir\n","            - CT_COVID\n","                - img1.png\n","                - img2.png\n","                - ......\n","            - CT_NonCOVID\n","                - img1.png\n","                - img2.png\n","                - ......\n","        \"\"\"\n","        self.root_dir = root_dir\n","        self.txt_path = [txt_COVID,txt_NonCOVID]\n","        self.classes = ['CT_COVID', 'CT_NonCOVID']\n","        self.num_cls = len(self.classes)\n","        self.img_list = []\n","        for c in range(self.num_cls):\n","            cls_list = [[os.path.join(self.root_dir,self.classes[c],item), c] for item in read_txt(self.txt_path[c])]\n","            self.img_list += cls_list\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.img_list)\n","\n","    def __getitem__(self, idx):\n","        if torch.is_tensor(idx):\n","            idx = idx.tolist()\n","\n","        img_path = self.img_list[idx][0]\n","        image = Image.open(img_path).convert('RGB')\n","\n","        if self.transform:\n","            image = self.transform(image)\n","        sample = {'img': image,\n","                  'label': int(self.img_list[idx][1])}\n","        return sample\n","\n","\n","\n","    \n","if __name__ == '__main__':\n","    trainset = CovidCTDataset(root_dir='/content/drive/My Drive/CS 168/Images-processed',\n","                       txt_COVID='/content/drive/My Drive/CS 168/Data-split/COVID/trainCT_COVID.txt',\n","                       txt_NonCOVID='/content/drive/My Drive/CS 168/Data-split/NonCOVID/trainCT_NonCOVID.txt',\n","                       transform= train_transformer)\n","    valset = CovidCTDataset(root_dir='/content/drive/My Drive/CS 168/Images-processed',\n","                          txt_COVID='/content/drive/My Drive/CS 168/Data-split/COVID/valCT_COVID.txt',\n","                          txt_NonCOVID='/content/drive/My Drive/CS 168/Data-split/NonCOVID/valCT_NonCOVID.txt',\n","                          transform= val_transformer)\n","    testset = CovidCTDataset(root_dir='/content/drive/My Drive/CS 168/Images-processed',\n","                            txt_COVID='/content/drive/My Drive/CS 168/Data-split/COVID/testCT_COVID.txt',\n","                            txt_NonCOVID='/content/drive/My Drive/CS 168/Data-split/NonCOVID/testCT_NonCOVID.txt',\n","                        transform= val_transformer)\n","    print(trainset.__len__())\n","    print(valset.__len__())\n","    print(testset.__len__())\n","\n","    train_loader = DataLoader(trainset, batch_size=batchsize, drop_last=False, shuffle=True)\n","    val_loader = DataLoader(valset, batch_size=batchsize, drop_last=False, shuffle=False)\n","    test_loader = DataLoader(testset, batch_size=batchsize, drop_last=False, shuffle=False)\n","    "],"execution_count":5,"outputs":[{"output_type":"stream","text":["425\n","118\n","203\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"d1-wYaTiHBaP","colab_type":"code","outputId":"c744ea7b-f7eb-483f-e90e-cf0a4153a4d3","executionInfo":{"status":"error","timestamp":1592114409131,"user_tz":420,"elapsed":167284,"user":{"displayName":"HELEN HUANG","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgPwfdcR39k2-MsYXu8TmO-R41dE5DWynTaQ89_=s64","userId":"15309384205789813260"}},"colab":{"base_uri":"https://localhost:8080/","height":341}},"source":["for batch_index, batch_samples in enumerate(train_loader):      \n","        data, target = batch_samples['img'], batch_samples['label']\n","skimage.io.imshow(data[0,1,:,:].numpy())"],"execution_count":6,"outputs":[{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-b54e60c82163>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_samples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m         \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_samples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'img'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_samples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mskimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    383\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    386\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-5-25328ab9e5eb>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mimg_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimg_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'RGB'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode)\u001b[0m\n\u001b[1;32m   2816\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2817\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2818\u001b[0;31m     \u001b[0mprefix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2819\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2820\u001b[0m     \u001b[0mpreinit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"id":"S7RtFdx9HBaT","colab_type":"code","colab":{}},"source":["#training process\n","\n","alpha = None\n","## alpha is None if mixup is not used\n","alpha_name = f'{alpha}'\n","device = 'cuda'\n","\n","def train(optimizer, epoch):\n","    \n","    model.train()\n","    \n","    train_loss = 0\n","    train_correct = 0\n","    \n","    for batch_index, batch_samples in enumerate(train_loader):\n","        \n","        # move data to device\n","        data, target = batch_samples['img'].to(device), batch_samples['label'].to(device)\n","        \n","        ## adjust data to meet the input dimension of model\n","#         data = data[:, 0, :, :]\n","#         data = data[:, None, :, :]    \n","        \n","        #mixup\n","#         data, targets_a, targets_b, lam = mixup_data(data, target, alpha, use_cuda=True)\n","        \n","        \n","        optimizer.zero_grad()\n","        output = model(data)\n","        criteria = nn.CrossEntropyLoss()\n","        loss = criteria(output, target.long())\n","        \n","        #mixup loss\n","#         loss = mixup_criterion(criteria, output, targets_a, targets_b, lam)\n","\n","        train_loss += criteria(output, target.long())\n","        \n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","        \n","        pred = output.argmax(dim=1, keepdim=True)\n","        train_correct += pred.eq(target.long().view_as(pred)).sum().item()\n","    \n","        # Display progress and write to tensorboard\n","        if batch_index % bs == 0:\n","            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tTrain Loss: {:.6f}'.format(\n","                epoch, batch_index, len(train_loader),\n","                100.0 * batch_index / len(train_loader), loss.item()/ bs))\n","    \n","#     print('\\nTrain set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n","#         train_loss/len(train_loader.dataset), train_correct, len(train_loader.dataset),\n","#         100.0 * train_correct / len(train_loader.dataset)))\n","#     f = open('model_result/{}.txt'.format(modelname), 'a+')\n","#     f.write('\\nTrain set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n","#         train_loss/len(train_loader.dataset), train_correct, len(train_loader.dataset),\n","#         100.0 * train_correct / len(train_loader.dataset)))\n","#     f.write('\\n')\n","#     f.close()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"DcaAaYnqHBaW","colab_type":"code","colab":{}},"source":["#validation process\n","\n","def val(epoch):\n","    \n","    model.eval()\n","    test_loss = 0\n","    correct = 0\n","    results = []\n","    \n","    TP = 0\n","    TN = 0\n","    FN = 0\n","    FP = 0\n","    \n","    \n","    criteria = nn.CrossEntropyLoss()\n","    # Don't update model\n","    with torch.no_grad():\n","        tpr_list = []\n","        fpr_list = []\n","        \n","        predlist=[]\n","        scorelist=[]\n","        targetlist=[]\n","        # Predict\n","        for batch_index, batch_samples in enumerate(val_loader):\n","            data, target = batch_samples['img'].to(device), batch_samples['label'].to(device)\n","            \n","#             data = data[:, 0, :, :]\n","#             data = data[:, None, :, :]\n","            output = model(data)\n","            \n","            test_loss += criteria(output, target.long())\n","            score = F.softmax(output, dim=1)\n","            pred = output.argmax(dim=1, keepdim=True)\n","#             print('target',target.long()[:, 2].view_as(pred))\n","            correct += pred.eq(target.long().view_as(pred)).sum().item()\n","            \n","#             print(output[:,1].cpu().numpy())\n","#             print((output[:,1]+output[:,0]).cpu().numpy())\n","#             predcpu=(output[:,1].cpu().numpy())/((output[:,1]+output[:,0]).cpu().numpy())\n","            targetcpu=target.long().cpu().numpy()\n","            predlist=np.append(predlist, pred.cpu().numpy())\n","            scorelist=np.append(scorelist, score.cpu().numpy()[:,1])\n","            targetlist=np.append(targetlist,targetcpu)\n","           \n","    return targetlist, scorelist, predlist\n","    \n","    # Write to tensorboard\n","#     writer.add_scalar('Test Accuracy', 100.0 * correct / len(test_loader.dataset), epoch)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"GqNAbGywHBaY","colab_type":"code","colab":{}},"source":["#testing process\n","\n","def test(epoch):\n","    \n","    model.eval()\n","    test_loss = 0\n","    correct = 0\n","    results = []\n","    \n","    TP = 0\n","    TN = 0\n","    FN = 0\n","    FP = 0\n","    \n","    \n","    criteria = nn.CrossEntropyLoss()\n","    # Don't update model\n","    with torch.no_grad():\n","        tpr_list = []\n","        fpr_list = []\n","        \n","        predlist=[]\n","        scorelist=[]\n","        targetlist=[]\n","        # Predict\n","        for batch_index, batch_samples in enumerate(test_loader):\n","            data, target = batch_samples['img'].to(device), batch_samples['label'].to(device)\n","#             data = data[:, 0, :, :]\n","#             data = data[:, None, :, :]\n","#             print(target)\n","            output = model(data)\n","            \n","            test_loss += criteria(output, target.long())\n","            score = F.softmax(output, dim=1)\n","            pred = output.argmax(dim=1, keepdim=True)\n","#             print('target',target.long()[:, 2].view_as(pred))\n","            correct += pred.eq(target.long().view_as(pred)).sum().item()\n","#             TP += ((pred == 1) & (target.long()[:, 2].view_as(pred).data == 1)).cpu().sum()\n","#             TN += ((pred == 0) & (target.long()[:, 2].view_as(pred) == 0)).cpu().sum()\n","# #             # FN    predict 0 label 1\n","#             FN += ((pred == 0) & (target.long()[:, 2].view_as(pred) == 1)).cpu().sum()\n","# #             # FP    predict 1 label 0\n","#             FP += ((pred == 1) & (target.long()[:, 2].view_as(pred) == 0)).cpu().sum()\n","#             print(TP,TN,FN,FP)\n","            \n","            \n","#             print(output[:,1].cpu().numpy())\n","#             print((output[:,1]+output[:,0]).cpu().numpy())\n","#             predcpu=(output[:,1].cpu().numpy())/((output[:,1]+output[:,0]).cpu().numpy())\n","            targetcpu=target.long().cpu().numpy()\n","            predlist=np.append(predlist, pred.cpu().numpy())\n","            scorelist=np.append(scorelist, score.cpu().numpy()[:,1])\n","            targetlist=np.append(targetlist,targetcpu)\n","    return targetlist, scorelist, predlist\n","    \n","    # Write to tensorboard\n","#     writer.add_scalar('Test Accuracy', 100.0 * correct / len(test_loader.dataset), epoch)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"mautCEGZHBaa","colab_type":"code","outputId":"673f6623-d5c9-4c49-f000-b7a32be367a4","executionInfo":{"status":"ok","timestamp":1591958263429,"user_tz":420,"elapsed":10175,"user":{"displayName":"HELEN HUANG","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgPwfdcR39k2-MsYXu8TmO-R41dE5DWynTaQ89_=s64","userId":"15309384205789813260"}},"colab":{"base_uri":"https://localhost:8080/","height":100,"referenced_widgets":["d28a480b64834b0eb8c65a9958e44d2c","2b78ff04832640b89bff09fc64cb67ee","9e43fc920901471f94292011c46d096a","f39265f81fb64aea8185a0236f72e2b7","8c037a303d3d41c1934fb32a7110db81","65ce4899d9c349e29345977bae6616da","456ad074240b41c1950e3012a242d82a","1fe0553c09ae4ac08a2cefaf1d2042ed"]}},"source":["'''CheXNet pretrained model'''\n","# DenseNet 121 (121 convolution layers)\n","\n","class DenseNet121(nn.Module):\n","    \"\"\"Model modified.\n","\n","    The architecture of our model is the same as standard DenseNet121\n","    except the classifier layer which has an additional sigmoid function.\n","\n","    \"\"\"\n","    def __init__(self, out_size):\n","        super(DenseNet121, self).__init__()\n","        self.densenet121 = torchvision.models.densenet121(pretrained=True)\n","        num_ftrs = self.densenet121.classifier.in_features\n","        self.densenet121.classifier = nn.Sequential(\n","            nn.Linear(num_ftrs, out_size),\n","            nn.Sigmoid()\n","        )\n","\n","    def forward(self, x):\n","        x = self.densenet121(x)\n","        return x\n","  \n","\n","device = 'cuda'\n","CKPT_PATH = 'model.pth.tar'\n","N_CLASSES = 14\n","\n","DenseNet121 = DenseNet121(N_CLASSES).cuda()\n","\n","CKPT_PATH = './CheXNet/model.pth.tar'\n","\n","if os.path.isfile(CKPT_PATH):\n","    checkpoint = torch.load(CKPT_PATH)        \n","    state_dict = checkpoint['state_dict']\n","    remove_data_parallel = False\n","\n","\n","    pattern = re.compile(\n","        r'^(.*denselayer\\d+\\.(?:norm|relu|conv))\\.((?:[12])\\.(?:weight|bias|running_mean|running_var))$')\n","    for key in list(state_dict.keys()):\n","        match = pattern.match(key)\n","        new_key = match.group(1) + match.group(2) if match else key\n","        new_key = new_key[7:] if remove_data_parallel else new_key\n","        new_key = new_key[7:]\n","        state_dict[new_key] = state_dict[key]\n","        del state_dict[key]\n","\n","\n","    DenseNet121.load_state_dict(checkpoint['state_dict'])\n","    print(\"=> loaded checkpoint\")\n","#     print(densenet121)\n","else:\n","    print(\"=> no checkpoint found\")\n","\n","# for parma in DenseNet121.parameters():\n","#         parma.requires_grad = False\n","DenseNet121.densenet121.classifier._modules['0'] = nn.Linear(in_features=1024, out_features=2, bias=True)\n","DenseNet121.densenet121.features.conv0 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n","# print(DenseNet121)\n","model = DenseNet121.to(device)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Downloading: \"https://download.pytorch.org/models/densenet121-a639ec97.pth\" to /root/.cache/torch/checkpoints/densenet121-a639ec97.pth\n"],"name":"stderr"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d28a480b64834b0eb8c65a9958e44d2c","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=32342954.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n","=> no checkpoint found\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"__JxODKJHBac","colab_type":"code","colab":{}},"source":["'''DenseNet121 pretrained model from xrv'''\n","\n","class DenseNetModel(nn.Module):\n","\n","    def __init__(self):\n","        \"\"\"\n","        Pass in parsed HyperOptArgumentParser to the model\n","        :param hparams:\n","        \"\"\"\n","        super(DenseNetModel, self).__init__()\n","\n","        self.dense_net = xrv.models.DenseNet(num_classes=2)\n","        self.criterion = nn.CrossEntropyLoss()\n","\n","    def forward(self, x):\n","        logits = self.dense_net(x)\n","        return logits\n","    \n","model = DenseNetModel().cuda()\n","modelname = 'DenseNet_medical'\n","# print(model)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"QsbK26ZIJGfh","colab_type":"code","colab":{}},"source":["# SimpleCNN\n","class SimpleCNN(torch.nn.Module):\n","    def __init__(self):\n","        super(SimpleCNN, self).__init__() # b, 3, 32, 32\n","        layer1 = torch.nn.Sequential() \n","        layer1.add_module('conv1', torch.nn.Conv2d(3, 32, 3, 1, padding=1))\n"," \n","        #b, 32, 32, 32\n","        layer1.add_module('relu1', torch.nn.ReLU(True)) \n","        layer1.add_module('pool1', torch.nn.MaxPool2d(2, 2)) # b, 32, 16, 16\n","        self.layer1 = layer1\n","        layer4 = torch.nn.Sequential()\n","        layer4.add_module('fc1', torch.nn.Linear(401408, 2))       \n","        self.layer4 = layer4\n"," \n","    def forward(self, x):\n","        conv1 = self.layer1(x)\n","        fc_input = conv1.view(conv1.size(0), -1)\n","        fc_out = self.layer4(fc_input)\n"," \n","model = SimpleCNN().cuda()\n","modelname = 'SimpleCNN'"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"clDwtJpoHBae","colab_type":"code","outputId":"4cb58955-1921-463f-a8d5-afa96e25db59","executionInfo":{"status":"ok","timestamp":1591958713496,"user_tz":420,"elapsed":1139,"user":{"displayName":"HELEN HUANG","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgPwfdcR39k2-MsYXu8TmO-R41dE5DWynTaQ89_=s64","userId":"15309384205789813260"}},"colab":{"base_uri":"https://localhost:8080/","height":83,"referenced_widgets":["0e7b22cf3fc34d1092e8a20ebc0bc8ed","451df4eb96dd489083ba443e64b933cf","6991ef1f02bc4b79b47e629d3b9a28d5","a6a0df30003647cea57b3190bcd43f4e","0b4f1f52fd57424f876cacb7db27f9f3","48e3b99c3aef4ef7b29e4287dc16e058","229ca44ab9f24ca9a2da5dab253d8c31","e5a5cf1c5a31480aba863ec793f9971c"]}},"source":["# ResNet18\n","import torchvision.models as models\n","model = models.resnet18(pretrained=True).cuda()\n","modelname = 'ResNet18'"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Downloading: \"https://download.pytorch.org/models/resnet18-5c106cde.pth\" to /root/.cache/torch/checkpoints/resnet18-5c106cde.pth\n"],"name":"stderr"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0e7b22cf3fc34d1092e8a20ebc0bc8ed","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=46827520.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"OAtgrbrEHBag","colab_type":"code","outputId":"d4beec5f-9b7c-4608-c213-75956ab22bdf","executionInfo":{"status":"ok","timestamp":1592114451458,"user_tz":420,"elapsed":11217,"user":{"displayName":"HELEN HUANG","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgPwfdcR39k2-MsYXu8TmO-R41dE5DWynTaQ89_=s64","userId":"15309384205789813260"}},"colab":{"base_uri":"https://localhost:8080/","height":83,"referenced_widgets":["59d77db376c34e84b1bffe36191149b6","46917d7883ba4390b531c3a8ffff453c","6e7fe052e5df4bf18912b1b305cdecf1","742cc3a9fda946299febe183933a2f4a","0f7bac4ee30740dda409d172604b4eba","d89198a8366145b78e545d3ac2c3f9fd","3f941ce50e924625b28040298df258e1","d313dcefa17b45ed80df9519f2185a40"]}},"source":["# Dense121\n","import torchvision.models as models\n","model = models.densenet121(pretrained=True).cuda()\n","modelname = 'Dense121'"],"execution_count":10,"outputs":[{"output_type":"stream","text":["Downloading: \"https://download.pytorch.org/models/densenet121-a639ec97.pth\" to /root/.cache/torch/checkpoints/densenet121-a639ec97.pth\n"],"name":"stderr"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"59d77db376c34e84b1bffe36191149b6","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=32342954.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"B0O6iB0rj758","colab_type":"code","outputId":"14b516be-7c7b-415e-fb84-53092b777cb2","executionInfo":{"status":"ok","timestamp":1592187857431,"user_tz":420,"elapsed":13499,"user":{"displayName":"HELEN HUANG","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgPwfdcR39k2-MsYXu8TmO-R41dE5DWynTaQ89_=s64","userId":"15309384205789813260"}},"colab":{"base_uri":"https://localhost:8080/","height":83,"referenced_widgets":["08a58f2836cc4e2fa9789fb7f192a568","85413331a5e742518f1f4b5fc88dd54c","130f7c0a8937453b84b7a65f38fc6c07","5db21e4a23ad4309844a98e709897f91","688287011a914f978c33342774601f50","f990d94a70d14276a3ead029d36029a0","57ca52da09e8460b8eeebffecda7a024","b5540b7bb5834c6f956fba6888fb0d93"]}},"source":["# Dense201\n","import torchvision.models as models\n","model = models.densenet201(pretrained=True).cuda()\n","modelname = 'Dense201'"],"execution_count":9,"outputs":[{"output_type":"stream","text":["Downloading: \"https://download.pytorch.org/models/densenet201-c1103571.pth\" to /root/.cache/torch/checkpoints/densenet201-c1103571.pth\n"],"name":"stderr"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"08a58f2836cc4e2fa9789fb7f192a568","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=81131730.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Cl7smHJIHBai","colab_type":"code","colab":{}},"source":["# Dense169\n","import torchvision.models as models\n","model = models.densenet169(pretrained=True).cuda()\n","modelname = 'Dense169'"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"8gT24ULEHBak","colab_type":"code","colab":{}},"source":["\"\"\"Load Self-Trans model\"\"\"\n","\"\"\"Change names and locations to the Self-Trans.pt\"\"\"\n","\n","\n","model = models.densenet169(pretrained=True).cuda()\n","# pretrained_net = torch.load('model_backup/Dense169.pt')\n","# pretrained_net = torch.load('model_backup/mixup/Dense169_0.6.pt')\n","pretrained_net = torch.load('model_backup/medical_transfer/medical_transfer_None_LUNA_moco_covid_moco.pt')\n","\n","\n","model.load_state_dict(pretrained_net)\n","\n","modelname = 'Dense169_ssl_luna_moco'"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"YsUhgz7zHBam","colab_type":"code","outputId":"8c191aa6-9f2b-4566-9889-8c37ada1598b","executionInfo":{"status":"ok","timestamp":1591959211603,"user_tz":420,"elapsed":2350,"user":{"displayName":"HELEN HUANG","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgPwfdcR39k2-MsYXu8TmO-R41dE5DWynTaQ89_=s64","userId":"15309384205789813260"}},"colab":{"base_uri":"https://localhost:8080/","height":83,"referenced_widgets":["621f13b4cd7a421b8e81e223ec942aa8","9fdc74bc2c094193ae44ddbe2a1eb334","157d3cced68341029cc9d2e23e37e01f","a2d69a8b649d4943a9a4c3aaa6bcf2b3","9a1f91f1d0fe4a758274a88c80e85093","3aec6db1a8824126aa15cc91b49aa6ea","f006c0fb37a344ea94be2695bef43a87","9a2559f356e8449a9c096bf7dbbb4aa2"]}},"source":["# ResNet50\n","import torchvision.models as models\n","model = models.resnet50(pretrained=True).cuda()\n","modelname = 'ResNet50'"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Downloading: \"https://download.pytorch.org/models/resnet50-19c8e357.pth\" to /root/.cache/torch/checkpoints/resnet50-19c8e357.pth\n"],"name":"stderr"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"621f13b4cd7a421b8e81e223ec942aa8","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=102502400.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"taCX0AkDHBao","colab_type":"code","outputId":"90cdb417-6144-4ea2-d507-9145789fbdf4","executionInfo":{"status":"ok","timestamp":1591958982200,"user_tz":420,"elapsed":6129,"user":{"displayName":"HELEN HUANG","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgPwfdcR39k2-MsYXu8TmO-R41dE5DWynTaQ89_=s64","userId":"15309384205789813260"}},"colab":{"base_uri":"https://localhost:8080/","height":83,"referenced_widgets":["845916cc78b7488b89c4b8c3306f5152","2f5b92f08064498bb3e6a4e9fcf53884","48896e214d0e4819bca8a4e3259e1e9f","863bbb9069fa497caa28038813342aec","5225fec20ec444f1932a3b0708341ebd","acf692d822664458ada09d7e40d7473a","46d5aaf4147d45da97e5c33d2e6755cf","405d48cc97934647a01cf1485ac5ed68"]}},"source":["#VGGNet\n","\n","import torchvision.models as models\n","model = models.vgg16(pretrained=True)\n","model = model.cuda()\n","modelname = 'vgg16'"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/checkpoints/vgg16-397923af.pth\n"],"name":"stderr"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"845916cc78b7488b89c4b8c3306f5152","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=553433881.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"gs_xRImSHBaq","colab_type":"code","outputId":"9bd71955-52c4-4f6c-e6fa-253b918a3088","executionInfo":{"status":"ok","timestamp":1591959142214,"user_tz":420,"elapsed":1212,"user":{"displayName":"HELEN HUANG","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgPwfdcR39k2-MsYXu8TmO-R41dE5DWynTaQ89_=s64","userId":"15309384205789813260"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["#efficientNet\n","\n","#!pip install efficientnet_pytorch\n","from efficientnet_pytorch import EfficientNet\n","model = EfficientNet.from_pretrained('efficientnet-b0', num_classes=2)\n","model = model.cuda()\n","modelname = 'efficientNet-b0'\n","\n","\n","model = EfficientNet.from_name('efficientnet-b1').cuda()\n","modelname = 'efficientNet_random'"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Loaded pretrained weights for efficientnet-b0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"e6yPEmWRHBas","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZA7lQf5XHBau","colab_type":"code","outputId":"a769f1cd-50fd-4a72-c7d3-ac960fd4e4d9","executionInfo":{"status":"ok","timestamp":1592190714059,"user_tz":420,"elapsed":2622670,"user":{"displayName":"HELEN HUANG","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgPwfdcR39k2-MsYXu8TmO-R41dE5DWynTaQ89_=s64","userId":"15309384205789813260"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["# train\n","bs =batchsize\n","votenum = 10\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","r_list = []\n","p_list = []\n","acc_list = []\n","AUC_list = []\n","# TP = 0\n","# TN = 0\n","# FN = 0\n","# FP = 0\n","vote_pred = np.zeros(valset.__len__())\n","vote_score = np.zeros(valset.__len__())\n","\n","#optimizer = optim.SGD(model.parameters(), lr=0.001, momentum = 0.9)\n","optimizer = optim.Adam(model.parameters(), lr=0.0001)\n","scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)\n","\n","# scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma = 0.95)\n","                                             \n","scheduler = StepLR(optimizer, step_size=1)\n","\n","total_epoch = 100\n","for epoch in range(1, total_epoch+1):\n","    train(optimizer, epoch)\n","    \n","    targetlist, scorelist, predlist = val(epoch)\n","    print('target',targetlist)\n","    print('score',scorelist)\n","    print('predict',predlist)\n","    vote_pred = vote_pred + predlist \n","    vote_score = vote_score + scorelist \n","\n","    if epoch % votenum == 0:\n","        \n","        # major vote\n","        vote_pred[vote_pred <= (votenum/2)] = 0\n","        vote_pred[vote_pred > (votenum/2)] = 1\n","        vote_score = vote_score/votenum\n","        \n","        print('vote_pred', vote_pred)\n","        print('targetlist', targetlist)\n","        TP = ((vote_pred == 1) & (targetlist == 1)).sum()\n","        TN = ((vote_pred == 0) & (targetlist == 0)).sum()\n","        FN = ((vote_pred == 0) & (targetlist == 1)).sum()\n","        FP = ((vote_pred == 1) & (targetlist == 0)).sum()\n","        \n","        \n","        print('TP=',TP,'TN=',TN,'FN=',FN,'FP=',FP)\n","        print('TP+FP',TP+FP)\n","        p = TP / (TP + FP)\n","        print('precision',p)\n","        p = TP / (TP + FP)\n","        r = TP / (TP + FN)\n","        print('recall',r)\n","        F1 = 2 * r * p / (r + p)\n","        acc = (TP + TN) / (TP + TN + FP + FN)\n","        print('F1',F1)\n","        print('acc',acc)\n","        AUC = roc_auc_score(targetlist, vote_score)\n","        print('AUCp', roc_auc_score(targetlist, vote_pred))\n","        print('AUC', AUC)\n","        \n","        \n","        \n","#         if epoch == total_epoch:\n","        torch.save(model.state_dict(), \"/content/drive/My Drive/CS 168/Models/{}_{}_covid_moco_covid.pt\".format(modelname,alpha_name))  \n","        \n","        vote_pred = np.zeros(valset.__len__())\n","        vote_score = np.zeros(valset.__len__())\n","        print('\\n The epoch is {}, average recall: {:.4f}, average precision: {:.4f},\\\n","average F1: {:.4f}, average accuracy: {:.4f}, average AUC: {:.4f}'.format(\n","        epoch, r, p, F1, acc, AUC))\n","\n","#         f = open('model_result/medical_transfer/{}_{}.txt'.format(modelname,alpha_name), 'a+')\n","#         f.write('\\n The epoch is {}, average recall: {:.4f}, average precision: {:.4f},\\\n","# average F1: {:.4f}, average accuracy: {:.4f}, average AUC: {:.4f}'.format(\n","#         epoch, r, p, F1, acc, AUC))\n","#         f.close()\n","\n"],"execution_count":10,"outputs":[{"output_type":"stream","text":["\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n"," 9.96127307e-01 9.88825619e-01 9.98147249e-01 9.10025239e-01\n"," 8.60054433e-01 9.95970011e-01 9.88810122e-01 9.93268549e-01\n"," 1.44339100e-01 3.55993658e-01 8.96008313e-01 1.69020638e-01\n"," 1.67402327e-02 9.75422740e-01 8.03951442e-01 9.06581342e-01\n"," 1.79175049e-01 2.77361006e-01 8.46093148e-02 3.14629138e-01\n"," 9.11098998e-03 9.04003084e-01 1.30153438e-02 4.28071469e-01\n"," 1.15245894e-01 3.15236151e-01 2.87922591e-01 5.72089921e-04\n"," 4.70685191e-04 2.67519522e-03 5.89728914e-02 1.59478659e-05\n"," 7.81458616e-01 7.19166517e-01 1.02912657e-01 2.75559068e-01\n"," 8.75182927e-01 9.21065629e-01]\n","predict [0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.\n"," 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 0. 0. 1. 1. 1.\n"," 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 1.]\n","Train Epoch: 29 [0/107 (0%)]\tTrain Loss: 0.003912\n","Train Epoch: 29 [4/107 (4%)]\tTrain Loss: 0.006659\n","Train Epoch: 29 [8/107 (7%)]\tTrain Loss: 0.004500\n","Train Epoch: 29 [12/107 (11%)]\tTrain Loss: 0.017859\n","Train Epoch: 29 [16/107 (15%)]\tTrain Loss: 0.096534\n","Train Epoch: 29 [20/107 (19%)]\tTrain Loss: 0.000596\n","Train Epoch: 29 [24/107 (22%)]\tTrain Loss: 0.007319\n","Train Epoch: 29 [28/107 (26%)]\tTrain Loss: 0.101095\n","Train Epoch: 29 [32/107 (30%)]\tTrain Loss: 0.007271\n","Train Epoch: 29 [36/107 (34%)]\tTrain Loss: 0.006573\n","Train Epoch: 29 [40/107 (37%)]\tTrain Loss: 0.000421\n","Train Epoch: 29 [44/107 (41%)]\tTrain Loss: 0.049543\n","Train Epoch: 29 [48/107 (45%)]\tTrain Loss: 0.047920\n","Train Epoch: 29 [52/107 (49%)]\tTrain Loss: 0.005707\n","Train Epoch: 29 [56/107 (52%)]\tTrain Loss: 0.000965\n","Train Epoch: 29 [60/107 (56%)]\tTrain Loss: 0.218840\n","Train Epoch: 29 [64/107 (60%)]\tTrain Loss: 0.005708\n","Train Epoch: 29 [68/107 (64%)]\tTrain Loss: 0.331327\n","Train Epoch: 29 [72/107 (67%)]\tTrain Loss: 0.018438\n","Train Epoch: 29 [76/107 (71%)]\tTrain Loss: 0.000751\n","Train Epoch: 29 [80/107 (75%)]\tTrain Loss: 0.051697\n","Train Epoch: 29 [84/107 (79%)]\tTrain Loss: 0.056551\n","Train Epoch: 29 [88/107 (82%)]\tTrain Loss: 0.007932\n","Train Epoch: 29 [92/107 (86%)]\tTrain Loss: 0.051836\n","Train Epoch: 29 [96/107 (90%)]\tTrain Loss: 0.052511\n","Train Epoch: 29 [100/107 (93%)]\tTrain Loss: 0.176347\n","Train Epoch: 29 [104/107 (97%)]\tTrain Loss: 0.052641\n","target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n","score [1.70271382e-01 9.28491235e-01 1.81145951e-01 7.20896542e-01\n"," 3.63594770e-01 2.80829817e-01 9.58556414e-01 4.46722806e-01\n"," 5.65292947e-02 5.90504289e-01 9.18186009e-01 8.12176391e-02\n"," 8.71753633e-01 2.73389164e-02 4.18602638e-02 4.25844075e-04\n"," 5.22020943e-02 5.95578812e-02 5.27095050e-03 1.18126418e-03\n"," 2.75760919e-01 2.61845887e-01 4.52214450e-01 9.85589564e-01\n"," 3.05940453e-02 5.66034079e-01 9.86443520e-01 1.92691325e-04\n"," 2.24322612e-05 1.00602258e-04 8.42208043e-03 3.99760553e-04\n"," 7.23630842e-03 1.85659371e-06 5.69014674e-07 1.12497808e-04\n"," 5.13353152e-03 2.26234764e-01 7.88007677e-03 1.96010151e-04\n"," 2.19313777e-04 7.77438472e-05 1.38343811e-01 1.59721691e-02\n"," 1.20354600e-01 2.32221540e-02 8.06288958e-01 6.17002845e-01\n"," 7.25155771e-01 7.90855765e-01 9.81786788e-01 4.51640631e-07\n"," 8.97215887e-06 2.46793590e-03 8.20250712e-09 4.06796607e-05\n"," 2.18587425e-02 2.03664988e-10 4.59806070e-05 1.77323632e-02\n"," 9.97998416e-01 9.97901201e-01 9.98766303e-01 9.83319461e-01\n"," 9.92039919e-01 8.43909800e-01 8.09724510e-01 9.98775184e-01\n"," 9.97984648e-01 9.01880682e-01 9.43528593e-01 2.40807608e-01\n"," 9.38309491e-01 5.59307098e-01 1.52178288e-01 4.77341682e-01\n"," 9.97358978e-01 9.88067269e-01 9.99329090e-01 9.95997906e-01\n"," 9.99394774e-01 9.96094286e-01 9.99460161e-01 8.83408129e-01\n"," 9.30159092e-01 9.86964166e-01 9.08495426e-01 9.35904443e-01\n"," 2.97044292e-02 6.92605436e-01 9.76505816e-01 8.59896779e-01\n"," 1.29978582e-01 9.98249590e-01 9.84400153e-01 9.93309259e-01\n"," 2.29027241e-01 6.25443935e-01 5.78925729e-01 9.66033399e-01\n"," 5.70980320e-03 7.78079510e-01 9.65165626e-03 7.39948869e-01\n"," 6.38322473e-01 9.23331320e-01 8.33575249e-01 1.26179380e-04\n"," 3.10567528e-04 3.31935036e-04 3.34475678e-03 1.73629349e-04\n"," 8.84286046e-01 9.24962223e-01 4.01509941e-01 2.05135599e-01\n"," 9.84201014e-01 9.14435804e-01]\n","predict [0. 1. 0. 1. 0. 0. 1. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n"," 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.\n"," 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0.\n"," 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1.\n"," 0. 1. 1. 1. 0. 1. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 1.]\n","Train Epoch: 30 [0/107 (0%)]\tTrain Loss: 0.026638\n","Train Epoch: 30 [4/107 (4%)]\tTrain Loss: 0.002662\n","Train Epoch: 30 [8/107 (7%)]\tTrain Loss: 0.006755\n","Train Epoch: 30 [12/107 (11%)]\tTrain Loss: 0.001269\n","Train Epoch: 30 [16/107 (15%)]\tTrain Loss: 0.081401\n","Train Epoch: 30 [20/107 (19%)]\tTrain Loss: 0.011497\n","Train Epoch: 30 [24/107 (22%)]\tTrain Loss: 0.129293\n","Train Epoch: 30 [28/107 (26%)]\tTrain Loss: 0.001431\n","Train Epoch: 30 [32/107 (30%)]\tTrain Loss: 0.005731\n","Train Epoch: 30 [36/107 (34%)]\tTrain Loss: 0.128242\n","Train Epoch: 30 [40/107 (37%)]\tTrain Loss: 0.005178\n","Train Epoch: 30 [44/107 (41%)]\tTrain Loss: 0.010996\n","Train Epoch: 30 [48/107 (45%)]\tTrain Loss: 0.000671\n","Train Epoch: 30 [52/107 (49%)]\tTrain Loss: 0.009587\n","Train Epoch: 30 [56/107 (52%)]\tTrain Loss: 0.000326\n","Train Epoch: 30 [60/107 (56%)]\tTrain Loss: 0.000578\n","Train Epoch: 30 [64/107 (60%)]\tTrain Loss: 0.004311\n","Train Epoch: 30 [68/107 (64%)]\tTrain Loss: 0.004019\n","Train Epoch: 30 [72/107 (67%)]\tTrain Loss: 0.004665\n","Train Epoch: 30 [76/107 (71%)]\tTrain Loss: 0.000790\n","Train Epoch: 30 [80/107 (75%)]\tTrain Loss: 0.003840\n","Train Epoch: 30 [84/107 (79%)]\tTrain Loss: 0.016710\n","Train Epoch: 30 [88/107 (82%)]\tTrain Loss: 0.024730\n","Train Epoch: 30 [92/107 (86%)]\tTrain Loss: 0.033128\n","Train Epoch: 30 [96/107 (90%)]\tTrain Loss: 0.005788\n","Train Epoch: 30 [100/107 (93%)]\tTrain Loss: 0.007130\n","Train Epoch: 30 [104/107 (97%)]\tTrain Loss: 0.047405\n","target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n","score [4.21814583e-02 8.02128613e-01 1.52687207e-01 8.62992287e-01\n"," 9.62769613e-03 1.44006282e-01 9.24618840e-01 5.45649767e-01\n"," 4.44675311e-02 6.62743092e-01 8.85827541e-01 1.65934175e-01\n"," 8.59533012e-01 1.85392424e-03 7.87392557e-02 1.27191497e-05\n"," 2.47377699e-04 9.74556282e-02 3.44945322e-04 2.89189484e-04\n"," 4.43474064e-03 3.22913490e-02 7.19171941e-01 9.94999290e-01\n"," 1.26011553e-03 5.58343709e-01 9.83391166e-01 1.41821364e-02\n"," 3.94219023e-05 1.86483783e-04 3.50148559e-01 1.90981280e-03\n"," 9.91380960e-02 3.69982189e-07 1.78190191e-07 2.70936998e-05\n"," 4.73456981e-04 3.37274373e-01 1.26193762e-02 3.49504757e-04\n"," 2.04812255e-04 9.27456422e-05 4.19587642e-01 2.14122850e-02\n"," 5.92082143e-02 6.83380812e-02 8.83864224e-01 5.73516846e-01\n"," 9.48553860e-01 9.48967159e-01 9.97685671e-01 1.01486905e-04\n"," 1.48575082e-05 4.62324265e-03 4.09045140e-04 1.49161511e-04\n"," 2.10929245e-01 6.30980082e-07 9.99179669e-04 1.16126332e-03\n"," 9.99539256e-01 9.99174893e-01 9.99566376e-01 9.99077559e-01\n"," 9.98135567e-01 9.66857910e-01 8.45917463e-01 9.99719560e-01\n"," 9.98745084e-01 9.57056761e-01 9.87530231e-01 2.37373471e-01\n"," 9.30825114e-01 9.81410027e-01 7.09442973e-01 8.96519005e-01\n"," 9.93852139e-01 9.96074200e-01 9.99467790e-01 9.98227775e-01\n"," 9.99767363e-01 9.98264849e-01 9.99902606e-01 9.91133749e-01\n"," 9.30384696e-01 9.98316169e-01 9.35112059e-01 9.89425778e-01\n"," 3.70128429e-03 7.82153845e-01 9.76892591e-01 7.50941992e-01\n"," 1.98037624e-01 9.99820173e-01 9.92879331e-01 9.98377919e-01\n"," 3.11101854e-01 6.77384615e-01 5.03210053e-02 8.17341268e-01\n"," 2.16578484e-01 9.51939046e-01 4.72413236e-03 8.03847551e-01\n"," 8.95584822e-01 9.69015479e-01 8.99601161e-01 1.30117603e-03\n"," 1.46932231e-04 1.45378022e-03 5.07016070e-02 4.54105000e-04\n"," 8.33083868e-01 9.88180280e-01 2.21801832e-01 1.05001569e-01\n"," 9.92248774e-01 9.62366045e-01]\n","predict [0. 1. 0. 1. 0. 0. 1. 1. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.\n"," 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.\n"," 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1.\n"," 0. 1. 0. 1. 0. 1. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 1.]\n","vote_pred [0. 1. 0. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1.\n"," 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1.\n"," 0. 1. 1. 1. 1. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1. 0. 1. 1. 1.]\n","targetlist [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n","TP= 46 TN= 49 FN= 12 FP= 11\n","TP+FP 57\n","precision 0.8070175438596491\n","recall 0.7931034482758621\n","F1 0.8\n","acc 0.8050847457627118\n","AUCp 0.8048850574712644\n","AUC 0.8543103448275863\n","\n"," The epoch is 30, average recall: 0.7931, average precision: 0.8070,average F1: 0.8000, average accuracy: 0.8051, average AUC: 0.8543\n","Train Epoch: 31 [0/107 (0%)]\tTrain Loss: 0.000477\n","Train Epoch: 31 [4/107 (4%)]\tTrain Loss: 0.027425\n","Train Epoch: 31 [8/107 (7%)]\tTrain Loss: 0.022675\n","Train Epoch: 31 [12/107 (11%)]\tTrain Loss: 0.000875\n","Train Epoch: 31 [16/107 (15%)]\tTrain Loss: 0.000228\n","Train Epoch: 31 [20/107 (19%)]\tTrain Loss: 0.001031\n","Train Epoch: 31 [24/107 (22%)]\tTrain Loss: 0.003164\n","Train Epoch: 31 [28/107 (26%)]\tTrain Loss: 0.006206\n","Train Epoch: 31 [32/107 (30%)]\tTrain Loss: 0.010900\n","Train Epoch: 31 [36/107 (34%)]\tTrain Loss: 0.000893\n","Train Epoch: 31 [40/107 (37%)]\tTrain Loss: 0.003390\n","Train Epoch: 31 [44/107 (41%)]\tTrain Loss: 0.000521\n","Train Epoch: 31 [48/107 (45%)]\tTrain Loss: 0.076150\n","Train Epoch: 31 [52/107 (49%)]\tTrain Loss: 0.416674\n","Train Epoch: 31 [56/107 (52%)]\tTrain Loss: 0.000042\n","Train Epoch: 31 [60/107 (56%)]\tTrain Loss: 0.000579\n","Train Epoch: 31 [64/107 (60%)]\tTrain Loss: 0.010158\n","Train Epoch: 31 [68/107 (64%)]\tTrain Loss: 0.202134\n","Train Epoch: 31 [72/107 (67%)]\tTrain Loss: 0.002970\n","Train Epoch: 31 [76/107 (71%)]\tTrain Loss: 0.030821\n","Train Epoch: 31 [80/107 (75%)]\tTrain Loss: 0.001763\n","Train Epoch: 31 [84/107 (79%)]\tTrain Loss: 0.002298\n","Train Epoch: 31 [88/107 (82%)]\tTrain Loss: 0.015109\n","Train Epoch: 31 [92/107 (86%)]\tTrain Loss: 0.005056\n","Train Epoch: 31 [96/107 (90%)]\tTrain Loss: 0.187961\n","Train Epoch: 31 [100/107 (93%)]\tTrain Loss: 0.137084\n","Train Epoch: 31 [104/107 (97%)]\tTrain Loss: 0.013452\n","target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n","score [6.82113646e-03 9.29144442e-01 1.69582799e-01 9.26846087e-01\n"," 3.40126693e-01 9.91011858e-02 8.98262560e-01 8.97625685e-01\n"," 1.45119103e-02 9.21927691e-01 9.98417020e-01 5.34323275e-01\n"," 9.75088775e-01 9.57436115e-02 3.11959565e-01 1.00443617e-03\n"," 2.91257910e-02 7.86815643e-01 1.32207312e-02 1.91923939e-02\n"," 1.47910342e-01 9.68387008e-01 7.66874194e-01 9.95445251e-01\n"," 6.13771975e-01 4.97004420e-01 8.90793741e-01 7.06433594e-01\n"," 9.71297384e-04 5.17940056e-03 8.10032547e-01 7.32043460e-02\n"," 1.35683119e-01 1.34904680e-04 3.16876016e-04 1.74954685e-03\n"," 1.12489490e-02 9.96769071e-01 8.85928035e-01 1.21087879e-01\n"," 1.14958115e-01 5.73588833e-02 9.97495830e-01 8.91784847e-01\n"," 8.77950013e-01 4.65516895e-01 3.37482214e-01 2.25766227e-01\n"," 5.39569497e-01 9.65606809e-01 9.15847898e-01 2.85867194e-04\n"," 2.28600507e-03 6.45636618e-02 7.08397920e-06 1.02772692e-03\n"," 5.22991419e-01 4.95616007e-07 1.27954088e-04 2.98718549e-02\n"," 9.99537706e-01 9.99233723e-01 9.99550998e-01 9.98618364e-01\n"," 9.91546094e-01 9.68120933e-01 9.72798169e-01 9.99946117e-01\n"," 9.99430001e-01 9.82226372e-01 9.96900320e-01 9.54996288e-01\n"," 9.97339189e-01 9.99219775e-01 9.87540185e-01 9.80564415e-01\n"," 9.99200523e-01 9.98151004e-01 9.98095572e-01 9.96224880e-01\n"," 9.98906016e-01 9.96411383e-01 9.98773396e-01 9.98745680e-01\n"," 9.92336571e-01 9.99617815e-01 9.56864357e-01 9.81052935e-01\n"," 5.29841408e-02 1.30313978e-01 9.15319920e-01 9.93095458e-01\n"," 9.54981923e-01 9.99792755e-01 9.73490000e-01 9.96875048e-01\n"," 1.94932625e-01 9.35918152e-01 8.46548021e-01 9.93957281e-01\n"," 9.42217648e-01 9.98902798e-01 7.28316844e-01 9.61326063e-01\n"," 9.79131222e-01 9.87973571e-01 9.77675378e-01 6.63875490e-02\n"," 4.23100311e-03 3.54417264e-02 5.71940914e-02 6.97177416e-03\n"," 8.30004334e-01 9.52550232e-01 7.79268861e-01 9.68328774e-01\n"," 9.99547303e-01 9.95428681e-01]\n","predict [0. 1. 0. 1. 0. 0. 1. 1. 0. 1. 1. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1.\n"," 1. 0. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 0. 0.\n"," 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1.\n"," 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1.]\n","Train Epoch: 32 [0/107 (0%)]\tTrain Loss: 0.003480\n","Train Epoch: 32 [4/107 (4%)]\tTrain Loss: 0.017094\n","Train Epoch: 32 [8/107 (7%)]\tTrain Loss: 0.013216\n","Train Epoch: 32 [12/107 (11%)]\tTrain Loss: 0.002498\n","Train Epoch: 32 [16/107 (15%)]\tTrain Loss: 0.010685\n","Train Epoch: 32 [20/107 (19%)]\tTrain Loss: 0.004092\n","Train Epoch: 32 [24/107 (22%)]\tTrain Loss: 0.000663\n","Train Epoch: 32 [28/107 (26%)]\tTrain Loss: 0.002132\n","Train Epoch: 32 [32/107 (30%)]\tTrain Loss: 0.034513\n","Train Epoch: 32 [36/107 (34%)]\tTrain Loss: 0.002463\n","Train Epoch: 32 [40/107 (37%)]\tTrain Loss: 0.004357\n","Train Epoch: 32 [44/107 (41%)]\tTrain Loss: 0.005324\n","Train Epoch: 32 [48/107 (45%)]\tTrain Loss: 0.007031\n","Train Epoch: 32 [52/107 (49%)]\tTrain Loss: 0.020365\n","Train Epoch: 32 [56/107 (52%)]\tTrain Loss: 0.015210\n","Train Epoch: 32 [60/107 (56%)]\tTrain Loss: 0.037282\n","Train Epoch: 32 [64/107 (60%)]\tTrain Loss: 0.018652\n","Train Epoch: 32 [68/107 (64%)]\tTrain Loss: 0.088228\n","Train Epoch: 32 [72/107 (67%)]\tTrain Loss: 0.005998\n","Train Epoch: 32 [76/107 (71%)]\tTrain Loss: 0.003789\n","Train Epoch: 32 [80/107 (75%)]\tTrain Loss: 0.001140\n","Train Epoch: 32 [84/107 (79%)]\tTrain Loss: 0.006752\n","Train Epoch: 32 [88/107 (82%)]\tTrain Loss: 0.002557\n","Train Epoch: 32 [92/107 (86%)]\tTrain Loss: 0.000384\n","Train Epoch: 32 [96/107 (90%)]\tTrain Loss: 0.000177\n","Train Epoch: 32 [100/107 (93%)]\tTrain Loss: 0.001197\n","Train Epoch: 32 [104/107 (97%)]\tTrain Loss: 0.194959\n","target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n","score [1.31483749e-02 9.99633670e-01 7.66317427e-01 8.74228239e-01\n"," 1.06251113e-01 4.59839493e-01 9.99101639e-01 4.70917612e-01\n"," 1.13862623e-02 9.68459129e-01 9.44535196e-01 2.43562490e-01\n"," 3.12130898e-01 6.32791519e-02 1.03625700e-01 9.94368929e-06\n"," 6.91476860e-04 3.44817862e-02 2.43307470e-04 2.38249148e-03\n"," 1.04157152e-02 4.17751849e-01 9.77924824e-01 9.99987245e-01\n"," 1.20051980e-01 8.57051969e-01 9.93768692e-01 3.81610721e-01\n"," 4.94345906e-04 1.79778261e-03 4.99279946e-01 1.33723244e-02\n"," 1.48592845e-01 4.12077025e-05 8.04985029e-05 1.08918706e-04\n"," 1.63975812e-03 9.53721046e-01 1.51188150e-02 3.29771987e-03\n"," 2.81009567e-03 2.62528984e-03 9.33296502e-01 1.86115429e-01\n"," 7.42357135e-01 8.19920972e-02 4.74135280e-01 1.24753222e-01\n"," 9.23464239e-01 8.88775289e-01 9.97774780e-01 8.93430028e-04\n"," 3.98066797e-04 1.00723710e-02 3.52268444e-06 7.56415189e-04\n"," 5.70680916e-01 2.13711155e-06 5.49928693e-04 1.63640603e-02\n"," 9.99966621e-01 9.99965310e-01 9.99993086e-01 9.99973655e-01\n"," 9.99922156e-01 9.98037159e-01 9.92743254e-01 9.99998450e-01\n"," 9.99999404e-01 9.92418289e-01 9.96422291e-01 8.11511695e-01\n"," 9.96448517e-01 9.99851227e-01 9.91503000e-01 9.82824087e-01\n"," 9.99994516e-01 9.99989152e-01 9.99998212e-01 9.99300957e-01\n"," 9.99952674e-01 9.99933839e-01 9.99997497e-01 9.78130341e-01\n"," 9.73663747e-01 9.99839783e-01 9.97926474e-01 9.99372303e-01\n"," 1.02616087e-01 7.54615426e-01 9.98528123e-01 9.28899169e-01\n"," 4.98205543e-01 9.99998093e-01 9.91333902e-01 9.99909163e-01\n"," 4.65521693e-01 2.12300301e-01 1.49806827e-01 9.88465428e-01\n"," 3.08172792e-01 9.97945726e-01 1.35080433e-02 5.51986635e-01\n"," 8.28973770e-01 8.89657199e-01 9.47159410e-01 3.07926498e-02\n"," 5.88890340e-04 1.37107540e-03 8.29130635e-02 1.69443851e-03\n"," 8.37928772e-01 9.93646264e-01 9.09345925e-01 9.60177600e-01\n"," 9.99982834e-01 9.97643054e-01]\n","predict [0. 1. 1. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.\n"," 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0.\n"," 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1.\n"," 0. 0. 0. 1. 0. 1. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1.]\n","Train Epoch: 33 [0/107 (0%)]\tTrain Loss: 0.003922\n","Train Epoch: 33 [4/107 (4%)]\tTrain Loss: 0.008691\n","Train Epoch: 33 [8/107 (7%)]\tTrain Loss: 0.087261\n","Train Epoch: 33 [12/107 (11%)]\tTrain Loss: 0.004644\n","Train Epoch: 33 [16/107 (15%)]\tTrain Loss: 0.115590\n","Train Epoch: 33 [20/107 (19%)]\tTrain Loss: 0.007596\n","Train Epoch: 33 [24/107 (22%)]\tTrain Loss: 0.014804\n","Train Epoch: 33 [28/107 (26%)]\tTrain Loss: 0.035878\n","Train Epoch: 33 [32/107 (30%)]\tTrain Loss: 0.007940\n","Train Epoch: 33 [36/107 (34%)]\tTrain Loss: 0.007495\n","Train Epoch: 33 [40/107 (37%)]\tTrain Loss: 0.048397\n","Train Epoch: 33 [44/107 (41%)]\tTrain Loss: 0.076897\n","Train Epoch: 33 [48/107 (45%)]\tTrain Loss: 0.016838\n","Train Epoch: 33 [52/107 (49%)]\tTrain Loss: 0.000562\n","Train Epoch: 33 [56/107 (52%)]\tTrain Loss: 0.003906\n","Train Epoch: 33 [60/107 (56%)]\tTrain Loss: 0.002165\n","Train Epoch: 33 [64/107 (60%)]\tTrain Loss: 0.131977\n","Train Epoch: 33 [68/107 (64%)]\tTrain Loss: 0.020970\n","Train Epoch: 33 [72/107 (67%)]\tTrain Loss: 0.000898\n","Train Epoch: 33 [76/107 (71%)]\tTrain Loss: 0.003550\n","Train Epoch: 33 [80/107 (75%)]\tTrain Loss: 0.002371\n","Train Epoch: 33 [84/107 (79%)]\tTrain Loss: 0.000999\n","Train Epoch: 33 [88/107 (82%)]\tTrain Loss: 0.246905\n","Train Epoch: 33 [92/107 (86%)]\tTrain Loss: 0.006698\n","Train Epoch: 33 [96/107 (90%)]\tTrain Loss: 0.000396\n","Train Epoch: 33 [100/107 (93%)]\tTrain Loss: 0.005234\n","Train Epoch: 33 [104/107 (97%)]\tTrain Loss: 0.003873\n","target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n","score [1.48936376e-01 7.68954992e-01 5.19566089e-02 8.12243581e-01\n"," 1.29042402e-01 3.40858489e-01 8.23447168e-01 2.71106839e-01\n"," 8.55877995e-02 9.30470645e-01 8.71449590e-01 1.47065505e-01\n"," 2.47920111e-01 5.70527278e-03 1.24886800e-02 1.45702288e-04\n"," 2.26566615e-03 1.56660173e-02 5.14236931e-03 2.03137416e-02\n"," 3.98129150e-02 3.59451771e-01 7.37721205e-01 9.98900890e-01\n"," 2.30178982e-01 4.44159806e-01 9.93435979e-01 1.33309839e-03\n"," 1.23728521e-03 2.09928653e-03 1.69236232e-02 5.18652983e-03\n"," 1.90463029e-02 1.94167995e-04 5.47491691e-05 4.16706840e-04\n"," 1.58827996e-03 6.46598563e-02 3.64461094e-02 4.43184096e-03\n"," 5.10776555e-03 1.29711395e-02 1.64019942e-01 2.07063258e-02\n"," 1.68297678e-01 2.32175514e-02 1.67524576e-01 1.54415537e-02\n"," 3.20213616e-01 8.35695624e-01 9.31505740e-01 8.11581791e-04\n"," 3.78656923e-03 2.38912646e-02 2.03003583e-04 3.27717699e-03\n"," 9.74217713e-01 2.29723883e-05 2.01922422e-03 8.07856023e-03\n"," 9.54782605e-01 9.70210075e-01 9.65418458e-01 8.93471956e-01\n"," 9.59603488e-01 8.58906150e-01 8.21966588e-01 9.99911547e-01\n"," 9.99759614e-01 7.85324454e-01 8.64389956e-01 3.03108752e-01\n"," 9.19515073e-01 9.89824593e-01 3.95069212e-01 5.90187848e-01\n"," 9.97479856e-01 9.92166519e-01 8.13099504e-01 9.69154656e-01\n"," 9.99290824e-01 9.99606907e-01 9.99413967e-01 3.24264377e-01\n"," 7.34907627e-01 9.90655482e-01 7.10569918e-01 8.31695318e-01\n"," 3.25415023e-02 5.29136956e-01 8.48182559e-01 9.59269464e-01\n"," 3.05694968e-01 9.99993682e-01 9.90094364e-01 9.99893904e-01\n"," 4.61394787e-01 6.91635549e-01 1.87206194e-01 9.88600314e-01\n"," 1.49640754e-01 8.99882555e-01 2.30636541e-02 9.23140466e-01\n"," 9.75762188e-01 9.97377753e-01 7.37543583e-01 2.48497911e-02\n"," 1.44536924e-02 3.21903937e-02 2.61040062e-01 3.36352363e-02\n"," 9.98144984e-01 8.87788773e-01 5.46253204e-01 2.94856913e-02\n"," 9.93293881e-01 9.59943473e-01]\n","predict [0. 1. 0. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.\n"," 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0.\n"," 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1.\n"," 0. 1. 0. 1. 0. 1. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 1. 1. 0. 1. 1.]\n","Train Epoch: 34 [0/107 (0%)]\tTrain Loss: 0.008151\n","Train Epoch: 34 [4/107 (4%)]\tTrain Loss: 0.125567\n","Train Epoch: 34 [8/107 (7%)]\tTrain Loss: 0.119992\n","Train Epoch: 34 [12/107 (11%)]\tTrain Loss: 0.013250\n","Train Epoch: 34 [16/107 (15%)]\tTrain Loss: 0.020961\n","Train Epoch: 34 [20/107 (19%)]\tTrain Loss: 0.171851\n","Train Epoch: 34 [24/107 (22%)]\tTrain Loss: 0.013740\n","Train Epoch: 34 [28/107 (26%)]\tTrain Loss: 0.004604\n","Train Epoch: 34 [32/107 (30%)]\tTrain Loss: 0.009094\n","Train Epoch: 34 [36/107 (34%)]\tTrain Loss: 0.001095\n","Train Epoch: 34 [40/107 (37%)]\tTrain Loss: 0.004512\n","Train Epoch: 34 [44/107 (41%)]\tTrain Loss: 0.056278\n","Train Epoch: 34 [48/107 (45%)]\tTrain Loss: 0.007468\n","Train Epoch: 34 [52/107 (49%)]\tTrain Loss: 0.045885\n","Train Epoch: 34 [56/107 (52%)]\tTrain Loss: 0.233116\n","Train Epoch: 34 [60/107 (56%)]\tTrain Loss: 0.079258\n","Train Epoch: 34 [64/107 (60%)]\tTrain Loss: 0.004339\n","Train Epoch: 34 [68/107 (64%)]\tTrain Loss: 0.003857\n","Train Epoch: 34 [72/107 (67%)]\tTrain Loss: 0.007255\n","Train Epoch: 34 [76/107 (71%)]\tTrain Loss: 0.002273\n","Train Epoch: 34 [80/107 (75%)]\tTrain Loss: 0.007039\n","Train Epoch: 34 [84/107 (79%)]\tTrain Loss: 0.040674\n","Train Epoch: 34 [88/107 (82%)]\tTrain Loss: 0.004877\n","Train Epoch: 34 [92/107 (86%)]\tTrain Loss: 0.008944\n","Train Epoch: 34 [96/107 (90%)]\tTrain Loss: 0.014624\n","Train Epoch: 34 [100/107 (93%)]\tTrain Loss: 0.005570\n","Train Epoch: 34 [104/107 (97%)]\tTrain Loss: 0.057158\n","target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n","score [2.57704794e-01 9.98540521e-01 9.91767883e-01 9.63328063e-01\n"," 5.61846554e-01 6.61047041e-01 9.97946441e-01 8.36714804e-01\n"," 4.55096513e-01 8.47936869e-01 9.87082124e-01 5.34249663e-01\n"," 9.08603489e-01 3.88764799e-01 5.28629065e-01 1.39761262e-03\n"," 9.22817178e-03 7.38930643e-01 1.34538844e-01 4.56526697e-01\n"," 9.57115233e-01 9.81520474e-01 9.96703446e-01 9.99664783e-01\n"," 8.11382055e-01 9.90275502e-01 9.99730766e-01 7.37494975e-02\n"," 2.63429694e-02 2.96080653e-02 7.23906994e-01 1.98592737e-01\n"," 5.56047618e-01 2.35197740e-03 1.10659725e-03 1.31071024e-02\n"," 2.35981345e-02 9.74717200e-01 5.95479250e-01 1.06989458e-01\n"," 7.39519373e-02 3.25532824e-01 9.19737160e-01 9.51246023e-01\n"," 9.94753122e-01 7.93675184e-01 9.25936937e-01 4.43089783e-01\n"," 9.22736824e-01 9.73370135e-01 9.51139271e-01 7.28201712e-06\n"," 3.65847663e-04 7.16286898e-02 3.79768011e-07 1.78898429e-03\n"," 7.64917254e-01 4.13806553e-08 1.68457438e-04 2.95967221e-01\n"," 9.99843121e-01 9.99958515e-01 9.99848008e-01 9.99610603e-01\n"," 9.99687195e-01 9.98811603e-01 9.99166489e-01 9.99996901e-01\n"," 9.99981642e-01 9.71520364e-01 9.99412537e-01 9.87681031e-01\n"," 9.97967422e-01 9.94768143e-01 9.86535847e-01 9.98224914e-01\n"," 9.99946237e-01 9.99894381e-01 9.99943852e-01 9.99912858e-01\n"," 9.99989986e-01 9.99987245e-01 9.99980450e-01 9.90707278e-01\n"," 9.69515383e-01 9.99263465e-01 9.98687446e-01 9.99238014e-01\n"," 7.89923608e-01 9.30549502e-01 9.88740504e-01 9.74717200e-01\n"," 8.47709000e-01 9.99913573e-01 9.97574270e-01 9.99821961e-01\n"," 8.08365285e-01 8.03705752e-01 7.56934881e-01 9.77358937e-01\n"," 4.99229908e-01 9.96726632e-01 1.67318389e-01 9.39098179e-01\n"," 9.66370940e-01 9.96544063e-01 9.71345782e-01 8.03035721e-02\n"," 1.56940445e-02 1.53271049e-01 2.50314027e-01 8.08921643e-03\n"," 8.04098487e-01 9.97014761e-01 9.78148699e-01 9.27966356e-01\n"," 9.98816729e-01 9.71832991e-01]\n","predict [0. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0. 1. 0. 0. 1. 0. 0. 1. 1. 1. 1.\n"," 1. 1. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 1. 1. 1. 1. 0.\n"," 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1.]\n","Train Epoch: 35 [0/107 (0%)]\tTrain Loss: 0.002373\n","Train Epoch: 35 [4/107 (4%)]\tTrain Loss: 0.031023\n","Train Epoch: 35 [8/107 (7%)]\tTrain Loss: 0.006125\n","Train Epoch: 35 [12/107 (11%)]\tTrain Loss: 0.003522\n","Train Epoch: 35 [16/107 (15%)]\tTrain Loss: 0.006493\n","Train Epoch: 35 [20/107 (19%)]\tTrain Loss: 0.003670\n","Train Epoch: 35 [24/107 (22%)]\tTrain Loss: 0.002853\n","Train Epoch: 35 [28/107 (26%)]\tTrain Loss: 0.008178\n","Train Epoch: 35 [32/107 (30%)]\tTrain Loss: 0.008144\n","Train Epoch: 35 [36/107 (34%)]\tTrain Loss: 0.049085\n","Train Epoch: 35 [40/107 (37%)]\tTrain Loss: 0.017986\n","Train Epoch: 35 [44/107 (41%)]\tTrain Loss: 0.159591\n","Train Epoch: 35 [48/107 (45%)]\tTrain Loss: 0.002451\n","Train Epoch: 35 [52/107 (49%)]\tTrain Loss: 0.003637\n","Train Epoch: 35 [56/107 (52%)]\tTrain Loss: 0.003005\n","Train Epoch: 35 [60/107 (56%)]\tTrain Loss: 0.007785\n","Train Epoch: 35 [64/107 (60%)]\tTrain Loss: 0.010630\n","Train Epoch: 35 [68/107 (64%)]\tTrain Loss: 0.002158\n","Train Epoch: 35 [72/107 (67%)]\tTrain Loss: 0.004684\n","Train Epoch: 35 [76/107 (71%)]\tTrain Loss: 0.012273\n","Train Epoch: 35 [80/107 (75%)]\tTrain Loss: 0.005924\n","Train Epoch: 35 [84/107 (79%)]\tTrain Loss: 0.005510\n","Train Epoch: 35 [88/107 (82%)]\tTrain Loss: 0.004473\n","Train Epoch: 35 [92/107 (86%)]\tTrain Loss: 0.071804\n","Train Epoch: 35 [96/107 (90%)]\tTrain Loss: 0.002725\n","Train Epoch: 35 [100/107 (93%)]\tTrain Loss: 0.001574\n","Train Epoch: 35 [104/107 (97%)]\tTrain Loss: 0.080265\n","target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n","score [5.96849442e-01 9.99467432e-01 9.60649788e-01 9.99526501e-01\n"," 8.80032063e-01 6.99147999e-01 9.96865213e-01 9.94932473e-01\n"," 6.61007822e-01 9.20766473e-01 9.80960250e-01 7.95473278e-01\n"," 6.79544687e-01 1.00578733e-01 6.82334900e-02 2.32406543e-03\n"," 2.78928056e-02 9.22931254e-01 3.44781484e-03 1.99228004e-02\n"," 5.37711620e-01 9.27702844e-01 8.19435418e-01 9.99656677e-01\n"," 5.25248706e-01 9.68178213e-01 9.99878645e-01 6.40300289e-03\n"," 1.14875264e-03 1.45214365e-03 9.21938866e-02 2.16762666e-02\n"," 2.47467995e-01 3.90640227e-04 4.48980165e-04 2.36438103e-02\n"," 1.32468753e-02 9.64048922e-01 7.70422995e-01 9.83986482e-02\n"," 8.48482102e-02 6.35380298e-02 9.60862339e-01 9.45994794e-01\n"," 9.85400975e-01 6.61406159e-01 9.99171257e-01 9.92958546e-01\n"," 9.96517897e-01 9.94180977e-01 9.99664187e-01 2.26339807e-06\n"," 1.05879553e-04 1.79587537e-03 3.74972906e-08 4.28238272e-05\n"," 1.51067212e-01 4.48298572e-08 6.85810082e-05 8.89686465e-01\n"," 9.99986172e-01 9.99993324e-01 9.99973655e-01 9.99944925e-01\n"," 9.99153733e-01 9.97124374e-01 9.99636292e-01 9.99992013e-01\n"," 9.99999046e-01 9.78206992e-01 9.99645472e-01 9.89582658e-01\n"," 9.98104811e-01 9.98409212e-01 9.49363053e-01 9.98403013e-01\n"," 9.99996662e-01 9.99993801e-01 9.99918699e-01 9.99989867e-01\n"," 9.99998927e-01 9.99995947e-01 9.99998450e-01 9.88228559e-01\n"," 9.48300779e-01 9.97549951e-01 9.85460699e-01 9.94265258e-01\n"," 1.81393877e-01 9.89162922e-01 9.80168998e-01 9.82337594e-01\n"," 5.82152903e-01 9.99917507e-01 9.98091519e-01 9.99465525e-01\n"," 6.52465284e-01 9.93746042e-01 9.87969995e-01 9.99235868e-01\n"," 7.79113352e-01 9.99641895e-01 6.31919146e-01 9.69405949e-01\n"," 9.77205575e-01 9.84315693e-01 9.44854915e-01 2.58662272e-02\n"," 2.23187044e-01 5.78257255e-02 2.05736741e-01 2.96381205e-01\n"," 9.63314354e-01 9.97907162e-01 9.94927645e-01 9.60526884e-01\n"," 9.99843478e-01 9.96890008e-01]\n","predict [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 1.\n"," 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1.]\n","Train Epoch: 36 [0/107 (0%)]\tTrain Loss: 0.002844\n","Train Epoch: 36 [4/107 (4%)]\tTrain Loss: 0.001828\n","Train Epoch: 36 [8/107 (7%)]\tTrain Loss: 0.006762\n","Train Epoch: 36 [12/107 (11%)]\tTrain Loss: 0.024101\n","Train Epoch: 36 [16/107 (15%)]\tTrain Loss: 0.001594\n","Train Epoch: 36 [20/107 (19%)]\tTrain Loss: 0.001078\n","Train Epoch: 36 [24/107 (22%)]\tTrain Loss: 0.001156\n","Train Epoch: 36 [28/107 (26%)]\tTrain Loss: 0.001275\n","Train Epoch: 36 [32/107 (30%)]\tTrain Loss: 0.011940\n","Train Epoch: 36 [36/107 (34%)]\tTrain Loss: 0.002757\n","Train Epoch: 36 [40/107 (37%)]\tTrain Loss: 0.017159\n","Train Epoch: 36 [44/107 (41%)]\tTrain Loss: 0.012090\n","Train Epoch: 36 [48/107 (45%)]\tTrain Loss: 0.028033\n","Train Epoch: 36 [52/107 (49%)]\tTrain Loss: 0.136645\n","Train Epoch: 36 [56/107 (52%)]\tTrain Loss: 0.117420\n","Train Epoch: 36 [60/107 (56%)]\tTrain Loss: 0.004326\n","Train Epoch: 36 [64/107 (60%)]\tTrain Loss: 0.026417\n","Train Epoch: 36 [68/107 (64%)]\tTrain Loss: 0.013505\n","Train Epoch: 36 [72/107 (67%)]\tTrain Loss: 0.003394\n","Train Epoch: 36 [76/107 (71%)]\tTrain Loss: 0.035542\n","Train Epoch: 36 [80/107 (75%)]\tTrain Loss: 0.005974\n","Train Epoch: 36 [84/107 (79%)]\tTrain Loss: 0.004770\n","Train Epoch: 36 [88/107 (82%)]\tTrain Loss: 0.017949\n","Train Epoch: 36 [92/107 (86%)]\tTrain Loss: 0.029215\n","Train Epoch: 36 [96/107 (90%)]\tTrain Loss: 0.007095\n","Train Epoch: 36 [100/107 (93%)]\tTrain Loss: 0.061640\n","Train Epoch: 36 [104/107 (97%)]\tTrain Loss: 0.030562\n","target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n","score [1.15056418e-01 8.90720308e-01 4.47408780e-02 4.35106635e-01\n"," 2.32401162e-01 3.04441806e-02 6.48437083e-01 3.37694049e-01\n"," 1.35013824e-02 6.94444895e-01 6.03339791e-01 4.12191272e-01\n"," 4.71724160e-02 1.51463682e-02 2.42319219e-02 6.91940496e-03\n"," 4.00947919e-03 1.91902176e-01 5.83193405e-03 1.05477851e-02\n"," 8.12105164e-02 3.77391189e-01 9.08508003e-02 9.99027967e-01\n"," 1.68168545e-01 2.54378498e-01 9.99378562e-01 8.82390663e-02\n"," 9.56186326e-04 2.86533352e-04 7.04277158e-01 5.26282787e-02\n"," 1.24646075e-01 3.43676168e-03 3.74624040e-03 5.53292781e-03\n"," 4.72734543e-03 5.49384654e-01 6.16106093e-01 1.02721073e-01\n"," 5.44486605e-02 1.70527473e-01 4.59221482e-01 1.81145683e-01\n"," 3.36545229e-01 9.46467519e-01 6.81325793e-01 4.06400859e-01\n"," 5.25759578e-01 9.12760556e-01 9.55738008e-01 8.57479637e-04\n"," 1.01713855e-02 6.31438196e-02 3.46332137e-03 8.49175267e-03\n"," 9.57140148e-01 1.15889015e-04 7.45733595e-03 1.24753363e-01\n"," 9.98100579e-01 9.95071054e-01 9.98612523e-01 9.99028563e-01\n"," 7.34820724e-01 3.52023423e-01 7.07409918e-01 9.99523759e-01\n"," 9.85696912e-01 9.95100915e-01 9.75824058e-01 5.06917953e-01\n"," 9.78627741e-01 9.89940107e-01 9.40688252e-01 9.91686165e-01\n"," 9.80655909e-01 9.76155043e-01 9.92567539e-01 9.96408641e-01\n"," 9.99611676e-01 9.99289632e-01 9.98595893e-01 9.95245636e-01\n"," 9.42409575e-01 9.99243140e-01 9.75756049e-01 9.53732967e-01\n"," 1.66317150e-02 4.15202349e-01 9.75182176e-01 9.70511436e-01\n"," 4.93254811e-01 9.99857068e-01 9.98181581e-01 9.98782814e-01\n"," 1.54288501e-01 9.84117210e-01 5.23450553e-01 9.98266935e-01\n"," 5.04875600e-01 9.84418035e-01 1.33266926e-01 9.53805208e-01\n"," 9.75419879e-01 9.98030841e-01 9.98742878e-01 5.67830019e-02\n"," 2.62007006e-02 8.21163133e-02 2.10425600e-01 5.31219900e-01\n"," 9.91135240e-01 1.59737512e-01 1.31403849e-01 5.52671015e-01\n"," 9.96430695e-01 9.57503617e-01]\n","predict [0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n"," 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0.\n"," 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 0. 1. 1. 1.\n"," 0. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0. 0. 0. 0. 1. 1. 0. 0. 1. 1. 1.]\n","Train Epoch: 37 [0/107 (0%)]\tTrain Loss: 0.003133\n","Train Epoch: 37 [4/107 (4%)]\tTrain Loss: 0.002900\n","Train Epoch: 37 [8/107 (7%)]\tTrain Loss: 0.208091\n","Train Epoch: 37 [12/107 (11%)]\tTrain Loss: 0.023316\n","Train Epoch: 37 [16/107 (15%)]\tTrain Loss: 0.009494\n","Train Epoch: 37 [20/107 (19%)]\tTrain Loss: 0.002100\n","Train Epoch: 37 [24/107 (22%)]\tTrain Loss: 0.000825\n","Train Epoch: 37 [28/107 (26%)]\tTrain Loss: 0.001065\n","Train Epoch: 37 [32/107 (30%)]\tTrain Loss: 0.005890\n","Train Epoch: 37 [36/107 (34%)]\tTrain Loss: 0.003243\n","Train Epoch: 37 [40/107 (37%)]\tTrain Loss: 0.011611\n","Train Epoch: 37 [44/107 (41%)]\tTrain Loss: 0.027530\n","Train Epoch: 37 [48/107 (45%)]\tTrain Loss: 0.007830\n","Train Epoch: 37 [52/107 (49%)]\tTrain Loss: 0.005406\n","Train Epoch: 37 [56/107 (52%)]\tTrain Loss: 0.002915\n","Train Epoch: 37 [60/107 (56%)]\tTrain Loss: 0.023851\n","Train Epoch: 37 [64/107 (60%)]\tTrain Loss: 0.003827\n","Train Epoch: 37 [68/107 (64%)]\tTrain Loss: 0.007185\n","Train Epoch: 37 [72/107 (67%)]\tTrain Loss: 0.001278\n","Train Epoch: 37 [76/107 (71%)]\tTrain Loss: 0.003143\n","Train Epoch: 37 [80/107 (75%)]\tTrain Loss: 0.000670\n","Train Epoch: 37 [84/107 (79%)]\tTrain Loss: 0.035880\n","Train Epoch: 37 [88/107 (82%)]\tTrain Loss: 0.001827\n","Train Epoch: 37 [92/107 (86%)]\tTrain Loss: 0.002984\n","Train Epoch: 37 [96/107 (90%)]\tTrain Loss: 0.003281\n","Train Epoch: 37 [100/107 (93%)]\tTrain Loss: 0.000248\n","Train Epoch: 37 [104/107 (97%)]\tTrain Loss: 0.003512\n","target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n","score [4.30195453e-03 9.26855922e-01 1.75710861e-02 8.28621015e-02\n"," 5.38702705e-04 1.35468820e-03 9.23469663e-01 6.19371384e-02\n"," 4.62174532e-04 5.93120977e-02 4.32905890e-02 1.72686391e-03\n"," 1.03284104e-03 8.88709619e-05 1.28664903e-03 6.98025985e-07\n"," 1.32479781e-05 3.13666090e-02 8.62971134e-03 2.85995118e-02\n"," 6.96874559e-01 9.19690251e-01 9.48942900e-01 9.99978185e-01\n"," 1.61246911e-01 9.66981709e-01 9.98819768e-01 3.61920781e-02\n"," 2.99190084e-04 8.68769494e-05 3.39948684e-01 1.64432079e-03\n"," 1.19384304e-02 2.39955483e-07 1.03166805e-07 3.53108735e-05\n"," 9.77954987e-05 1.21953003e-01 1.04856759e-03 7.74894761e-06\n"," 7.40068162e-06 2.56439507e-05 9.78586152e-02 1.65835593e-03\n"," 3.40681374e-01 4.87211812e-03 7.05606565e-02 8.96019489e-03\n"," 1.25373125e-01 7.46692121e-01 7.70515978e-01 1.91403402e-07\n"," 7.09789820e-05 1.17643783e-03 1.86050187e-07 1.09195556e-04\n"," 2.71361619e-01 5.69417002e-09 1.31418328e-05 3.73212621e-03\n"," 9.98805165e-01 9.98131573e-01 9.99100447e-01 9.99535084e-01\n"," 9.98737514e-01 1.75021604e-01 2.97661066e-01 9.99952555e-01\n"," 9.95601058e-01 7.65919864e-01 5.35843432e-01 2.07751114e-02\n"," 9.83424723e-01 9.93884385e-01 8.73457670e-01 9.99593794e-01\n"," 9.99741137e-01 9.99769390e-01 9.98542309e-01 9.90454972e-01\n"," 9.99105275e-01 9.99015927e-01 9.99349177e-01 9.92089391e-01\n"," 9.78081763e-01 9.99726474e-01 9.93181288e-01 9.96805787e-01\n"," 4.54400713e-03 4.39252481e-02 9.78091419e-01 7.02741444e-01\n"," 1.04848661e-01 9.99982595e-01 9.98130739e-01 9.99155760e-01\n"," 3.50631833e-01 7.39609897e-01 8.13002288e-01 7.03715011e-02\n"," 5.43185463e-03 9.26160276e-01 6.95881713e-03 2.97753930e-01\n"," 8.61118376e-01 9.86832917e-01 9.91371393e-01 9.62973805e-04\n"," 1.15253366e-04 1.06580304e-02 1.41394939e-02 5.31540625e-03\n"," 9.96066749e-01 7.29371727e-01 8.90026987e-03 1.62643135e-01\n"," 9.98751163e-01 9.99799669e-01]\n","predict [0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1.\n"," 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 0.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 0. 1. 1. 1.\n"," 0. 1. 1. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 1.]\n","Train Epoch: 38 [0/107 (0%)]\tTrain Loss: 0.098338\n","Train Epoch: 38 [4/107 (4%)]\tTrain Loss: 0.001965\n","Train Epoch: 38 [8/107 (7%)]\tTrain Loss: 0.014693\n","Train Epoch: 38 [12/107 (11%)]\tTrain Loss: 0.022203\n","Train Epoch: 38 [16/107 (15%)]\tTrain Loss: 0.007367\n","Train Epoch: 38 [20/107 (19%)]\tTrain Loss: 0.000909\n","Train Epoch: 38 [24/107 (22%)]\tTrain Loss: 0.000635\n","Train Epoch: 38 [28/107 (26%)]\tTrain Loss: 0.005621\n","Train Epoch: 38 [32/107 (30%)]\tTrain Loss: 0.013455\n","Train Epoch: 38 [36/107 (34%)]\tTrain Loss: 0.010843\n","Train Epoch: 38 [40/107 (37%)]\tTrain Loss: 0.039099\n","Train Epoch: 38 [44/107 (41%)]\tTrain Loss: 0.003765\n","Train Epoch: 38 [48/107 (45%)]\tTrain Loss: 0.000145\n","Train Epoch: 38 [52/107 (49%)]\tTrain Loss: 0.000663\n","Train Epoch: 38 [56/107 (52%)]\tTrain Loss: 0.000569\n","Train Epoch: 38 [60/107 (56%)]\tTrain Loss: 0.003499\n","Train Epoch: 38 [64/107 (60%)]\tTrain Loss: 0.000745\n","Train Epoch: 38 [68/107 (64%)]\tTrain Loss: 0.000223\n","Train Epoch: 38 [72/107 (67%)]\tTrain Loss: 0.003355\n","Train Epoch: 38 [76/107 (71%)]\tTrain Loss: 0.000403\n","Train Epoch: 38 [80/107 (75%)]\tTrain Loss: 0.131311\n","Train Epoch: 38 [84/107 (79%)]\tTrain Loss: 0.000500\n","Train Epoch: 38 [88/107 (82%)]\tTrain Loss: 0.061847\n","Train Epoch: 38 [92/107 (86%)]\tTrain Loss: 0.027909\n","Train Epoch: 38 [96/107 (90%)]\tTrain Loss: 0.002198\n","Train Epoch: 38 [100/107 (93%)]\tTrain Loss: 0.054543\n","Train Epoch: 38 [104/107 (97%)]\tTrain Loss: 0.015348\n","target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n","score [1.94205672e-01 9.92451012e-01 4.82724234e-02 2.47718155e-01\n"," 4.14060429e-02 6.94609508e-02 9.28165138e-01 2.46212661e-01\n"," 1.73304193e-02 8.24157447e-02 7.19407916e-01 2.84090489e-02\n"," 5.91529347e-02 6.66331872e-03 4.84530348e-03 1.82529690e-03\n"," 9.68609750e-03 7.07310671e-03 1.73623941e-03 7.73952983e-04\n"," 4.42901440e-03 6.08053356e-02 1.72672242e-01 9.99532461e-01\n"," 1.73364040e-02 2.61005312e-02 9.99811709e-01 1.40384631e-03\n"," 4.86734898e-05 1.83571956e-05 2.13716421e-02 1.60096940e-02\n"," 7.07814023e-02 3.91562353e-04 3.14383593e-04 4.55149799e-04\n"," 6.85284985e-03 7.98635423e-01 1.80835873e-02 2.22965633e-03\n"," 2.82625132e-03 3.01986001e-03 7.56435454e-01 1.61414705e-02\n"," 7.05934316e-02 8.52279603e-01 7.47919858e-01 3.80689651e-01\n"," 8.36251020e-01 1.69365406e-01 9.79130208e-01 4.14875988e-03\n"," 5.04310429e-03 2.20377184e-02 1.78734330e-03 5.01485579e-02\n"," 9.96030867e-01 1.14133453e-03 8.28800316e-04 7.32432352e-03\n"," 9.99981284e-01 9.99799550e-01 9.99990702e-01 9.99974251e-01\n"," 7.71984637e-01 9.49461818e-01 9.92655516e-01 9.99999523e-01\n"," 9.99895096e-01 9.99864459e-01 9.86629844e-01 8.13524067e-01\n"," 9.99317884e-01 9.99692321e-01 8.04720581e-01 8.51086676e-01\n"," 9.99976635e-01 9.99918938e-01 9.99664903e-01 9.98449802e-01\n"," 9.99937057e-01 9.99831080e-01 9.99990344e-01 9.25280809e-01\n"," 9.91904497e-01 9.99938488e-01 1.89054608e-01 1.45084813e-01\n"," 1.22113444e-03 1.16629023e-02 2.94547945e-01 9.78015006e-01\n"," 6.35554731e-01 9.99731481e-01 9.66165602e-01 9.99538660e-01\n"," 4.45443094e-01 7.95349002e-01 5.83706081e-01 9.93402421e-01\n"," 1.78968534e-01 9.99963880e-01 4.53207716e-02 9.54687178e-01\n"," 9.38457072e-01 9.98793840e-01 9.99441803e-01 3.87366414e-02\n"," 1.56928739e-03 2.91039012e-02 4.67651248e-01 3.37946892e-01\n"," 9.78645742e-01 9.94153321e-01 9.68695059e-02 2.20018819e-01\n"," 9.82852757e-01 9.99488473e-01]\n","predict [0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n"," 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0.\n"," 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1.\n"," 0. 1. 1. 1. 0. 1. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 1.]\n","Train Epoch: 39 [0/107 (0%)]\tTrain Loss: 0.007707\n","Train Epoch: 39 [4/107 (4%)]\tTrain Loss: 0.039383\n","Train Epoch: 39 [8/107 (7%)]\tTrain Loss: 0.027044\n","Train Epoch: 39 [12/107 (11%)]\tTrain Loss: 0.001149\n","Train Epoch: 39 [16/107 (15%)]\tTrain Loss: 0.000358\n","Train Epoch: 39 [20/107 (19%)]\tTrain Loss: 0.000538\n","Train Epoch: 39 [24/107 (22%)]\tTrain Loss: 0.001552\n","Train Epoch: 39 [28/107 (26%)]\tTrain Loss: 0.005924\n","Train Epoch: 39 [32/107 (30%)]\tTrain Loss: 0.025736\n","Train Epoch: 39 [36/107 (34%)]\tTrain Loss: 0.006284\n","Train Epoch: 39 [40/107 (37%)]\tTrain Loss: 0.006091\n","Train Epoch: 39 [44/107 (41%)]\tTrain Loss: 0.011663\n","Train Epoch: 39 [48/107 (45%)]\tTrain Loss: 0.002797\n","Train Epoch: 39 [52/107 (49%)]\tTrain Loss: 0.010002\n","Train Epoch: 39 [56/107 (52%)]\tTrain Loss: 0.016279\n","Train Epoch: 39 [60/107 (56%)]\tTrain Loss: 0.048188\n","Train Epoch: 39 [64/107 (60%)]\tTrain Loss: 0.021181\n","Train Epoch: 39 [68/107 (64%)]\tTrain Loss: 0.002806\n","Train Epoch: 39 [72/107 (67%)]\tTrain Loss: 0.000767\n","Train Epoch: 39 [76/107 (71%)]\tTrain Loss: 0.016076\n","Train Epoch: 39 [80/107 (75%)]\tTrain Loss: 0.001704\n","Train Epoch: 39 [84/107 (79%)]\tTrain Loss: 0.000923\n","Train Epoch: 39 [88/107 (82%)]\tTrain Loss: 0.043139\n","Train Epoch: 39 [92/107 (86%)]\tTrain Loss: 0.001982\n","Train Epoch: 39 [96/107 (90%)]\tTrain Loss: 0.000559\n","Train Epoch: 39 [100/107 (93%)]\tTrain Loss: 0.000801\n","Train Epoch: 39 [104/107 (97%)]\tTrain Loss: 0.001326\n","target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n","score [1.19035840e-01 9.77424264e-01 2.65304238e-01 6.25216663e-01\n"," 2.23971345e-02 6.16546631e-01 9.77077365e-01 6.97404921e-01\n"," 3.15777324e-02 1.63738191e-01 2.81903565e-01 1.41638562e-01\n"," 2.42485683e-02 1.51125550e-01 2.15081528e-01 3.47328535e-03\n"," 8.39182176e-03 2.87316203e-01 1.33787811e-01 5.54912016e-02\n"," 3.80516380e-01 9.84996915e-01 9.75644052e-01 9.98804688e-01\n"," 9.15771604e-01 8.90454233e-01 9.98917818e-01 4.51149493e-02\n"," 6.94462797e-03 9.18192905e-04 6.66439831e-01 3.95295799e-01\n"," 3.36315870e-01 8.99122751e-05 2.38354551e-05 3.24450870e-04\n"," 7.12560397e-03 8.87600541e-01 1.17799453e-01 9.65445652e-04\n"," 2.22141319e-03 4.27000225e-03 9.39979494e-01 5.93720257e-01\n"," 5.03294349e-01 5.75794458e-01 2.02815369e-01 3.81895542e-01\n"," 3.53275508e-01 4.59907949e-01 7.00791657e-01 6.61550803e-05\n"," 2.06383760e-03 8.68827943e-03 2.96393791e-05 4.97405697e-03\n"," 9.02582705e-01 1.11351858e-06 2.25019804e-03 1.10443514e-02\n"," 9.99935627e-01 9.99476612e-01 9.99973059e-01 9.99923587e-01\n"," 9.81109738e-01 9.87182379e-01 9.76604998e-01 9.99872446e-01\n"," 9.88630295e-01 9.93900895e-01 5.90409517e-01 2.76703566e-01\n"," 9.98222768e-01 9.97655153e-01 9.41602826e-01 9.97919858e-01\n"," 9.99421716e-01 9.98867393e-01 9.99873400e-01 9.97093797e-01\n"," 9.99490023e-01 9.97526109e-01 9.99969840e-01 9.63964522e-01\n"," 8.98275435e-01 9.99235868e-01 9.52737093e-01 9.94093239e-01\n"," 1.21628761e-01 9.91554797e-01 9.80259538e-01 8.01956236e-01\n"," 5.25367737e-01 9.99610960e-01 9.95703280e-01 9.99142289e-01\n"," 3.14687818e-01 9.52508152e-01 8.49199295e-01 9.95640278e-01\n"," 4.34378088e-02 9.97508168e-01 2.51989424e-01 8.94774914e-01\n"," 9.57392395e-01 9.96865094e-01 9.97360170e-01 1.17773652e-01\n"," 2.22739920e-01 7.38570392e-02 5.33723235e-01 2.69417495e-01\n"," 9.93526697e-01 9.97457087e-01 9.98400748e-01 9.90363061e-01\n"," 9.99099731e-01 9.99956965e-01]\n","predict [0. 1. 0. 1. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1.\n"," 1. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0.\n"," 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1.\n"," 0. 1. 1. 1. 0. 1. 0. 1. 1. 1. 1. 0. 0. 0. 1. 0. 1. 1. 1. 1. 1. 1.]\n","Train Epoch: 40 [0/107 (0%)]\tTrain Loss: 0.000865\n","Train Epoch: 40 [4/107 (4%)]\tTrain Loss: 0.001143\n","Train Epoch: 40 [8/107 (7%)]\tTrain Loss: 0.074582\n","Train Epoch: 40 [12/107 (11%)]\tTrain Loss: 0.021215\n","Train Epoch: 40 [16/107 (15%)]\tTrain Loss: 0.000954\n","Train Epoch: 40 [20/107 (19%)]\tTrain Loss: 0.006095\n","Train Epoch: 40 [24/107 (22%)]\tTrain Loss: 0.003553\n","Train Epoch: 40 [28/107 (26%)]\tTrain Loss: 0.023880\n","Train Epoch: 40 [32/107 (30%)]\tTrain Loss: 0.007822\n","Train Epoch: 40 [36/107 (34%)]\tTrain Loss: 0.023007\n","Train Epoch: 40 [40/107 (37%)]\tTrain Loss: 0.001098\n","Train Epoch: 40 [44/107 (41%)]\tTrain Loss: 0.003841\n","Train Epoch: 40 [48/107 (45%)]\tTrain Loss: 0.008448\n","Train Epoch: 40 [52/107 (49%)]\tTrain Loss: 0.000589\n","Train Epoch: 40 [56/107 (52%)]\tTrain Loss: 0.000664\n","Train Epoch: 40 [60/107 (56%)]\tTrain Loss: 0.001236\n","Train Epoch: 40 [64/107 (60%)]\tTrain Loss: 0.001189\n","Train Epoch: 40 [68/107 (64%)]\tTrain Loss: 0.003332\n","Train Epoch: 40 [72/107 (67%)]\tTrain Loss: 0.001231\n","Train Epoch: 40 [76/107 (71%)]\tTrain Loss: 0.003628\n","Train Epoch: 40 [80/107 (75%)]\tTrain Loss: 0.016925\n","Train Epoch: 40 [84/107 (79%)]\tTrain Loss: 0.000413\n","Train Epoch: 40 [88/107 (82%)]\tTrain Loss: 0.013487\n","Train Epoch: 40 [92/107 (86%)]\tTrain Loss: 0.001472\n","Train Epoch: 40 [96/107 (90%)]\tTrain Loss: 0.005740\n","Train Epoch: 40 [100/107 (93%)]\tTrain Loss: 0.005436\n","Train Epoch: 40 [104/107 (97%)]\tTrain Loss: 0.039385\n","target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n","score [8.02486110e-03 5.66736400e-01 5.70159289e-04 4.74982083e-01\n"," 2.77168653e-03 1.46509781e-01 1.20088458e-01 7.62735426e-01\n"," 5.22773573e-03 2.06578046e-01 4.43318248e-01 1.76919162e-01\n"," 1.58471260e-02 7.09767714e-02 1.10844381e-01 7.78743743e-06\n"," 3.31981631e-04 2.80046556e-02 5.64463611e-04 1.31912180e-03\n"," 1.29299634e-03 4.93304133e-01 8.59359652e-03 9.97561812e-01\n"," 8.92954469e-02 1.45225585e-01 9.99063194e-01 4.13652342e-05\n"," 1.72990040e-05 1.62338893e-05 1.75440416e-03 1.57220755e-02\n"," 1.74954697e-01 2.35592029e-06 1.23372629e-07 2.49705899e-05\n"," 3.55839118e-04 2.87085444e-01 8.65835231e-03 1.66109367e-05\n"," 1.31956156e-04 1.12744696e-04 8.44737709e-01 2.78115980e-02\n"," 2.29475051e-01 7.16827214e-01 9.62137282e-02 1.35098726e-01\n"," 4.76882041e-01 8.90297651e-01 7.84539759e-01 8.64443166e-07\n"," 2.62446520e-06 1.74269095e-04 9.78905046e-09 7.44611225e-06\n"," 6.47033215e-01 3.10267700e-10 2.62429076e-05 1.78548027e-04\n"," 9.99991417e-01 9.99981642e-01 9.99997616e-01 9.99998689e-01\n"," 8.83171260e-01 9.64666724e-01 9.85727847e-01 9.99766290e-01\n"," 9.98036802e-01 9.97029006e-01 9.01840091e-01 7.62561738e-01\n"," 9.99705374e-01 9.99957323e-01 9.69977379e-01 9.98605072e-01\n"," 9.99856472e-01 9.99552548e-01 9.93302941e-01 9.98458266e-01\n"," 9.99361932e-01 9.91909921e-01 9.99400258e-01 9.99410629e-01\n"," 9.97630119e-01 9.99973536e-01 8.72760296e-01 7.07272410e-01\n"," 5.60119795e-03 8.20498347e-01 9.83011901e-01 9.54215527e-01\n"," 3.63595724e-01 9.99452174e-01 9.95725513e-01 9.94392753e-01\n"," 3.88729721e-02 9.95034337e-01 9.06769097e-01 9.99166965e-01\n"," 7.79651105e-01 9.99911547e-01 1.42640069e-01 8.69764745e-01\n"," 9.48020518e-01 9.98218596e-01 9.95261014e-01 4.58546132e-02\n"," 2.32166797e-02 4.45158929e-02 5.70226490e-01 2.32166782e-01\n"," 9.94814694e-01 9.54047859e-01 5.38116097e-01 8.45626414e-01\n"," 9.99589980e-01 9.99910355e-01]\n","predict [0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n"," 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0.\n"," 0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1.\n"," 0. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0. 0. 0. 1. 0. 1. 1. 1. 1. 1. 1.]\n","vote_pred [0. 1. 0. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.\n"," 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0.\n"," 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1.\n"," 0. 1. 1. 1. 0. 1. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1.]\n","targetlist [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n","TP= 48 TN= 45 FN= 10 FP= 15\n","TP+FP 63\n","precision 0.7619047619047619\n","recall 0.8275862068965517\n","F1 0.7933884297520662\n","acc 0.788135593220339\n","AUCp 0.7887931034482758\n","AUC 0.8666666666666667\n","\n"," The epoch is 40, average recall: 0.8276, average precision: 0.7619,average F1: 0.7934, average accuracy: 0.7881, average AUC: 0.8667\n","Train Epoch: 41 [0/107 (0%)]\tTrain Loss: 0.005350\n","Train Epoch: 41 [4/107 (4%)]\tTrain Loss: 0.000415\n","Train Epoch: 41 [8/107 (7%)]\tTrain Loss: 0.009313\n","Train Epoch: 41 [12/107 (11%)]\tTrain Loss: 0.002155\n","Train Epoch: 41 [16/107 (15%)]\tTrain Loss: 0.000964\n","Train Epoch: 41 [20/107 (19%)]\tTrain Loss: 0.002565\n","Train Epoch: 41 [24/107 (22%)]\tTrain Loss: 0.003025\n","Train Epoch: 41 [28/107 (26%)]\tTrain Loss: 0.003189\n","Train Epoch: 41 [32/107 (30%)]\tTrain Loss: 0.010532\n","Train Epoch: 41 [36/107 (34%)]\tTrain Loss: 0.009214\n","Train Epoch: 41 [40/107 (37%)]\tTrain Loss: 0.001326\n","Train Epoch: 41 [44/107 (41%)]\tTrain Loss: 0.008901\n","Train Epoch: 41 [48/107 (45%)]\tTrain Loss: 0.084830\n","Train Epoch: 41 [52/107 (49%)]\tTrain Loss: 0.009728\n","Train Epoch: 41 [56/107 (52%)]\tTrain Loss: 0.004780\n","Train Epoch: 41 [60/107 (56%)]\tTrain Loss: 0.002117\n","Train Epoch: 41 [64/107 (60%)]\tTrain Loss: 0.029590\n","Train Epoch: 41 [68/107 (64%)]\tTrain Loss: 0.009456\n","Train Epoch: 41 [72/107 (67%)]\tTrain Loss: 0.008885\n","Train Epoch: 41 [76/107 (71%)]\tTrain Loss: 0.004076\n","Train Epoch: 41 [80/107 (75%)]\tTrain Loss: 0.008263\n","Train Epoch: 41 [84/107 (79%)]\tTrain Loss: 0.001173\n","Train Epoch: 41 [88/107 (82%)]\tTrain Loss: 0.094183\n","Train Epoch: 41 [92/107 (86%)]\tTrain Loss: 0.007603\n","Train Epoch: 41 [96/107 (90%)]\tTrain Loss: 0.005524\n","Train Epoch: 41 [100/107 (93%)]\tTrain Loss: 0.006847\n","Train Epoch: 41 [104/107 (97%)]\tTrain Loss: 0.001709\n","target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n","score [3.43000948e-01 9.71380830e-01 3.38622369e-02 9.67880547e-01\n"," 1.42440021e-01 5.56564569e-01 8.97367954e-01 9.82451737e-01\n"," 8.19200575e-02 7.72915363e-01 9.07339633e-01 1.94507867e-01\n"," 1.39932618e-01 1.22371446e-02 1.49337128e-01 3.15393254e-06\n"," 1.86938010e-02 8.76330018e-01 2.62364745e-03 2.13927738e-02\n"," 2.39184611e-02 6.94498360e-01 9.69520569e-01 9.99886751e-01\n"," 1.63029283e-01 9.76099491e-01 9.99567330e-01 4.70493571e-04\n"," 3.06266083e-05 2.05440529e-06 4.90489304e-01 4.36310936e-03\n"," 6.83384761e-02 4.92010884e-07 3.36338353e-08 1.22388883e-06\n"," 5.56787345e-05 1.26102582e-01 3.53376294e-04 3.64357879e-06\n"," 6.10247707e-06 1.25795314e-05 8.67720783e-01 4.49383073e-03\n"," 8.27461362e-01 9.22797561e-01 4.99042273e-01 1.70299411e-01\n"," 7.62926996e-01 9.27796304e-01 9.77071345e-01 1.10749568e-06\n"," 1.12619011e-04 5.85074129e-04 2.14310035e-07 3.05026879e-05\n"," 8.68525445e-01 1.08466738e-08 1.94692911e-04 1.11387316e-02\n"," 9.99936461e-01 9.99885082e-01 9.99947309e-01 9.99975562e-01\n"," 9.99011755e-01 9.99202788e-01 9.99902129e-01 9.99973893e-01\n"," 9.99888301e-01 9.98974323e-01 9.97678936e-01 9.89027381e-01\n"," 9.99927759e-01 9.99892116e-01 9.99245644e-01 9.99859929e-01\n"," 9.99977350e-01 9.99844670e-01 9.99830723e-01 9.98375177e-01\n"," 9.99719441e-01 9.99338567e-01 9.99849200e-01 9.91891503e-01\n"," 9.87233281e-01 9.99630690e-01 9.92598653e-01 9.94325936e-01\n"," 1.87792070e-02 9.87899899e-01 9.75415111e-01 9.77589667e-01\n"," 7.24698365e-01 9.99785364e-01 9.97357190e-01 9.99707282e-01\n"," 3.05498362e-01 9.94005024e-01 9.07880127e-01 9.99702632e-01\n"," 4.84573245e-01 9.99788105e-01 9.32912435e-03 9.59697902e-01\n"," 9.65788186e-01 9.69972551e-01 9.54020619e-01 2.01553348e-02\n"," 4.56641382e-03 2.35547200e-02 2.90856630e-01 4.83916342e-01\n"," 9.96597111e-01 9.97310042e-01 7.71929502e-01 7.12441683e-01\n"," 9.99867797e-01 9.99817789e-01]\n","predict [0. 1. 0. 1. 0. 1. 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1.\n"," 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0.\n"," 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1.\n"," 0. 1. 1. 1. 0. 1. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1.]\n","Train Epoch: 42 [0/107 (0%)]\tTrain Loss: 0.023094\n","Train Epoch: 42 [4/107 (4%)]\tTrain Loss: 0.000524\n","Train Epoch: 42 [8/107 (7%)]\tTrain Loss: 0.001068\n","Train Epoch: 42 [12/107 (11%)]\tTrain Loss: 0.000087\n","Train Epoch: 42 [16/107 (15%)]\tTrain Loss: 0.005641\n","Train Epoch: 42 [20/107 (19%)]\tTrain Loss: 0.000632\n","Train Epoch: 42 [24/107 (22%)]\tTrain Loss: 0.000708\n","Train Epoch: 42 [28/107 (26%)]\tTrain Loss: 0.016412\n","Train Epoch: 42 [32/107 (30%)]\tTrain Loss: 0.001431\n","Train Epoch: 42 [36/107 (34%)]\tTrain Loss: 0.001906\n","Train Epoch: 42 [40/107 (37%)]\tTrain Loss: 0.001126\n","Train Epoch: 42 [44/107 (41%)]\tTrain Loss: 0.002428\n","Train Epoch: 42 [48/107 (45%)]\tTrain Loss: 0.029846\n","Train Epoch: 42 [52/107 (49%)]\tTrain Loss: 0.012250\n","Train Epoch: 42 [56/107 (52%)]\tTrain Loss: 0.002128\n","Train Epoch: 42 [60/107 (56%)]\tTrain Loss: 0.032762\n","Train Epoch: 42 [64/107 (60%)]\tTrain Loss: 0.012429\n","Train Epoch: 42 [68/107 (64%)]\tTrain Loss: 0.004118\n","Train Epoch: 42 [72/107 (67%)]\tTrain Loss: 0.014272\n","Train Epoch: 42 [76/107 (71%)]\tTrain Loss: 0.003848\n","Train Epoch: 42 [80/107 (75%)]\tTrain Loss: 0.001045\n","Train Epoch: 42 [84/107 (79%)]\tTrain Loss: 0.007830\n","Train Epoch: 42 [88/107 (82%)]\tTrain Loss: 0.013890\n","Train Epoch: 42 [92/107 (86%)]\tTrain Loss: 0.014614\n","Train Epoch: 42 [96/107 (90%)]\tTrain Loss: 0.008002\n","Train Epoch: 42 [100/107 (93%)]\tTrain Loss: 0.014161\n","Train Epoch: 42 [104/107 (97%)]\tTrain Loss: 0.000727\n","target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n","score [2.84140781e-02 6.31652653e-01 1.16115855e-03 8.62765834e-02\n"," 2.15435727e-03 2.89985817e-02 3.19220245e-01 1.39040589e-01\n"," 4.12239088e-03 4.23336059e-01 3.57923776e-01 8.39376599e-02\n"," 9.81363002e-03 7.74517503e-06 6.40926955e-05 2.76663087e-07\n"," 2.45808369e-05 1.31519129e-02 8.28093471e-05 1.47122785e-03\n"," 9.69504064e-04 3.17853540e-02 1.03388347e-01 9.87446725e-01\n"," 3.17458883e-02 8.24530516e-03 6.49621427e-01 1.61096489e-06\n"," 2.32723460e-06 2.13415228e-06 4.38436051e-04 2.51883961e-04\n"," 3.27056507e-03 8.55431097e-07 1.25283872e-08 5.23612835e-06\n"," 1.72211696e-06 3.72837903e-03 2.15031905e-04 1.06338985e-06\n"," 2.31778517e-06 8.21973026e-06 7.65293539e-02 7.51693879e-05\n"," 3.76050696e-02 5.30347936e-02 1.94733217e-02 2.83296872e-03\n"," 2.31462717e-01 6.97500587e-01 8.38068485e-01 8.10976144e-06\n"," 8.18922308e-06 2.04684344e-04 1.57349518e-06 5.49639662e-05\n"," 5.42134941e-01 7.80968250e-08 7.06118517e-05 3.63048593e-06\n"," 9.99219775e-01 9.93025780e-01 9.99336541e-01 9.99046504e-01\n"," 2.16160834e-01 3.58804949e-02 2.83048362e-01 9.70511436e-01\n"," 9.67392981e-01 9.92876172e-01 5.82714491e-02 1.04025258e-02\n"," 9.65568841e-01 9.94457006e-01 7.92823374e-01 9.93941307e-01\n"," 9.92799997e-01 9.81120408e-01 5.73795676e-01 4.78935838e-01\n"," 9.57334459e-01 9.55917358e-01 9.86990154e-01 9.52977777e-01\n"," 7.51025558e-01 9.80750024e-01 1.40875489e-01 9.95781869e-02\n"," 1.83941715e-03 1.49123818e-02 9.61420059e-01 8.13348413e-01\n"," 3.29278678e-01 9.97729242e-01 9.62337673e-01 9.96760547e-01\n"," 1.12193644e-01 2.27214307e-01 3.52759175e-02 5.18547893e-02\n"," 1.29981205e-01 9.93750393e-01 1.10748988e-02 8.36257100e-01\n"," 7.04055548e-01 9.56257582e-01 9.81474638e-01 3.39893624e-03\n"," 8.86281836e-04 7.79183931e-04 1.39570385e-02 2.22088210e-03\n"," 9.84081864e-01 2.30861649e-01 4.44227969e-03 7.12841749e-03\n"," 9.87091303e-01 8.24111998e-01]\n","predict [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n"," 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 1. 1. 1. 0. 0.\n"," 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 1. 1. 0. 1. 1. 1.\n"," 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1.]\n","Train Epoch: 43 [0/107 (0%)]\tTrain Loss: 0.001281\n","Train Epoch: 43 [4/107 (4%)]\tTrain Loss: 0.004574\n","Train Epoch: 43 [8/107 (7%)]\tTrain Loss: 0.004444\n","Train Epoch: 43 [12/107 (11%)]\tTrain Loss: 0.015640\n","Train Epoch: 43 [16/107 (15%)]\tTrain Loss: 0.000718\n","Train Epoch: 43 [20/107 (19%)]\tTrain Loss: 0.003353\n","Train Epoch: 43 [24/107 (22%)]\tTrain Loss: 0.014709\n","Train Epoch: 43 [28/107 (26%)]\tTrain Loss: 0.027389\n","Train Epoch: 43 [32/107 (30%)]\tTrain Loss: 0.001275\n","Train Epoch: 43 [36/107 (34%)]\tTrain Loss: 0.000704\n","Train Epoch: 43 [40/107 (37%)]\tTrain Loss: 0.005502\n","Train Epoch: 43 [44/107 (41%)]\tTrain Loss: 0.019723\n","Train Epoch: 43 [48/107 (45%)]\tTrain Loss: 0.005408\n","Train Epoch: 43 [52/107 (49%)]\tTrain Loss: 0.010391\n","Train Epoch: 43 [56/107 (52%)]\tTrain Loss: 0.070580\n","Train Epoch: 43 [60/107 (56%)]\tTrain Loss: 0.103313\n","Train Epoch: 43 [64/107 (60%)]\tTrain Loss: 0.005610\n","Train Epoch: 43 [68/107 (64%)]\tTrain Loss: 0.019427\n","Train Epoch: 43 [72/107 (67%)]\tTrain Loss: 0.068959\n","Train Epoch: 43 [76/107 (71%)]\tTrain Loss: 0.004981\n","Train Epoch: 43 [80/107 (75%)]\tTrain Loss: 0.052374\n","Train Epoch: 43 [84/107 (79%)]\tTrain Loss: 0.000260\n","Train Epoch: 43 [88/107 (82%)]\tTrain Loss: 0.017100\n","Train Epoch: 43 [92/107 (86%)]\tTrain Loss: 0.001756\n","Train Epoch: 43 [96/107 (90%)]\tTrain Loss: 0.011976\n","Train Epoch: 43 [100/107 (93%)]\tTrain Loss: 0.006771\n","Train Epoch: 43 [104/107 (97%)]\tTrain Loss: 0.001518\n","target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n","score [7.49523845e-03 9.08809125e-01 3.59001428e-01 8.81038785e-01\n"," 5.48426099e-02 6.08922422e-01 9.58100855e-01 9.69876885e-01\n"," 2.82405373e-02 2.57910460e-01 6.77221537e-01 5.74239120e-02\n"," 3.70428234e-01 2.18363497e-02 5.43993674e-02 1.85670413e-03\n"," 7.19681056e-03 7.33540714e-01 1.05082905e-02 1.74458660e-02\n"," 1.72062367e-02 9.83535409e-01 3.33532870e-01 9.99096751e-01\n"," 6.76057100e-01 5.03680229e-01 9.93833899e-01 3.84057942e-03\n"," 3.20407003e-03 1.92666776e-04 2.90181790e-03 9.86662693e-03\n"," 6.17898285e-01 2.50731217e-04 4.00159741e-04 8.54503829e-04\n"," 1.82507175e-03 8.67442489e-01 2.90430307e-01 2.52358266e-04\n"," 3.45076493e-04 1.99674908e-03 9.40443873e-01 4.51281428e-01\n"," 2.93987811e-01 1.14032909e-01 5.76398790e-01 2.12091640e-01\n"," 6.05641127e-01 8.09976339e-01 8.12788725e-01 3.97223312e-06\n"," 1.08246400e-04 2.54823896e-03 3.22243898e-09 1.48462958e-03\n"," 1.30349919e-01 1.78162551e-07 1.28841653e-04 3.05919413e-04\n"," 9.99147296e-01 9.86759126e-01 9.98673677e-01 9.97445583e-01\n"," 9.87134039e-01 9.97435510e-01 9.81432498e-01 9.95430708e-01\n"," 9.95461524e-01 9.83412147e-01 4.80355561e-01 1.03075460e-01\n"," 8.28197598e-01 9.96356785e-01 9.19421732e-01 9.98117805e-01\n"," 9.96969640e-01 9.92257714e-01 9.77935195e-01 9.81677055e-01\n"," 9.82179582e-01 9.48315442e-01 9.87381339e-01 9.77662385e-01\n"," 9.68168378e-01 9.82318521e-01 4.97223526e-01 6.68822587e-01\n"," 2.47634307e-01 4.59812544e-02 9.18031871e-01 6.75786793e-01\n"," 5.50341606e-01 9.97230470e-01 7.74550200e-01 9.95287657e-01\n"," 8.52336764e-01 5.96165538e-01 9.38428193e-02 8.55228603e-01\n"," 6.89235926e-01 9.96719301e-01 2.74209976e-01 8.68521035e-02\n"," 6.62461221e-02 3.46682996e-01 5.09813070e-01 2.06866562e-01\n"," 1.34832161e-02 6.07592473e-03 4.91589367e-01 9.51516256e-02\n"," 9.80057478e-01 7.88112879e-01 8.29085931e-02 8.65171552e-02\n"," 9.86575902e-01 9.99891520e-01]\n","predict [0. 1. 0. 1. 0. 1. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1.\n"," 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0.\n"," 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0. 0. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 0. 1. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 1.]\n","Train Epoch: 44 [0/107 (0%)]\tTrain Loss: 0.006301\n","Train Epoch: 44 [4/107 (4%)]\tTrain Loss: 0.046555\n","Train Epoch: 44 [8/107 (7%)]\tTrain Loss: 0.005634\n","Train Epoch: 44 [12/107 (11%)]\tTrain Loss: 0.005532\n","Train Epoch: 44 [16/107 (15%)]\tTrain Loss: 0.100344\n","Train Epoch: 44 [20/107 (19%)]\tTrain Loss: 0.000559\n","Train Epoch: 44 [24/107 (22%)]\tTrain Loss: 0.005359\n","Train Epoch: 44 [28/107 (26%)]\tTrain Loss: 0.011486\n","Train Epoch: 44 [32/107 (30%)]\tTrain Loss: 0.002256\n","Train Epoch: 44 [36/107 (34%)]\tTrain Loss: 0.000369\n","Train Epoch: 44 [40/107 (37%)]\tTrain Loss: 0.004397\n","Train Epoch: 44 [44/107 (41%)]\tTrain Loss: 0.000889\n","Train Epoch: 44 [48/107 (45%)]\tTrain Loss: 0.041703\n","Train Epoch: 44 [52/107 (49%)]\tTrain Loss: 0.001271\n","Train Epoch: 44 [56/107 (52%)]\tTrain Loss: 0.002130\n","Train Epoch: 44 [60/107 (56%)]\tTrain Loss: 0.002005\n","Train Epoch: 44 [64/107 (60%)]\tTrain Loss: 0.007475\n","Train Epoch: 44 [68/107 (64%)]\tTrain Loss: 0.000267\n","Train Epoch: 44 [72/107 (67%)]\tTrain Loss: 0.001180\n","Train Epoch: 44 [76/107 (71%)]\tTrain Loss: 0.001644\n","Train Epoch: 44 [80/107 (75%)]\tTrain Loss: 0.002438\n","Train Epoch: 44 [84/107 (79%)]\tTrain Loss: 0.002864\n","Train Epoch: 44 [88/107 (82%)]\tTrain Loss: 0.000514\n","Train Epoch: 44 [92/107 (86%)]\tTrain Loss: 0.001822\n","Train Epoch: 44 [96/107 (90%)]\tTrain Loss: 0.003392\n","Train Epoch: 44 [100/107 (93%)]\tTrain Loss: 0.324537\n","Train Epoch: 44 [104/107 (97%)]\tTrain Loss: 0.000266\n","target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n","score [2.63927039e-03 9.87846196e-01 6.91236973e-01 9.68137145e-01\n"," 5.00786491e-02 1.69945061e-01 9.91167903e-01 9.92214739e-01\n"," 5.57194091e-03 8.84205043e-01 9.59145308e-01 1.03796698e-01\n"," 8.55347216e-01 2.35328361e-01 7.18996882e-01 8.87241564e-04\n"," 1.18301273e-03 8.05752039e-01 5.09917736e-03 4.88673858e-02\n"," 5.46611063e-02 9.98755574e-01 9.55953538e-01 9.99751985e-01\n"," 9.92578685e-01 2.77856648e-01 9.29160118e-01 1.96728623e-03\n"," 7.60417990e-03 9.46254935e-04 1.49160773e-02 6.05988503e-01\n"," 9.63700175e-01 7.04330741e-05 7.46992591e-05 1.02661585e-03\n"," 6.49123453e-03 9.57351446e-01 5.58927059e-01 1.04649726e-03\n"," 4.18117922e-03 2.40121186e-02 9.91956711e-01 9.78616774e-01\n"," 9.66253161e-01 5.84683001e-01 9.96159077e-01 9.82029021e-01\n"," 9.94086027e-01 9.89892006e-01 9.99100804e-01 6.61230879e-04\n"," 6.21051586e-04 2.37754136e-02 2.81236055e-07 3.32676917e-02\n"," 9.90976930e-01 6.67739732e-06 2.09882113e-04 1.35932828e-03\n"," 9.99995232e-01 9.99922991e-01 9.99993682e-01 9.99997139e-01\n"," 9.99953508e-01 9.99956608e-01 9.99662876e-01 9.99926925e-01\n"," 9.99931693e-01 9.97757733e-01 8.34478259e-01 6.28156960e-01\n"," 9.99919295e-01 9.99861836e-01 9.99697447e-01 9.99989629e-01\n"," 9.99882698e-01 9.99448597e-01 9.99969959e-01 9.99910116e-01\n"," 9.99945998e-01 9.99691367e-01 9.99992251e-01 9.95444775e-01\n"," 9.98805285e-01 9.99859810e-01 9.97107327e-01 9.99868631e-01\n"," 6.30712032e-01 4.09136236e-01 9.83645201e-01 9.79824781e-01\n"," 9.19176996e-01 9.99860406e-01 9.90632176e-01 9.99246359e-01\n"," 5.82537293e-01 4.43381727e-01 4.12984282e-01 9.92716908e-01\n"," 4.88882631e-01 9.99980927e-01 7.44580150e-01 8.34283650e-01\n"," 5.96950114e-01 8.54253173e-01 9.68170166e-01 5.02091825e-01\n"," 2.15923619e-02 6.61657425e-03 7.18150198e-01 1.14083346e-02\n"," 9.97621357e-01 9.98348713e-01 9.92269516e-01 9.73471463e-01\n"," 9.98201489e-01 9.99869943e-01]\n","predict [0. 1. 1. 1. 0. 0. 1. 1. 0. 1. 1. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 1. 1. 1.\n"," 1. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1.\n"," 1. 0. 0. 1. 0. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 0. 1. 1. 1. 1. 1. 1.]\n","Train Epoch: 45 [0/107 (0%)]\tTrain Loss: 0.001163\n","Train Epoch: 45 [4/107 (4%)]\tTrain Loss: 0.000269\n","Train Epoch: 45 [8/107 (7%)]\tTrain Loss: 0.008594\n","Train Epoch: 45 [12/107 (11%)]\tTrain Loss: 0.013847\n","Train Epoch: 45 [16/107 (15%)]\tTrain Loss: 0.010512\n","Train Epoch: 45 [20/107 (19%)]\tTrain Loss: 0.003001\n","Train Epoch: 45 [24/107 (22%)]\tTrain Loss: 0.018956\n","Train Epoch: 45 [28/107 (26%)]\tTrain Loss: 0.014075\n","Train Epoch: 45 [32/107 (30%)]\tTrain Loss: 0.002400\n","Train Epoch: 45 [36/107 (34%)]\tTrain Loss: 0.003235\n","Train Epoch: 45 [40/107 (37%)]\tTrain Loss: 0.002294\n","Train Epoch: 45 [44/107 (41%)]\tTrain Loss: 0.001782\n","Train Epoch: 45 [48/107 (45%)]\tTrain Loss: 0.001664\n","Train Epoch: 45 [52/107 (49%)]\tTrain Loss: 0.004863\n","Train Epoch: 45 [56/107 (52%)]\tTrain Loss: 0.010250\n","Train Epoch: 45 [60/107 (56%)]\tTrain Loss: 0.012574\n","Train Epoch: 45 [64/107 (60%)]\tTrain Loss: 0.001934\n","Train Epoch: 45 [68/107 (64%)]\tTrain Loss: 0.005193\n","Train Epoch: 45 [72/107 (67%)]\tTrain Loss: 0.001225\n","Train Epoch: 45 [76/107 (71%)]\tTrain Loss: 0.000303\n","Train Epoch: 45 [80/107 (75%)]\tTrain Loss: 0.000860\n","Train Epoch: 45 [84/107 (79%)]\tTrain Loss: 0.000311\n","Train Epoch: 45 [88/107 (82%)]\tTrain Loss: 0.008262\n","Train Epoch: 45 [92/107 (86%)]\tTrain Loss: 0.007970\n","Train Epoch: 45 [96/107 (90%)]\tTrain Loss: 0.008440\n","Train Epoch: 45 [100/107 (93%)]\tTrain Loss: 0.016815\n","Train Epoch: 45 [104/107 (97%)]\tTrain Loss: 0.006821\n","target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n","score [9.88958031e-03 4.64461535e-01 1.67356688e-03 6.66646957e-01\n"," 5.21037459e-01 1.54369744e-02 4.42475855e-01 5.99022329e-01\n"," 4.74242354e-03 8.30158770e-01 5.42475939e-01 6.80697620e-01\n"," 3.40457529e-01 9.16795492e-01 7.71930993e-01 1.62576081e-03\n"," 3.90648767e-02 2.09341407e-01 5.75235346e-03 2.72704512e-01\n"," 6.17291927e-01 9.97878551e-01 9.80410278e-01 9.97216225e-01\n"," 9.94386077e-01 8.22609425e-01 9.90386724e-01 8.81288259e-04\n"," 1.88154716e-03 3.01972439e-04 2.99828291e-01 7.21806467e-01\n"," 3.01847905e-01 1.64428763e-02 4.14392632e-03 3.15237753e-02\n"," 3.74089438e-03 1.57276750e-01 8.08127463e-01 3.62265797e-04\n"," 1.45943079e-03 1.08972535e-01 7.49281764e-01 2.25542769e-01\n"," 4.48922724e-01 8.36293638e-01 2.48114258e-01 1.82451695e-01\n"," 2.18295068e-01 8.70679259e-01 4.35206532e-01 3.87741102e-06\n"," 1.95481829e-04 7.26064784e-04 4.83869194e-08 1.08749943e-03\n"," 7.06804037e-01 1.14845477e-07 4.61559830e-05 2.21731584e-03\n"," 9.99497533e-01 9.98296201e-01 9.99753654e-01 9.99890089e-01\n"," 9.98481572e-01 9.98505831e-01 9.98780310e-01 9.99592483e-01\n"," 9.98330414e-01 9.97980773e-01 7.60867953e-01 6.29468322e-01\n"," 9.99494314e-01 9.99797285e-01 9.99618292e-01 9.96266067e-01\n"," 9.98023391e-01 9.91997182e-01 9.95468974e-01 9.94147778e-01\n"," 9.97949183e-01 9.98314619e-01 9.95917261e-01 9.96377528e-01\n"," 9.95601177e-01 9.99740303e-01 9.94264543e-01 9.86982048e-01\n"," 8.04497600e-01 9.98326480e-01 9.96382236e-01 9.60277081e-01\n"," 9.51040626e-01 9.99873996e-01 9.98117447e-01 9.99865651e-01\n"," 2.28901744e-01 9.98916626e-01 9.92084026e-01 9.99837875e-01\n"," 9.89005685e-01 9.99674678e-01 9.82933164e-01 9.64394569e-01\n"," 7.89759636e-01 9.80689228e-01 9.30184960e-01 7.45001554e-01\n"," 4.25901890e-01 3.44139002e-02 7.66156614e-01 1.25217587e-01\n"," 9.96633947e-01 9.93521035e-01 9.99170303e-01 9.97292936e-01\n"," 9.99715388e-01 9.99985695e-01]\n","predict [0. 0. 0. 1. 1. 0. 0. 1. 0. 1. 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1.\n"," 1. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0. 0.\n"," 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 0. 1. 1. 1. 1. 1. 1.]\n","Train Epoch: 46 [0/107 (0%)]\tTrain Loss: 0.001560\n","Train Epoch: 46 [4/107 (4%)]\tTrain Loss: 0.006920\n","Train Epoch: 46 [8/107 (7%)]\tTrain Loss: 0.046517\n","Train Epoch: 46 [12/107 (11%)]\tTrain Loss: 0.084736\n","Train Epoch: 46 [16/107 (15%)]\tTrain Loss: 0.002213\n","Train Epoch: 46 [20/107 (19%)]\tTrain Loss: 0.100733\n","Train Epoch: 46 [24/107 (22%)]\tTrain Loss: 0.002186\n","Train Epoch: 46 [28/107 (26%)]\tTrain Loss: 0.002350\n","Train Epoch: 46 [32/107 (30%)]\tTrain Loss: 0.020516\n","Train Epoch: 46 [36/107 (34%)]\tTrain Loss: 0.004506\n","Train Epoch: 46 [40/107 (37%)]\tTrain Loss: 0.000556\n","Train Epoch: 46 [44/107 (41%)]\tTrain Loss: 0.005532\n","Train Epoch: 46 [48/107 (45%)]\tTrain Loss: 0.025802\n","Train Epoch: 46 [52/107 (49%)]\tTrain Loss: 0.002679\n","Train Epoch: 46 [56/107 (52%)]\tTrain Loss: 0.045274\n","Train Epoch: 46 [60/107 (56%)]\tTrain Loss: 0.002513\n","Train Epoch: 46 [64/107 (60%)]\tTrain Loss: 0.042698\n","Train Epoch: 46 [68/107 (64%)]\tTrain Loss: 0.005057\n","Train Epoch: 46 [72/107 (67%)]\tTrain Loss: 0.007226\n","Train Epoch: 46 [76/107 (71%)]\tTrain Loss: 0.001405\n","Train Epoch: 46 [80/107 (75%)]\tTrain Loss: 0.145071\n","Train Epoch: 46 [84/107 (79%)]\tTrain Loss: 0.006135\n","Train Epoch: 46 [88/107 (82%)]\tTrain Loss: 0.002761\n","Train Epoch: 46 [92/107 (86%)]\tTrain Loss: 0.027407\n","Train Epoch: 46 [96/107 (90%)]\tTrain Loss: 0.000661\n","Train Epoch: 46 [100/107 (93%)]\tTrain Loss: 0.008976\n","Train Epoch: 46 [104/107 (97%)]\tTrain Loss: 0.000258\n","target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n","score [1.48099186e-02 9.33576405e-01 5.04683793e-01 9.92115796e-01\n"," 1.11871526e-01 5.43313101e-02 9.73239124e-01 9.82734382e-01\n"," 1.36602912e-02 2.65642852e-01 5.42645633e-01 1.66751798e-02\n"," 2.00143635e-01 8.27757001e-01 8.15364778e-01 5.66651142e-05\n"," 1.18580041e-02 6.17802382e-01 1.27052441e-02 5.40257454e-01\n"," 5.67541718e-01 9.99841690e-01 9.81748641e-01 9.99999762e-01\n"," 9.94166970e-01 8.42116952e-01 9.99869466e-01 1.05259024e-01\n"," 7.76545554e-02 1.40137738e-02 7.62698472e-01 3.24229747e-01\n"," 2.74668723e-01 4.06562118e-04 7.51631596e-05 2.43079895e-03\n"," 8.21527373e-03 6.30090177e-01 4.67676250e-03 2.13222684e-05\n"," 1.37924490e-05 4.00668010e-04 9.76871431e-01 3.98262180e-02\n"," 9.63819325e-01 9.31605175e-02 4.63537499e-02 5.55770174e-02\n"," 3.02122146e-01 5.28968692e-01 6.93827748e-01 3.25058063e-05\n"," 4.18393165e-02 4.09512036e-03 2.94796769e-06 8.33269283e-02\n"," 7.88482428e-01 1.14021299e-04 3.29473725e-04 6.22493982e-01\n"," 9.98737752e-01 9.97950494e-01 9.99444783e-01 9.99689698e-01\n"," 9.99958634e-01 9.99445498e-01 9.98539209e-01 9.99998569e-01\n"," 9.99974251e-01 9.99882936e-01 7.92572498e-01 7.68896639e-02\n"," 9.99026895e-01 9.78563130e-01 9.76622283e-01 9.97272551e-01\n"," 9.99199688e-01 9.99928355e-01 9.99908209e-01 9.79073763e-01\n"," 9.99616981e-01 9.95080352e-01 9.99083519e-01 9.99775350e-01\n"," 9.82325971e-01 9.99997139e-01 9.98685300e-01 9.97663498e-01\n"," 6.86058462e-01 9.99986053e-01 9.99893665e-01 9.88165021e-01\n"," 9.02063131e-01 9.99998808e-01 9.99849558e-01 9.99962687e-01\n"," 9.61236835e-01 7.28592277e-01 7.37847924e-01 9.99998689e-01\n"," 9.88513291e-01 9.92179275e-01 1.60658538e-01 7.43415236e-01\n"," 5.63512564e-01 6.95892572e-01 9.18462455e-01 8.23220387e-02\n"," 1.64163212e-04 2.90773138e-02 9.98659015e-01 1.45313563e-04\n"," 9.98183310e-01 9.99993563e-01 9.99372780e-01 9.31860149e-01\n"," 9.99998927e-01 1.00000000e+00]\n","predict [0. 1. 1. 1. 0. 0. 1. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0. 1. 0. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0.\n"," 0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0. 0. 0. 1. 0. 1. 1. 1. 1. 1. 1.]\n","Train Epoch: 47 [0/107 (0%)]\tTrain Loss: 0.132444\n","Train Epoch: 47 [4/107 (4%)]\tTrain Loss: 0.024896\n","Train Epoch: 47 [8/107 (7%)]\tTrain Loss: 0.000625\n","Train Epoch: 47 [12/107 (11%)]\tTrain Loss: 0.001739\n","Train Epoch: 47 [16/107 (15%)]\tTrain Loss: 0.049317\n","Train Epoch: 47 [20/107 (19%)]\tTrain Loss: 0.002787\n","Train Epoch: 47 [24/107 (22%)]\tTrain Loss: 0.000314\n","Train Epoch: 47 [28/107 (26%)]\tTrain Loss: 0.000941\n","Train Epoch: 47 [32/107 (30%)]\tTrain Loss: 0.005211\n","Train Epoch: 47 [36/107 (34%)]\tTrain Loss: 0.022762\n","Train Epoch: 47 [40/107 (37%)]\tTrain Loss: 0.018701\n","Train Epoch: 47 [44/107 (41%)]\tTrain Loss: 0.011709\n","Train Epoch: 47 [48/107 (45%)]\tTrain Loss: 0.006727\n","Train Epoch: 47 [52/107 (49%)]\tTrain Loss: 0.002370\n","Train Epoch: 47 [56/107 (52%)]\tTrain Loss: 0.005945\n","Train Epoch: 47 [60/107 (56%)]\tTrain Loss: 0.014745\n","Train Epoch: 47 [64/107 (60%)]\tTrain Loss: 0.005900\n","Train Epoch: 47 [68/107 (64%)]\tTrain Loss: 0.020275\n","Train Epoch: 47 [72/107 (67%)]\tTrain Loss: 0.001538\n","Train Epoch: 47 [76/107 (71%)]\tTrain Loss: 0.000113\n","Train Epoch: 47 [80/107 (75%)]\tTrain Loss: 0.032451\n","Train Epoch: 47 [84/107 (79%)]\tTrain Loss: 0.000039\n","Train Epoch: 47 [88/107 (82%)]\tTrain Loss: 0.000098\n","Train Epoch: 47 [92/107 (86%)]\tTrain Loss: 0.000041\n","Train Epoch: 47 [96/107 (90%)]\tTrain Loss: 0.000187\n","Train Epoch: 47 [100/107 (93%)]\tTrain Loss: 0.001613\n","Train Epoch: 47 [104/107 (97%)]\tTrain Loss: 0.006727\n","target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n","score [2.24182918e-03 7.85092533e-01 3.47475559e-02 9.60390985e-01\n"," 2.62333872e-03 3.72853249e-01 7.90917873e-01 9.29335713e-01\n"," 2.57640867e-03 6.00420654e-01 8.73643398e-01 1.90185308e-02\n"," 4.13456559e-02 4.74223518e-04 1.25620961e-02 3.19187035e-04\n"," 2.75877435e-02 8.49212646e-01 4.07920743e-05 3.48372996e-04\n"," 1.47268744e-04 9.98026431e-01 7.92692840e-01 9.99979377e-01\n"," 9.92509484e-01 1.86111733e-01 9.98337507e-01 2.49999546e-04\n"," 1.51807435e-05 1.25372526e-05 1.29006139e-03 1.76277291e-02\n"," 3.79507281e-02 2.50773955e-05 3.51235440e-06 2.91752658e-04\n"," 9.29961680e-04 9.81629610e-01 1.25974994e-02 3.03185894e-04\n"," 7.76359811e-04 5.19158132e-03 9.92786825e-01 6.88803243e-03\n"," 6.22983761e-02 2.37224326e-02 7.29813725e-02 1.88818946e-01\n"," 8.65666509e-01 9.58941460e-01 9.55334067e-01 4.29799184e-06\n"," 8.12113853e-07 8.84181136e-05 7.56623542e-10 5.32202503e-05\n"," 5.41657448e-01 1.57404489e-09 2.58116575e-06 8.01458373e-06\n"," 9.99696851e-01 9.87480700e-01 9.99995589e-01 9.99990344e-01\n"," 1.01139218e-01 9.79723394e-01 9.96542990e-01 9.99985576e-01\n"," 9.99910593e-01 9.98534322e-01 7.86617100e-01 4.70988862e-02\n"," 9.70559180e-01 9.98852730e-01 9.57594693e-01 4.05841231e-01\n"," 9.96264756e-01 9.92837131e-01 9.90400970e-01 9.98817623e-01\n"," 9.99624133e-01 9.79891121e-01 9.99779403e-01 9.99882340e-01\n"," 9.98791158e-01 9.99995828e-01 5.27592003e-01 7.02506721e-01\n"," 1.67314906e-03 9.99943376e-01 9.75396872e-01 9.75751638e-01\n"," 8.66336644e-01 9.99998331e-01 9.99599040e-01 9.99991179e-01\n"," 1.54108256e-02 9.82339442e-01 1.32658392e-01 9.99994159e-01\n"," 9.96036828e-01 9.99840260e-01 7.52703696e-02 1.79148689e-01\n"," 3.76294583e-01 6.24101996e-01 9.84400451e-01 1.21881701e-02\n"," 4.68635080e-05 1.60455253e-04 7.28759885e-01 4.35999110e-02\n"," 9.64499593e-01 9.99542356e-01 8.77298772e-01 7.77299941e-01\n"," 9.99999046e-01 9.99992490e-01]\n","predict [0. 1. 0. 1. 0. 0. 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1.\n"," 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n"," 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0.\n"," 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1.\n"," 0. 1. 0. 1. 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 1. 1. 1. 1. 1. 1.]\n","Train Epoch: 48 [0/107 (0%)]\tTrain Loss: 0.002647\n","Train Epoch: 48 [4/107 (4%)]\tTrain Loss: 0.000995\n","Train Epoch: 48 [8/107 (7%)]\tTrain Loss: 0.028713\n","Train Epoch: 48 [12/107 (11%)]\tTrain Loss: 0.002784\n","Train Epoch: 48 [16/107 (15%)]\tTrain Loss: 0.004786\n","Train Epoch: 48 [20/107 (19%)]\tTrain Loss: 0.089286\n","Train Epoch: 48 [24/107 (22%)]\tTrain Loss: 0.000188\n","Train Epoch: 48 [28/107 (26%)]\tTrain Loss: 0.016495\n","Train Epoch: 48 [32/107 (30%)]\tTrain Loss: 0.069594\n","Train Epoch: 48 [36/107 (34%)]\tTrain Loss: 0.005563\n","Train Epoch: 48 [40/107 (37%)]\tTrain Loss: 0.286163\n","Train Epoch: 48 [44/107 (41%)]\tTrain Loss: 0.003618\n","Train Epoch: 48 [48/107 (45%)]\tTrain Loss: 0.001510\n","Train Epoch: 48 [52/107 (49%)]\tTrain Loss: 0.001295\n","Train Epoch: 48 [56/107 (52%)]\tTrain Loss: 0.042731\n","Train Epoch: 48 [60/107 (56%)]\tTrain Loss: 0.012379\n","Train Epoch: 48 [64/107 (60%)]\tTrain Loss: 0.009321\n","Train Epoch: 48 [68/107 (64%)]\tTrain Loss: 0.001103\n","Train Epoch: 48 [72/107 (67%)]\tTrain Loss: 0.000548\n","Train Epoch: 48 [76/107 (71%)]\tTrain Loss: 0.000866\n","Train Epoch: 48 [80/107 (75%)]\tTrain Loss: 0.018871\n","Train Epoch: 48 [84/107 (79%)]\tTrain Loss: 0.001817\n","Train Epoch: 48 [88/107 (82%)]\tTrain Loss: 0.029476\n","Train Epoch: 48 [92/107 (86%)]\tTrain Loss: 0.007811\n","Train Epoch: 48 [96/107 (90%)]\tTrain Loss: 0.002076\n","Train Epoch: 48 [100/107 (93%)]\tTrain Loss: 0.000523\n","Train Epoch: 48 [104/107 (97%)]\tTrain Loss: 0.002933\n","target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n","score [1.68728635e-01 9.47360933e-01 2.55880076e-02 9.97541308e-01\n"," 5.10783633e-03 7.13303924e-01 9.74352717e-01 9.89572346e-01\n"," 3.94548118e-01 7.78611302e-01 8.72812212e-01 6.67349398e-02\n"," 9.63035464e-01 3.00846325e-04 2.76547149e-02 4.46489546e-04\n"," 1.89421419e-02 2.69997627e-01 1.87341213e-01 2.48219579e-01\n"," 2.46617585e-01 9.99089003e-01 9.99741971e-01 9.99938726e-01\n"," 9.93157089e-01 9.40060139e-01 9.52250957e-01 4.78853035e-04\n"," 9.32048482e-04 4.85144265e-04 1.76029839e-02 9.62210715e-01\n"," 1.00269586e-01 4.68933140e-05 5.91638777e-03 2.92834040e-04\n"," 2.23549172e-01 9.98629570e-01 6.20115161e-01 2.46861740e-03\n"," 4.43681347e-04 9.71745700e-03 9.97553647e-01 8.22036684e-01\n"," 9.51821148e-01 5.80211580e-01 9.06589448e-01 7.02316821e-01\n"," 6.38262630e-01 9.74641085e-01 8.77472281e-01 6.72970607e-04\n"," 5.15309795e-09 6.07274997e-04 6.04090005e-08 2.57520288e-01\n"," 4.56677496e-01 5.67762087e-11 2.79396307e-04 1.40237660e-06\n"," 9.99951959e-01 9.99857306e-01 9.99991059e-01 9.99987245e-01\n"," 9.99376237e-01 9.97501791e-01 9.97978270e-01 9.99977469e-01\n"," 9.99902487e-01 9.99932408e-01 8.26258719e-01 1.20642506e-01\n"," 9.98529673e-01 9.98428881e-01 9.96413648e-01 9.99943972e-01\n"," 9.99483466e-01 9.98272777e-01 9.99880552e-01 9.99489665e-01\n"," 9.99577940e-01 9.86512780e-01 9.96419907e-01 9.94539797e-01\n"," 9.93435025e-01 9.99898076e-01 9.42369223e-01 9.96725798e-01\n"," 8.66505667e-04 9.97891128e-01 1.34022951e-01 9.95525897e-01\n"," 9.33194757e-01 9.99859095e-01 9.98542547e-01 9.97560740e-01\n"," 2.42093399e-01 9.79501665e-01 9.99545276e-01 9.99048650e-01\n"," 9.65627313e-01 9.99638557e-01 9.77309287e-01 6.07894361e-01\n"," 9.62019488e-02 2.15743452e-01 9.79322135e-01 5.68867803e-01\n"," 4.51587707e-01 2.71545857e-01 9.18438733e-01 2.93595523e-01\n"," 9.99521494e-01 9.97578442e-01 8.04718554e-01 9.82200742e-01\n"," 9.99207795e-01 9.99952197e-01]\n","predict [0. 1. 0. 1. 0. 1. 1. 1. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1.\n"," 1. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0. 1. 1. 1. 1. 1.\n"," 0. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 0. 0. 1. 0. 1. 1. 1. 1. 1. 1.]\n","Train Epoch: 49 [0/107 (0%)]\tTrain Loss: 0.000466\n","Train Epoch: 49 [4/107 (4%)]\tTrain Loss: 0.000377\n","Train Epoch: 49 [8/107 (7%)]\tTrain Loss: 0.004685\n","Train Epoch: 49 [12/107 (11%)]\tTrain Loss: 0.019644\n","Train Epoch: 49 [16/107 (15%)]\tTrain Loss: 0.044196\n","Train Epoch: 49 [20/107 (19%)]\tTrain Loss: 0.000370\n","Train Epoch: 49 [24/107 (22%)]\tTrain Loss: 0.000629\n","Train Epoch: 49 [28/107 (26%)]\tTrain Loss: 0.003062\n","Train Epoch: 49 [32/107 (30%)]\tTrain Loss: 0.000285\n","Train Epoch: 49 [36/107 (34%)]\tTrain Loss: 0.006521\n","Train Epoch: 49 [40/107 (37%)]\tTrain Loss: 0.050361\n","Train Epoch: 49 [44/107 (41%)]\tTrain Loss: 0.269063\n","Train Epoch: 49 [48/107 (45%)]\tTrain Loss: 0.000598\n","Train Epoch: 49 [52/107 (49%)]\tTrain Loss: 0.005173\n","Train Epoch: 49 [56/107 (52%)]\tTrain Loss: 0.230687\n","Train Epoch: 49 [60/107 (56%)]\tTrain Loss: 0.005470\n","Train Epoch: 49 [64/107 (60%)]\tTrain Loss: 0.013191\n","Train Epoch: 49 [68/107 (64%)]\tTrain Loss: 0.011284\n","Train Epoch: 49 [72/107 (67%)]\tTrain Loss: 0.007932\n","Train Epoch: 49 [76/107 (71%)]\tTrain Loss: 0.000212\n","Train Epoch: 49 [80/107 (75%)]\tTrain Loss: 0.022187\n","Train Epoch: 49 [84/107 (79%)]\tTrain Loss: 0.004517\n","Train Epoch: 49 [88/107 (82%)]\tTrain Loss: 0.001249\n","Train Epoch: 49 [92/107 (86%)]\tTrain Loss: 0.001907\n","Train Epoch: 49 [96/107 (90%)]\tTrain Loss: 0.001102\n","Train Epoch: 49 [100/107 (93%)]\tTrain Loss: 0.022020\n","Train Epoch: 49 [104/107 (97%)]\tTrain Loss: 0.000760\n","target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n","score [4.49982006e-03 4.91403043e-02 2.49499269e-03 9.27072585e-01\n"," 1.25756506e-02 5.93441248e-01 7.09609330e-01 3.29393715e-01\n"," 5.07518873e-02 2.26816699e-01 1.62269939e-02 1.43945264e-02\n"," 9.82135683e-02 5.71755867e-04 3.52586759e-03 4.53845496e-05\n"," 1.39415571e-02 1.06473360e-02 1.01457094e-03 3.71689792e-03\n"," 4.27348576e-02 9.99300957e-01 9.90368664e-01 9.99951124e-01\n"," 9.95374262e-01 8.74995589e-01 9.99563396e-01 1.07001336e-02\n"," 1.55635353e-04 1.02980121e-03 6.57485604e-01 2.75023520e-01\n"," 5.36673004e-03 6.53618326e-06 2.69232005e-05 5.60279987e-05\n"," 5.56446939e-05 9.98017907e-01 8.05349555e-03 2.54443348e-05\n"," 3.05574104e-05 2.53263488e-03 9.99523878e-01 1.22889601e-01\n"," 5.23932338e-01 1.10383026e-01 3.14524591e-01 4.78690527e-02\n"," 3.54335874e-01 9.73860681e-01 8.15097034e-01 1.54725480e-04\n"," 7.12720839e-06 7.32523971e-04 5.96438531e-06 1.90270180e-03\n"," 6.41862094e-01 3.29431003e-07 2.03475138e-04 4.90818320e-06\n"," 9.96091664e-01 9.91593838e-01 9.98066843e-01 9.85401750e-01\n"," 9.98359740e-01 8.98403645e-01 9.98189867e-01 9.99987483e-01\n"," 9.99668121e-01 9.99981046e-01 9.16293979e-01 4.61591482e-01\n"," 9.09603179e-01 9.98963475e-01 9.99637127e-01 9.95102167e-01\n"," 9.78140116e-01 9.91188884e-01 9.99944210e-01 9.78508532e-01\n"," 9.98237848e-01 9.19341743e-01 9.99809921e-01 8.93925130e-01\n"," 1.30061939e-01 9.99016404e-01 9.93305147e-01 9.89037335e-01\n"," 4.32394881e-04 8.44347358e-01 9.04057562e-01 9.84537244e-01\n"," 3.78300875e-01 9.98047709e-01 9.48246241e-01 9.88237679e-01\n"," 3.24682981e-01 9.25963819e-01 6.81634426e-01 8.36196899e-01\n"," 7.87685931e-01 9.57980275e-01 3.60502899e-02 2.87489355e-01\n"," 1.74223915e-01 6.83780611e-02 9.57150757e-01 9.02536809e-02\n"," 6.13706186e-02 8.64907354e-02 9.52256680e-01 7.80756250e-02\n"," 9.99624491e-01 9.98853207e-01 1.88516825e-02 4.28325713e-01\n"," 9.99961734e-01 9.94865239e-01]\n","predict [0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1.\n"," 1. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0.\n"," 0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1.\n"," 0. 1. 1. 1. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 1. 0. 0. 1. 1.]\n","Train Epoch: 50 [0/107 (0%)]\tTrain Loss: 0.020832\n","Train Epoch: 50 [4/107 (4%)]\tTrain Loss: 0.000951\n","Train Epoch: 50 [8/107 (7%)]\tTrain Loss: 0.003585\n","Train Epoch: 50 [12/107 (11%)]\tTrain Loss: 0.096664\n","Train Epoch: 50 [16/107 (15%)]\tTrain Loss: 0.022817\n","Train Epoch: 50 [20/107 (19%)]\tTrain Loss: 0.002515\n","Train Epoch: 50 [24/107 (22%)]\tTrain Loss: 0.000392\n","Train Epoch: 50 [28/107 (26%)]\tTrain Loss: 0.132902\n","Train Epoch: 50 [32/107 (30%)]\tTrain Loss: 0.003253\n","Train Epoch: 50 [36/107 (34%)]\tTrain Loss: 0.005989\n","Train Epoch: 50 [40/107 (37%)]\tTrain Loss: 0.001604\n","Train Epoch: 50 [44/107 (41%)]\tTrain Loss: 0.061664\n","Train Epoch: 50 [48/107 (45%)]\tTrain Loss: 0.009156\n","Train Epoch: 50 [52/107 (49%)]\tTrain Loss: 0.003488\n","Train Epoch: 50 [56/107 (52%)]\tTrain Loss: 0.001945\n","Train Epoch: 50 [60/107 (56%)]\tTrain Loss: 0.007070\n","Train Epoch: 50 [64/107 (60%)]\tTrain Loss: 0.002143\n","Train Epoch: 50 [68/107 (64%)]\tTrain Loss: 0.104562\n","Train Epoch: 50 [72/107 (67%)]\tTrain Loss: 0.029598\n","Train Epoch: 50 [76/107 (71%)]\tTrain Loss: 0.020449\n","Train Epoch: 50 [80/107 (75%)]\tTrain Loss: 0.008957\n","Train Epoch: 50 [84/107 (79%)]\tTrain Loss: 0.049500\n","Train Epoch: 50 [88/107 (82%)]\tTrain Loss: 0.055979\n","Train Epoch: 50 [92/107 (86%)]\tTrain Loss: 0.000672\n","Train Epoch: 50 [96/107 (90%)]\tTrain Loss: 0.007488\n","Train Epoch: 50 [100/107 (93%)]\tTrain Loss: 0.000516\n","Train Epoch: 50 [104/107 (97%)]\tTrain Loss: 0.170031\n","target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n","score [5.56819178e-02 4.53033224e-02 3.51485820e-03 8.26363206e-01\n"," 3.96946222e-01 1.07353494e-01 2.88207948e-01 7.77733028e-01\n"," 3.12636085e-02 6.73323497e-02 6.53122663e-02 1.19293081e-02\n"," 1.91240996e-01 1.73368275e-01 1.28899276e-01 1.01963142e-02\n"," 9.65230525e-01 7.23797530e-02 1.26618339e-04 3.65204411e-03\n"," 4.50271182e-03 9.76342320e-01 9.19536710e-01 9.99264181e-01\n"," 8.94487262e-01 7.34387040e-01 9.92143512e-01 9.86035317e-02\n"," 2.47478863e-04 1.25866279e-03 5.46947837e-01 6.54139444e-02\n"," 4.85912897e-02 3.81852180e-04 2.38075299e-04 3.38123785e-03\n"," 5.29437792e-03 9.72880304e-01 2.10142910e-01 3.62471823e-04\n"," 7.37071619e-04 1.07160453e-02 9.53922033e-01 6.08666241e-01\n"," 8.37998390e-01 7.88131058e-02 7.16449440e-01 8.38822573e-02\n"," 8.19619894e-01 9.68110323e-01 9.50750649e-01 3.06945731e-05\n"," 7.10310087e-06 7.98060093e-03 6.88534074e-09 4.28877713e-04\n"," 2.05504194e-01 1.41693377e-08 5.30347734e-06 4.91771149e-04\n"," 9.99539852e-01 9.95816648e-01 9.99791324e-01 9.99497890e-01\n"," 9.97668207e-01 9.98913765e-01 9.99324799e-01 9.99531507e-01\n"," 9.99195993e-01 9.98662114e-01 9.70273793e-01 8.67805481e-01\n"," 9.99569356e-01 9.99540448e-01 9.98369992e-01 9.97856081e-01\n"," 9.92938221e-01 9.92318511e-01 9.99876976e-01 9.99012113e-01\n"," 9.99689102e-01 9.94295418e-01 9.99527574e-01 9.40056801e-01\n"," 7.60969222e-01 9.99542475e-01 9.95112121e-01 9.83438075e-01\n"," 3.67484358e-03 9.13416028e-01 7.81198263e-01 9.96507227e-01\n"," 8.98865938e-01 9.99567449e-01 9.56490397e-01 9.41933453e-01\n"," 2.14420874e-02 7.70331323e-01 4.38789010e-01 9.92152750e-01\n"," 3.04277569e-01 9.95999336e-01 1.35878205e-01 7.04983696e-02\n"," 1.18230067e-01 1.21022597e-01 8.74391735e-01 3.08534974e-04\n"," 1.19894103e-03 1.03543758e-01 4.87843156e-02 2.34189816e-02\n"," 9.52916503e-01 9.39955056e-01 9.25879598e-01 8.60895514e-01\n"," 9.97851133e-01 9.96766090e-01]\n","predict [0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 1.\n"," 1. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 1. 0. 1. 0.\n"," 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1.\n"," 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1.]\n","vote_pred [0. 1. 0. 1. 0. 0. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1.\n"," 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0.\n"," 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1.\n"," 0. 1. 0. 1. 1. 1. 0. 1. 0. 1. 1. 0. 0. 0. 1. 0. 1. 1. 1. 1. 1. 1.]\n","targetlist [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n","TP= 48 TN= 42 FN= 10 FP= 18\n","TP+FP 66\n","precision 0.7272727272727273\n","recall 0.8275862068965517\n","F1 0.7741935483870968\n","acc 0.7627118644067796\n","AUCp 0.7637931034482758\n","AUC 0.8658045977011494\n","\n"," The epoch is 50, average recall: 0.8276, average precision: 0.7273,average F1: 0.7742, average accuracy: 0.7627, average AUC: 0.8658\n","Train Epoch: 51 [0/107 (0%)]\tTrain Loss: 0.001733\n","Train Epoch: 51 [4/107 (4%)]\tTrain Loss: 0.014210\n","Train Epoch: 51 [8/107 (7%)]\tTrain Loss: 0.014635\n","Train Epoch: 51 [12/107 (11%)]\tTrain Loss: 0.005973\n","Train Epoch: 51 [16/107 (15%)]\tTrain Loss: 0.000255\n","Train Epoch: 51 [20/107 (19%)]\tTrain Loss: 0.003583\n","Train Epoch: 51 [24/107 (22%)]\tTrain Loss: 0.016938\n","Train Epoch: 51 [28/107 (26%)]\tTrain Loss: 0.008109\n","Train Epoch: 51 [32/107 (30%)]\tTrain Loss: 0.000839\n","Train Epoch: 51 [36/107 (34%)]\tTrain Loss: 0.000665\n","Train Epoch: 51 [40/107 (37%)]\tTrain Loss: 0.001774\n","Train Epoch: 51 [44/107 (41%)]\tTrain Loss: 0.001009\n","Train Epoch: 51 [48/107 (45%)]\tTrain Loss: 0.003616\n","Train Epoch: 51 [52/107 (49%)]\tTrain Loss: 0.000909\n","Train Epoch: 51 [56/107 (52%)]\tTrain Loss: 0.000408\n","Train Epoch: 51 [60/107 (56%)]\tTrain Loss: 0.000344\n","Train Epoch: 51 [64/107 (60%)]\tTrain Loss: 0.007741\n","Train Epoch: 51 [68/107 (64%)]\tTrain Loss: 0.000316\n","Train Epoch: 51 [72/107 (67%)]\tTrain Loss: 0.002989\n","Train Epoch: 51 [76/107 (71%)]\tTrain Loss: 0.006416\n","Train Epoch: 51 [80/107 (75%)]\tTrain Loss: 0.000244\n","Train Epoch: 51 [84/107 (79%)]\tTrain Loss: 0.000213\n","Train Epoch: 51 [88/107 (82%)]\tTrain Loss: 0.000236\n","Train Epoch: 51 [92/107 (86%)]\tTrain Loss: 0.003318\n","Train Epoch: 51 [96/107 (90%)]\tTrain Loss: 0.037478\n","Train Epoch: 51 [100/107 (93%)]\tTrain Loss: 0.003023\n","Train Epoch: 51 [104/107 (97%)]\tTrain Loss: 0.000763\n","target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n","score [4.44164611e-02 5.05869508e-01 1.70771182e-02 9.92305219e-01\n"," 7.65408039e-01 2.90468276e-01 9.41123784e-01 9.94675994e-01\n"," 1.05939023e-01 3.60175163e-01 1.53736159e-01 2.52032261e-02\n"," 2.22555757e-01 2.42784396e-01 7.11656213e-01 2.08745897e-03\n"," 4.46509749e-01 5.45350984e-02 1.23153237e-04 1.65288060e-04\n"," 8.07418674e-03 9.96329725e-01 7.82962441e-01 9.99938011e-01\n"," 9.89376783e-01 6.19114220e-01 8.31438482e-01 1.16505720e-01\n"," 5.35845757e-05 1.23120219e-04 3.51479977e-01 2.04609577e-02\n"," 2.64314972e-02 1.42996066e-06 5.60288925e-07 5.69678559e-05\n"," 6.50910777e-04 9.94598389e-01 4.22470532e-02 3.42916428e-05\n"," 1.00453079e-04 8.46336188e-05 9.95551109e-01 1.06983900e-01\n"," 9.50853109e-01 2.04208633e-03 4.98597212e-02 9.74149164e-03\n"," 6.56504631e-01 9.05949712e-01 8.17508280e-01 1.42298103e-03\n"," 3.15720655e-07 6.03642082e-04 2.56183341e-08 1.04062143e-03\n"," 8.29165697e-01 1.89794889e-07 1.60552241e-04 3.04160458e-05\n"," 9.97248232e-01 9.89179969e-01 9.99801934e-01 9.99656558e-01\n"," 9.99623537e-01 9.99807537e-01 9.99770343e-01 9.99698877e-01\n"," 9.99799669e-01 9.98883307e-01 5.57847559e-01 1.52705640e-01\n"," 9.99774635e-01 9.99450505e-01 9.99081254e-01 9.91696179e-01\n"," 9.97936606e-01 9.95660841e-01 9.99928594e-01 9.97301161e-01\n"," 9.99538183e-01 9.97437119e-01 9.99739349e-01 9.65664327e-01\n"," 8.50764632e-01 9.97287869e-01 9.95038569e-01 9.90950823e-01\n"," 5.23970090e-02 9.87990201e-01 9.19192314e-01 9.97835696e-01\n"," 9.79301095e-01 9.99994397e-01 9.99043405e-01 9.99562085e-01\n"," 1.44304663e-01 5.49020171e-01 8.08614492e-01 9.97562528e-01\n"," 8.24027240e-01 9.99365509e-01 6.93892539e-02 5.08878291e-01\n"," 4.74997401e-01 8.69880021e-01 9.94832873e-01 4.25991784e-05\n"," 5.65847149e-04 1.87343292e-04 4.31863032e-03 2.63193753e-02\n"," 9.98317242e-01 9.96349931e-01 9.49285686e-01 9.53319252e-01\n"," 9.99609172e-01 9.99298811e-01]\n","predict [0. 1. 0. 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1.\n"," 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0.\n"," 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1.\n"," 0. 1. 1. 1. 1. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1.]\n","Train Epoch: 52 [0/107 (0%)]\tTrain Loss: 0.009529\n","Train Epoch: 52 [4/107 (4%)]\tTrain Loss: 0.002042\n","Train Epoch: 52 [8/107 (7%)]\tTrain Loss: 0.000542\n","Train Epoch: 52 [12/107 (11%)]\tTrain Loss: 0.000074\n","Train Epoch: 52 [16/107 (15%)]\tTrain Loss: 0.000296\n","Train Epoch: 52 [20/107 (19%)]\tTrain Loss: 0.001444\n","Train Epoch: 52 [24/107 (22%)]\tTrain Loss: 0.000137\n","Train Epoch: 52 [28/107 (26%)]\tTrain Loss: 0.017013\n","Train Epoch: 52 [32/107 (30%)]\tTrain Loss: 0.000706\n","Train Epoch: 52 [36/107 (34%)]\tTrain Loss: 0.001784\n","Train Epoch: 52 [40/107 (37%)]\tTrain Loss: 0.000607\n","Train Epoch: 52 [44/107 (41%)]\tTrain Loss: 0.000268\n","Train Epoch: 52 [48/107 (45%)]\tTrain Loss: 0.000446\n","Train Epoch: 52 [52/107 (49%)]\tTrain Loss: 0.007040\n","Train Epoch: 52 [56/107 (52%)]\tTrain Loss: 0.001566\n","Train Epoch: 52 [60/107 (56%)]\tTrain Loss: 0.000393\n","Train Epoch: 52 [64/107 (60%)]\tTrain Loss: 0.001875\n","Train Epoch: 52 [68/107 (64%)]\tTrain Loss: 0.000650\n","Train Epoch: 52 [72/107 (67%)]\tTrain Loss: 0.001729\n","Train Epoch: 52 [76/107 (71%)]\tTrain Loss: 0.001957\n","Train Epoch: 52 [80/107 (75%)]\tTrain Loss: 0.001045\n","Train Epoch: 52 [84/107 (79%)]\tTrain Loss: 0.001527\n","Train Epoch: 52 [88/107 (82%)]\tTrain Loss: 0.015470\n","Train Epoch: 52 [92/107 (86%)]\tTrain Loss: 0.000975\n","Train Epoch: 52 [96/107 (90%)]\tTrain Loss: 0.002971\n","Train Epoch: 52 [100/107 (93%)]\tTrain Loss: 0.088989\n","Train Epoch: 52 [104/107 (97%)]\tTrain Loss: 0.001755\n","target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n","score [1.24037541e-01 3.24224383e-01 8.71337485e-03 9.84982133e-01\n"," 8.71114671e-01 6.96601987e-01 8.15608442e-01 9.77135062e-01\n"," 4.99074683e-02 9.54485655e-01 6.11536682e-01 4.33254123e-01\n"," 4.24727023e-01 5.69528416e-02 2.74046510e-01 1.77553055e-04\n"," 5.26945759e-03 2.72136152e-01 1.75640881e-02 1.15128877e-02\n"," 8.69141877e-01 9.97876525e-01 9.84050333e-01 9.99549687e-01\n"," 9.87268269e-01 9.81789052e-01 9.72304106e-01 3.22766840e-01\n"," 3.74920946e-03 3.11862845e-02 7.55901039e-01 4.68323559e-01\n"," 1.19850449e-01 3.84440937e-05 5.22328592e-05 7.03011407e-04\n"," 2.37176232e-02 9.46083486e-01 9.86123309e-02 2.55622726e-05\n"," 2.16435423e-04 7.94664811e-05 9.82516170e-01 3.99834931e-01\n"," 9.61332858e-01 1.28879085e-01 5.27033925e-01 2.24920347e-01\n"," 7.72188663e-01 9.73218918e-01 8.97960782e-01 8.07251636e-05\n"," 1.70810870e-03 8.99831019e-03 7.04576308e-08 8.15004518e-04\n"," 8.95167351e-01 1.19194443e-09 1.19184158e-04 1.72305182e-02\n"," 9.98126805e-01 9.93698597e-01 9.99555409e-01 9.99734938e-01\n"," 9.77059603e-01 9.41570222e-01 9.94288325e-01 9.95529234e-01\n"," 9.93039727e-01 9.92871225e-01 6.82278335e-01 3.38310689e-01\n"," 9.99366701e-01 9.99772966e-01 9.98520792e-01 9.98317599e-01\n"," 9.83226836e-01 9.81357455e-01 9.99305725e-01 9.84027147e-01\n"," 9.93615866e-01 9.71235514e-01 9.97091651e-01 9.55675364e-01\n"," 7.77655482e-01 9.88086343e-01 9.58198905e-01 9.83247697e-01\n"," 1.54251769e-01 9.87363398e-01 9.54044044e-01 9.97500360e-01\n"," 9.85091090e-01 9.99988437e-01 9.99400496e-01 9.99669552e-01\n"," 9.20650542e-01 9.64040577e-01 9.47440386e-01 9.91285980e-01\n"," 7.67665565e-01 9.97027338e-01 3.27325426e-02 9.20398533e-01\n"," 8.12897801e-01 9.94664669e-01 9.99834299e-01 3.10309976e-03\n"," 9.84406914e-04 8.93214485e-04 2.84592062e-02 2.25693323e-02\n"," 7.44296312e-01 9.87712324e-01 8.55473757e-01 8.33238423e-01\n"," 9.93334651e-01 9.96421695e-01]\n","predict [0. 0. 0. 1. 1. 1. 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1.\n"," 1. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0.\n"," 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1.]\n","Train Epoch: 53 [0/107 (0%)]\tTrain Loss: 0.014131\n","Train Epoch: 53 [4/107 (4%)]\tTrain Loss: 0.007729\n","Train Epoch: 53 [8/107 (7%)]\tTrain Loss: 0.017810\n","Train Epoch: 53 [12/107 (11%)]\tTrain Loss: 0.007813\n","Train Epoch: 53 [16/107 (15%)]\tTrain Loss: 0.001240\n","Train Epoch: 53 [20/107 (19%)]\tTrain Loss: 0.003873\n","Train Epoch: 53 [24/107 (22%)]\tTrain Loss: 0.029677\n","Train Epoch: 53 [28/107 (26%)]\tTrain Loss: 0.001564\n","Train Epoch: 53 [32/107 (30%)]\tTrain Loss: 0.167495\n","Train Epoch: 53 [36/107 (34%)]\tTrain Loss: 0.002332\n","Train Epoch: 53 [40/107 (37%)]\tTrain Loss: 0.003608\n","Train Epoch: 53 [44/107 (41%)]\tTrain Loss: 0.001455\n","Train Epoch: 53 [48/107 (45%)]\tTrain Loss: 0.000597\n","Train Epoch: 53 [52/107 (49%)]\tTrain Loss: 0.002546\n","Train Epoch: 53 [56/107 (52%)]\tTrain Loss: 0.056097\n","Train Epoch: 53 [60/107 (56%)]\tTrain Loss: 0.003234\n","Train Epoch: 53 [64/107 (60%)]\tTrain Loss: 0.015252\n","Train Epoch: 53 [68/107 (64%)]\tTrain Loss: 0.000823\n","Train Epoch: 53 [72/107 (67%)]\tTrain Loss: 0.046767\n","Train Epoch: 53 [76/107 (71%)]\tTrain Loss: 0.008140\n","Train Epoch: 53 [80/107 (75%)]\tTrain Loss: 0.005565\n","Train Epoch: 53 [84/107 (79%)]\tTrain Loss: 0.003694\n","Train Epoch: 53 [88/107 (82%)]\tTrain Loss: 0.004682\n","Train Epoch: 53 [92/107 (86%)]\tTrain Loss: 0.001802\n","Train Epoch: 53 [96/107 (90%)]\tTrain Loss: 0.000139\n","Train Epoch: 53 [100/107 (93%)]\tTrain Loss: 0.001180\n","Train Epoch: 53 [104/107 (97%)]\tTrain Loss: 0.003607\n","target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n","score [5.99319719e-05 9.45030451e-02 5.62589616e-03 4.00823921e-01\n"," 4.49513942e-02 3.80857438e-02 3.30619156e-01 1.15820477e-02\n"," 1.96745060e-03 6.44956622e-03 3.43063563e-01 3.61327489e-04\n"," 3.97324294e-01 8.88550561e-03 7.56093860e-02 3.06987954e-06\n"," 1.27668599e-07 3.97632457e-03 1.23779243e-02 3.34118903e-02\n"," 2.74101961e-02 9.21450078e-01 9.30988252e-01 9.91239011e-01\n"," 6.69443548e-01 7.50665426e-01 3.27428877e-01 7.23455596e-05\n"," 8.96600366e-04 1.56664047e-02 2.17317813e-03 1.06508754e-01\n"," 2.37394378e-01 2.08704108e-07 3.37869963e-08 3.34525294e-06\n"," 1.32708621e-04 6.29309192e-02 6.70980371e-04 1.17454499e-08\n"," 1.06078147e-07 1.63277343e-06 1.94390714e-01 2.59412341e-02\n"," 6.80630684e-01 6.46018516e-03 5.19199848e-01 1.34053985e-02\n"," 6.72882676e-01 2.31592640e-01 9.57083166e-01 2.68067453e-08\n"," 4.40792883e-06 9.59888042e-04 2.41556063e-11 2.26116335e-05\n"," 2.93474585e-01 2.74907236e-10 6.41456737e-08 9.88099710e-05\n"," 9.99741137e-01 9.97137904e-01 9.99826014e-01 9.99852180e-01\n"," 9.90919590e-01 9.02263582e-01 8.77572000e-01 9.99617100e-01\n"," 9.80194330e-01 9.95356381e-01 5.11139035e-01 3.24050151e-02\n"," 9.92667615e-01 9.97028530e-01 9.79045630e-01 9.98221099e-01\n"," 9.77146804e-01 9.71498072e-01 9.99602735e-01 9.13556635e-01\n"," 9.99227881e-01 9.96497333e-01 9.99579251e-01 4.02937233e-01\n"," 1.29374057e-01 8.80130470e-01 9.83741999e-01 9.75777030e-01\n"," 8.40747729e-02 2.72583794e-02 6.72835529e-01 9.90958810e-01\n"," 8.46381903e-01 9.99555290e-01 9.99219894e-01 9.99756277e-01\n"," 3.78568977e-01 4.84555364e-02 9.79705513e-01 9.70021605e-01\n"," 8.39452386e-01 9.98548567e-01 4.44508536e-04 9.65418696e-01\n"," 6.63345516e-01 8.52797270e-01 9.99278963e-01 5.10499638e-04\n"," 1.88659833e-04 2.34203253e-04 2.52861762e-03 3.10303061e-04\n"," 3.26163620e-01 8.03367436e-01 5.83646834e-01 8.17308307e-01\n"," 9.86434937e-01 9.16029036e-01]\n","predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1.\n"," 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0.\n"," 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1.\n"," 0. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1.]\n","Train Epoch: 54 [0/107 (0%)]\tTrain Loss: 0.018975\n","Train Epoch: 54 [4/107 (4%)]\tTrain Loss: 0.003814\n","Train Epoch: 54 [8/107 (7%)]\tTrain Loss: 0.000166\n","Train Epoch: 54 [12/107 (11%)]\tTrain Loss: 0.001035\n","Train Epoch: 54 [16/107 (15%)]\tTrain Loss: 0.000595\n","Train Epoch: 54 [20/107 (19%)]\tTrain Loss: 0.004932\n","Train Epoch: 54 [24/107 (22%)]\tTrain Loss: 0.002029\n","Train Epoch: 54 [28/107 (26%)]\tTrain Loss: 0.000697\n","Train Epoch: 54 [32/107 (30%)]\tTrain Loss: 0.005061\n","Train Epoch: 54 [36/107 (34%)]\tTrain Loss: 0.000610\n","Train Epoch: 54 [40/107 (37%)]\tTrain Loss: 0.006656\n","Train Epoch: 54 [44/107 (41%)]\tTrain Loss: 0.013453\n","Train Epoch: 54 [48/107 (45%)]\tTrain Loss: 0.003045\n","Train Epoch: 54 [52/107 (49%)]\tTrain Loss: 0.009460\n","Train Epoch: 54 [56/107 (52%)]\tTrain Loss: 0.000809\n","Train Epoch: 54 [60/107 (56%)]\tTrain Loss: 0.005121\n","Train Epoch: 54 [64/107 (60%)]\tTrain Loss: 0.001247\n","Train Epoch: 54 [68/107 (64%)]\tTrain Loss: 0.014984\n","Train Epoch: 54 [72/107 (67%)]\tTrain Loss: 0.000363\n","Train Epoch: 54 [76/107 (71%)]\tTrain Loss: 0.002001\n","Train Epoch: 54 [80/107 (75%)]\tTrain Loss: 0.004505\n","Train Epoch: 54 [84/107 (79%)]\tTrain Loss: 0.000184\n","Train Epoch: 54 [88/107 (82%)]\tTrain Loss: 0.005487\n","Train Epoch: 54 [92/107 (86%)]\tTrain Loss: 0.000281\n","Train Epoch: 54 [96/107 (90%)]\tTrain Loss: 0.002221\n","Train Epoch: 54 [100/107 (93%)]\tTrain Loss: 0.001883\n","Train Epoch: 54 [104/107 (97%)]\tTrain Loss: 0.004264\n","target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n","score [2.03487900e-04 6.71848774e-01 1.20356847e-02 7.10953712e-01\n"," 6.31969944e-02 2.98999883e-02 9.65367913e-01 9.69101787e-01\n"," 6.57973764e-03 7.92703748e-01 2.56941944e-01 1.25640497e-01\n"," 1.31142765e-01 1.16701938e-01 1.82816312e-01 3.71836868e-05\n"," 4.02141450e-05 3.15738678e-01 6.33534491e-02 1.03535458e-01\n"," 1.81065127e-01 9.99682546e-01 9.97221828e-01 9.99517918e-01\n"," 9.99571621e-01 9.95519757e-01 9.73137438e-01 4.04829532e-03\n"," 4.55538975e-03 9.43670329e-03 2.84331948e-01 1.38702737e-02\n"," 2.90238231e-01 5.18107390e-06 9.28568898e-06 6.97347859e-05\n"," 3.31764895e-04 9.95936155e-01 9.97072384e-02 3.53496557e-06\n"," 8.46869443e-06 5.21254879e-05 9.27568734e-01 9.41848159e-02\n"," 9.64014888e-01 6.49967883e-03 7.60938406e-01 7.81982988e-02\n"," 2.75329351e-01 2.19453454e-01 9.24629211e-01 3.84192553e-08\n"," 1.26135174e-05 5.14310435e-04 1.25304558e-10 6.01228385e-05\n"," 1.65781483e-01 1.41710599e-09 1.60356603e-06 2.93478806e-04\n"," 9.99928713e-01 9.99102116e-01 9.99934673e-01 9.99703586e-01\n"," 9.99177516e-01 9.95527685e-01 9.98197258e-01 9.99970436e-01\n"," 9.99555528e-01 9.91576493e-01 9.69724298e-01 3.64366144e-01\n"," 9.99738395e-01 9.98042464e-01 9.84488070e-01 9.99664068e-01\n"," 9.97065127e-01 9.99487162e-01 9.99946833e-01 9.97982621e-01\n"," 9.99930382e-01 9.99760926e-01 9.99625921e-01 9.43261445e-01\n"," 7.57387280e-01 9.90387619e-01 9.99246716e-01 9.97882426e-01\n"," 9.13523957e-02 9.97545302e-01 9.15068448e-01 9.85977411e-01\n"," 8.68125319e-01 9.99992609e-01 9.99973059e-01 9.99988198e-01\n"," 3.70231003e-01 8.72069478e-01 9.96613801e-01 9.98650253e-01\n"," 7.77179837e-01 9.96329129e-01 4.31216694e-03 7.65599191e-01\n"," 3.27818811e-01 7.87116528e-01 9.32186961e-01 8.73047509e-04\n"," 5.10915357e-04 6.90621266e-04 9.28898063e-03 1.08420387e-01\n"," 6.98572397e-01 9.97673333e-01 8.12647402e-01 9.90955770e-01\n"," 9.99969721e-01 9.99443591e-01]\n","predict [0. 1. 0. 1. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1.\n"," 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0.\n"," 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1.\n"," 0. 1. 1. 1. 1. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1.]\n","Train Epoch: 55 [0/107 (0%)]\tTrain Loss: 0.011825\n","Train Epoch: 55 [4/107 (4%)]\tTrain Loss: 0.000219\n","Train Epoch: 55 [8/107 (7%)]\tTrain Loss: 0.005975\n","Train Epoch: 55 [12/107 (11%)]\tTrain Loss: 0.000641\n","Train Epoch: 55 [16/107 (15%)]\tTrain Loss: 0.006462\n","Train Epoch: 55 [20/107 (19%)]\tTrain Loss: 0.000216\n","Train Epoch: 55 [24/107 (22%)]\tTrain Loss: 0.000739\n","Train Epoch: 55 [28/107 (26%)]\tTrain Loss: 0.003763\n","Train Epoch: 55 [32/107 (30%)]\tTrain Loss: 0.000892\n","Train Epoch: 55 [36/107 (34%)]\tTrain Loss: 0.000197\n","Train Epoch: 55 [40/107 (37%)]\tTrain Loss: 0.000141\n","Train Epoch: 55 [44/107 (41%)]\tTrain Loss: 0.002116\n","Train Epoch: 55 [48/107 (45%)]\tTrain Loss: 0.059071\n","Train Epoch: 55 [52/107 (49%)]\tTrain Loss: 0.000315\n","Train Epoch: 55 [56/107 (52%)]\tTrain Loss: 0.000316\n","Train Epoch: 55 [60/107 (56%)]\tTrain Loss: 0.000101\n","Train Epoch: 55 [64/107 (60%)]\tTrain Loss: 0.140494\n","Train Epoch: 55 [68/107 (64%)]\tTrain Loss: 0.002500\n","Train Epoch: 55 [72/107 (67%)]\tTrain Loss: 0.007329\n","Train Epoch: 55 [76/107 (71%)]\tTrain Loss: 0.001551\n","Train Epoch: 55 [80/107 (75%)]\tTrain Loss: 0.002318\n","Train Epoch: 55 [84/107 (79%)]\tTrain Loss: 0.002096\n","Train Epoch: 55 [88/107 (82%)]\tTrain Loss: 0.000698\n","Train Epoch: 55 [92/107 (86%)]\tTrain Loss: 0.175442\n","Train Epoch: 55 [96/107 (90%)]\tTrain Loss: 0.088284\n","Train Epoch: 55 [100/107 (93%)]\tTrain Loss: 0.365568\n","Train Epoch: 55 [104/107 (97%)]\tTrain Loss: 0.016513\n","target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n","score [3.85674532e-04 4.92288843e-02 2.49105878e-03 2.00311989e-01\n"," 2.30454747e-02 2.34290231e-02 3.78368974e-01 1.34762898e-01\n"," 1.62194471e-03 8.96329820e-01 7.44709671e-01 2.34220833e-01\n"," 1.40468538e-01 5.22353463e-02 3.28917988e-02 7.07317493e-04\n"," 1.30431503e-02 2.16510962e-03 1.27002550e-03 3.39538269e-02\n"," 1.06709385e-02 9.84525800e-01 7.42335021e-01 9.82798398e-01\n"," 9.51467752e-01 7.40463555e-01 8.62518489e-01 4.26446944e-02\n"," 1.18585565e-04 2.21260521e-03 5.54868169e-02 3.50537803e-03\n"," 3.46939526e-02 1.24191756e-05 8.43280213e-06 1.88533461e-03\n"," 9.12456866e-03 9.90201414e-01 2.60821991e-02 8.04158299e-06\n"," 8.07554170e-06 1.47280138e-04 5.00537574e-01 1.18730843e-01\n"," 3.82336527e-01 2.51573380e-02 1.05159208e-01 1.15724057e-02\n"," 1.64840873e-02 3.47994804e-01 3.68565172e-01 1.34618022e-04\n"," 2.14035012e-04 3.42656765e-03 6.39500968e-08 6.40388753e-04\n"," 8.64391327e-02 3.56247398e-07 3.10145078e-05 2.81763193e-03\n"," 9.98695314e-01 9.48789120e-01 9.95077848e-01 9.68702912e-01\n"," 7.97520995e-01 5.94012201e-01 7.30038226e-01 9.99755919e-01\n"," 9.92151856e-01 4.25283521e-01 6.30983055e-01 4.31135833e-01\n"," 9.68347788e-01 9.83689070e-01 6.74606264e-01 9.52332675e-01\n"," 9.19092238e-01 9.56030369e-01 9.96852577e-01 9.83147860e-01\n"," 9.98635352e-01 9.89328563e-01 8.98135602e-01 3.89355958e-01\n"," 2.35903770e-01 9.84691679e-01 8.40863168e-01 9.00387049e-01\n"," 6.08766405e-03 8.58869433e-01 9.65001225e-01 3.60395432e-01\n"," 3.42318207e-01 9.99865174e-01 9.99478519e-01 9.99741614e-01\n"," 1.42206416e-01 2.18878448e-01 3.97991270e-01 9.12858009e-01\n"," 2.50515401e-01 9.27341104e-01 6.27163984e-03 9.49265838e-01\n"," 5.57176471e-01 9.24181521e-01 9.25848722e-01 3.88314091e-02\n"," 2.34226808e-02 9.66948643e-03 1.89176034e-02 2.44306643e-02\n"," 4.89131004e-01 9.46114600e-01 2.36826748e-01 3.76186937e-01\n"," 9.81892943e-01 9.78961349e-01]\n","predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1.\n"," 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 0. 0. 1. 1. 1.\n"," 0. 0. 0. 1. 0. 1. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1.]\n","Train Epoch: 56 [0/107 (0%)]\tTrain Loss: 0.025490\n","Train Epoch: 56 [4/107 (4%)]\tTrain Loss: 0.003563\n","Train Epoch: 56 [8/107 (7%)]\tTrain Loss: 0.033248\n","Train Epoch: 56 [12/107 (11%)]\tTrain Loss: 0.009341\n","Train Epoch: 56 [16/107 (15%)]\tTrain Loss: 0.000740\n","Train Epoch: 56 [20/107 (19%)]\tTrain Loss: 0.004954\n","Train Epoch: 56 [24/107 (22%)]\tTrain Loss: 0.000057\n","Train Epoch: 56 [28/107 (26%)]\tTrain Loss: 0.000665\n","Train Epoch: 56 [32/107 (30%)]\tTrain Loss: 0.134027\n","Train Epoch: 56 [36/107 (34%)]\tTrain Loss: 0.001264\n","Train Epoch: 56 [40/107 (37%)]\tTrain Loss: 0.017305\n","Train Epoch: 56 [44/107 (41%)]\tTrain Loss: 0.000597\n","Train Epoch: 56 [48/107 (45%)]\tTrain Loss: 0.126309\n","Train Epoch: 56 [52/107 (49%)]\tTrain Loss: 0.002020\n","Train Epoch: 56 [56/107 (52%)]\tTrain Loss: 0.001018\n","Train Epoch: 56 [60/107 (56%)]\tTrain Loss: 0.000887\n","Train Epoch: 56 [64/107 (60%)]\tTrain Loss: 0.001255\n","Train Epoch: 56 [68/107 (64%)]\tTrain Loss: 0.001283\n","Train Epoch: 56 [72/107 (67%)]\tTrain Loss: 0.002723\n","Train Epoch: 56 [76/107 (71%)]\tTrain Loss: 0.002638\n","Train Epoch: 56 [80/107 (75%)]\tTrain Loss: 0.081490\n","Train Epoch: 56 [84/107 (79%)]\tTrain Loss: 0.004232\n","Train Epoch: 56 [88/107 (82%)]\tTrain Loss: 0.022087\n","Train Epoch: 56 [92/107 (86%)]\tTrain Loss: 0.006998\n","Train Epoch: 56 [96/107 (90%)]\tTrain Loss: 0.013911\n","Train Epoch: 56 [100/107 (93%)]\tTrain Loss: 0.006516\n","Train Epoch: 56 [104/107 (97%)]\tTrain Loss: 0.000947\n","target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n","score [2.90932087e-03 4.30702060e-01 2.74643954e-02 5.87580979e-01\n"," 5.02938628e-02 1.15058705e-01 8.44783723e-01 4.37708974e-01\n"," 1.51273133e-02 4.95321341e-02 2.43494540e-01 3.68248881e-03\n"," 1.64908290e-01 8.87183964e-01 8.67145061e-01 3.08382820e-04\n"," 1.13902235e-04 8.09001445e-04 8.73004028e-04 3.45665574e-01\n"," 2.94314861e-01 9.98524249e-01 9.89896417e-01 9.99881625e-01\n"," 9.97066081e-01 9.36886191e-01 9.76895571e-01 2.74851285e-02\n"," 8.03584917e-05 2.42386525e-03 4.17914353e-02 6.01853192e-01\n"," 4.52459216e-01 3.62951832e-05 7.95329906e-05 1.72930231e-04\n"," 8.84559471e-03 9.81181145e-01 9.20738578e-01 9.23026746e-05\n"," 1.48109772e-04 1.04351910e-02 8.81716251e-01 2.99287122e-02\n"," 7.35141516e-01 5.25909141e-02 9.31334913e-01 8.94366026e-01\n"," 8.92743587e-01 8.76019076e-02 9.88638580e-01 3.76215417e-06\n"," 4.88292317e-05 4.14215261e-03 6.91036606e-09 4.29968350e-04\n"," 3.19778442e-01 5.97669514e-09 2.65798553e-06 5.18430606e-04\n"," 9.99990702e-01 9.99553382e-01 9.99959588e-01 9.99959230e-01\n"," 9.99969482e-01 9.99750793e-01 9.96605277e-01 9.99999046e-01\n"," 9.99852777e-01 9.95096505e-01 9.85069454e-01 9.41982746e-01\n"," 9.99990463e-01 9.99938965e-01 9.96439636e-01 9.97565866e-01\n"," 9.99731004e-01 9.99731362e-01 9.99992490e-01 9.99912381e-01\n"," 9.99998569e-01 9.99981999e-01 9.99887586e-01 8.90289843e-01\n"," 8.22700560e-01 9.99803126e-01 9.91386533e-01 9.93621290e-01\n"," 2.12269947e-02 9.58841383e-01 8.62750530e-01 9.56206024e-01\n"," 9.41858828e-01 9.99900460e-01 9.97171462e-01 9.91613746e-01\n"," 4.32107113e-02 7.61390686e-01 8.89663160e-01 9.99882936e-01\n"," 9.82292175e-01 9.99961376e-01 2.58488685e-01 6.81239143e-02\n"," 1.22058891e-01 7.30786100e-02 9.67899323e-01 2.91288123e-02\n"," 1.14033241e-02 2.05923710e-03 2.20317822e-02 6.30852655e-02\n"," 4.06420201e-01 9.95887697e-01 9.47324455e-01 9.66928303e-01\n"," 9.99390483e-01 9.99650955e-01]\n","predict [0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1.\n"," 1. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 1. 0. 1. 1.\n"," 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1.\n"," 0. 1. 1. 1. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1.]\n","Train Epoch: 57 [0/107 (0%)]\tTrain Loss: 0.007885\n","Train Epoch: 57 [4/107 (4%)]\tTrain Loss: 0.008107\n","Train Epoch: 57 [8/107 (7%)]\tTrain Loss: 0.000099\n","Train Epoch: 57 [12/107 (11%)]\tTrain Loss: 0.002610\n","Train Epoch: 57 [16/107 (15%)]\tTrain Loss: 0.000249\n","Train Epoch: 57 [20/107 (19%)]\tTrain Loss: 0.001797\n","Train Epoch: 57 [24/107 (22%)]\tTrain Loss: 0.213542\n","Train Epoch: 57 [28/107 (26%)]\tTrain Loss: 0.020762\n","Train Epoch: 57 [32/107 (30%)]\tTrain Loss: 0.026834\n","Train Epoch: 57 [36/107 (34%)]\tTrain Loss: 0.072385\n","Train Epoch: 57 [40/107 (37%)]\tTrain Loss: 0.006933\n","Train Epoch: 57 [44/107 (41%)]\tTrain Loss: 0.000803\n","Train Epoch: 57 [48/107 (45%)]\tTrain Loss: 0.168615\n","Train Epoch: 57 [52/107 (49%)]\tTrain Loss: 0.000405\n","Train Epoch: 57 [56/107 (52%)]\tTrain Loss: 0.107316\n","Train Epoch: 57 [60/107 (56%)]\tTrain Loss: 0.031354\n","Train Epoch: 57 [64/107 (60%)]\tTrain Loss: 0.003505\n","Train Epoch: 57 [68/107 (64%)]\tTrain Loss: 0.008515\n","Train Epoch: 57 [72/107 (67%)]\tTrain Loss: 0.021049\n","Train Epoch: 57 [76/107 (71%)]\tTrain Loss: 0.118014\n","Train Epoch: 57 [80/107 (75%)]\tTrain Loss: 0.009178\n","Train Epoch: 57 [84/107 (79%)]\tTrain Loss: 0.001644\n","Train Epoch: 57 [88/107 (82%)]\tTrain Loss: 0.008294\n","Train Epoch: 57 [92/107 (86%)]\tTrain Loss: 0.004729\n","Train Epoch: 57 [96/107 (90%)]\tTrain Loss: 0.001940\n","Train Epoch: 57 [100/107 (93%)]\tTrain Loss: 0.018418\n","Train Epoch: 57 [104/107 (97%)]\tTrain Loss: 0.023814\n","target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n","score [6.73854768e-01 9.43061650e-01 4.00634527e-01 9.94360149e-01\n"," 5.61742961e-01 4.04358864e-01 9.72918212e-01 9.97472823e-01\n"," 1.46177843e-01 9.25665140e-01 4.91302043e-01 5.62496781e-01\n"," 4.07694995e-01 3.85760255e-02 2.89343834e-01 3.66341351e-06\n"," 4.81929034e-02 1.80390164e-01 3.65173757e-01 2.88525134e-01\n"," 2.55021036e-01 9.66336966e-01 9.96217906e-01 9.99995589e-01\n"," 9.74086523e-01 9.55776751e-01 9.98663783e-01 2.97858026e-02\n"," 2.03029625e-03 3.63949798e-02 2.19327733e-02 6.76248431e-01\n"," 3.80005948e-02 2.88712522e-06 2.79004020e-08 1.98065190e-05\n"," 1.02942425e-03 6.08409762e-01 1.60325319e-01 4.90718151e-08\n"," 5.93391967e-08 4.16373405e-05 9.48763788e-01 2.09892049e-01\n"," 9.29979742e-01 2.56976131e-02 9.43238854e-01 6.83341205e-01\n"," 5.58452368e-01 8.06527436e-01 9.81141388e-01 4.13881658e-08\n"," 7.21709104e-04 1.75544037e-03 2.54442006e-15 2.64809987e-05\n"," 8.29853211e-03 6.38103416e-12 2.34089526e-07 7.07139354e-03\n"," 9.99379873e-01 9.83204246e-01 9.91064966e-01 9.97012615e-01\n"," 9.99936104e-01 9.99741018e-01 9.99609649e-01 9.99999762e-01\n"," 9.99999166e-01 9.56329405e-01 9.30610299e-01 9.79304731e-01\n"," 9.99917626e-01 9.99808490e-01 9.96785879e-01 9.28854346e-01\n"," 9.96284068e-01 9.96589661e-01 9.99943972e-01 9.99880791e-01\n"," 9.99970317e-01 9.99992490e-01 9.99969363e-01 9.10438061e-01\n"," 8.93888593e-01 9.97851133e-01 9.63975787e-01 8.86907995e-01\n"," 9.38550949e-01 9.99751508e-01 9.98679936e-01 9.75558519e-01\n"," 8.84586751e-01 1.00000000e+00 9.99998212e-01 9.99998212e-01\n"," 8.64544988e-01 5.15024126e-01 8.82499218e-01 9.99935627e-01\n"," 6.44128978e-01 9.95045543e-01 1.14765381e-02 9.91297364e-01\n"," 9.70572114e-01 9.97994781e-01 9.99868035e-01 5.91557939e-03\n"," 1.35041133e-03 1.05727569e-03 7.93758705e-02 4.29949723e-02\n"," 9.98331010e-01 9.99926329e-01 9.92567420e-01 9.25042987e-01\n"," 9.98507798e-01 9.84901011e-01]\n","predict [1. 1. 0. 1. 1. 0. 1. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1.\n"," 1. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 1. 1.\n"," 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1.]\n","Train Epoch: 58 [0/107 (0%)]\tTrain Loss: 0.002733\n","Train Epoch: 58 [4/107 (4%)]\tTrain Loss: 0.038072\n","Train Epoch: 58 [8/107 (7%)]\tTrain Loss: 0.069938\n","Train Epoch: 58 [12/107 (11%)]\tTrain Loss: 0.014436\n","Train Epoch: 58 [16/107 (15%)]\tTrain Loss: 0.003990\n","Train Epoch: 58 [20/107 (19%)]\tTrain Loss: 0.003336\n","Train Epoch: 58 [24/107 (22%)]\tTrain Loss: 0.002831\n","Train Epoch: 58 [28/107 (26%)]\tTrain Loss: 0.000776\n","Train Epoch: 58 [32/107 (30%)]\tTrain Loss: 0.004024\n","Train Epoch: 58 [36/107 (34%)]\tTrain Loss: 0.016725\n","Train Epoch: 58 [40/107 (37%)]\tTrain Loss: 0.028563\n","Train Epoch: 58 [44/107 (41%)]\tTrain Loss: 0.007141\n","Train Epoch: 58 [48/107 (45%)]\tTrain Loss: 0.001754\n","Train Epoch: 58 [52/107 (49%)]\tTrain Loss: 0.011209\n","Train Epoch: 58 [56/107 (52%)]\tTrain Loss: 0.002022\n","Train Epoch: 58 [60/107 (56%)]\tTrain Loss: 0.000651\n","Train Epoch: 58 [64/107 (60%)]\tTrain Loss: 0.002771\n","Train Epoch: 58 [68/107 (64%)]\tTrain Loss: 0.001134\n","Train Epoch: 58 [72/107 (67%)]\tTrain Loss: 0.001917\n","Train Epoch: 58 [76/107 (71%)]\tTrain Loss: 0.018094\n","Train Epoch: 58 [80/107 (75%)]\tTrain Loss: 0.000737\n","Train Epoch: 58 [84/107 (79%)]\tTrain Loss: 0.002699\n","Train Epoch: 58 [88/107 (82%)]\tTrain Loss: 0.001331\n","Train Epoch: 58 [92/107 (86%)]\tTrain Loss: 0.001187\n","Train Epoch: 58 [96/107 (90%)]\tTrain Loss: 0.003187\n","Train Epoch: 58 [100/107 (93%)]\tTrain Loss: 0.011871\n","Train Epoch: 58 [104/107 (97%)]\tTrain Loss: 0.114686\n","target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n","score [1.40698969e-01 9.89797831e-01 8.24185386e-02 8.63002241e-01\n"," 2.81635765e-02 1.94712773e-01 9.86805797e-01 9.32188213e-01\n"," 2.49121571e-03 3.78720909e-01 3.78207028e-01 7.31312064e-03\n"," 5.19213732e-03 1.95819754e-02 1.51895899e-02 1.44562900e-05\n"," 9.59332159e-04 3.84946121e-03 2.94969231e-02 6.15402102e-01\n"," 2.46203825e-01 9.97143209e-01 9.97928619e-01 9.99987483e-01\n"," 9.97936726e-01 9.94916677e-01 9.80885088e-01 8.79928283e-03\n"," 1.42114295e-03 1.92537438e-03 3.33797745e-02 4.69760507e-01\n"," 1.80078745e-01 1.75330078e-05 6.22364678e-06 8.40779558e-06\n"," 3.39555874e-04 9.58381712e-01 1.94369137e-01 1.30432306e-06\n"," 1.77490881e-06 9.24245396e-05 9.63496387e-01 3.97623517e-02\n"," 4.65708375e-01 1.64004281e-01 9.59427118e-01 7.08468974e-01\n"," 9.54429626e-01 4.53094870e-01 9.84720469e-01 4.09263566e-06\n"," 4.51825181e-04 9.36017931e-03 4.76982620e-10 1.19698867e-04\n"," 1.81278944e-01 1.71290147e-08 5.77953699e-07 1.57065759e-03\n"," 9.99886990e-01 9.97473776e-01 9.99426365e-01 9.99882817e-01\n"," 9.98566687e-01 9.95279312e-01 9.96465564e-01 9.99959707e-01\n"," 9.99483109e-01 9.97130454e-01 9.55716133e-01 9.81064856e-01\n"," 9.99873281e-01 9.99435723e-01 8.02361012e-01 9.82458830e-01\n"," 9.98801231e-01 9.98446763e-01 9.99973178e-01 9.98597801e-01\n"," 9.99447286e-01 9.99730766e-01 9.99691963e-01 9.43449736e-01\n"," 9.51309562e-01 9.99662757e-01 9.98471320e-01 9.94186401e-01\n"," 4.95981127e-02 9.87022996e-01 8.19218338e-01 9.13679481e-01\n"," 9.28125918e-01 9.99997616e-01 9.99816120e-01 9.99059737e-01\n"," 8.53798866e-01 6.56017125e-01 9.25245404e-01 9.98699069e-01\n"," 7.37851322e-01 9.98401701e-01 7.43894186e-03 8.92207086e-01\n"," 8.50333750e-01 7.56771028e-01 9.99153852e-01 6.93286136e-02\n"," 5.36753796e-02 1.04633532e-02 7.77844265e-02 2.87197232e-02\n"," 9.79157269e-01 9.98334110e-01 8.94154429e-01 9.63602424e-01\n"," 9.99953866e-01 9.97739553e-01]\n","predict [0. 1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1.\n"," 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1.\n"," 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1.]\n","Train Epoch: 59 [0/107 (0%)]\tTrain Loss: 0.004491\n","Train Epoch: 59 [4/107 (4%)]\tTrain Loss: 0.004151\n","Train Epoch: 59 [8/107 (7%)]\tTrain Loss: 0.000614\n","Train Epoch: 59 [12/107 (11%)]\tTrain Loss: 0.000571\n","Train Epoch: 59 [16/107 (15%)]\tTrain Loss: 0.005845\n","Train Epoch: 59 [20/107 (19%)]\tTrain Loss: 0.000543\n","Train Epoch: 59 [24/107 (22%)]\tTrain Loss: 0.000913\n","Train Epoch: 59 [28/107 (26%)]\tTrain Loss: 0.000523\n","Train Epoch: 59 [32/107 (30%)]\tTrain Loss: 0.027787\n","Train Epoch: 59 [36/107 (34%)]\tTrain Loss: 0.000107\n","Train Epoch: 59 [40/107 (37%)]\tTrain Loss: 0.011492\n","Train Epoch: 59 [44/107 (41%)]\tTrain Loss: 0.001367\n","Train Epoch: 59 [48/107 (45%)]\tTrain Loss: 0.010480\n","Train Epoch: 59 [52/107 (49%)]\tTrain Loss: 0.001043\n","Train Epoch: 59 [56/107 (52%)]\tTrain Loss: 0.001257\n","Train Epoch: 59 [60/107 (56%)]\tTrain Loss: 0.002080\n","Train Epoch: 59 [64/107 (60%)]\tTrain Loss: 0.002317\n","Train Epoch: 59 [68/107 (64%)]\tTrain Loss: 0.000387\n","Train Epoch: 59 [72/107 (67%)]\tTrain Loss: 0.001007\n","Train Epoch: 59 [76/107 (71%)]\tTrain Loss: 0.003815\n","Train Epoch: 59 [80/107 (75%)]\tTrain Loss: 0.009219\n","Train Epoch: 59 [84/107 (79%)]\tTrain Loss: 0.022543\n","Train Epoch: 59 [88/107 (82%)]\tTrain Loss: 0.053141\n","Train Epoch: 59 [92/107 (86%)]\tTrain Loss: 0.000381\n","Train Epoch: 59 [96/107 (90%)]\tTrain Loss: 0.021155\n","Train Epoch: 59 [100/107 (93%)]\tTrain Loss: 0.003606\n","Train Epoch: 59 [104/107 (97%)]\tTrain Loss: 0.052228\n","target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n","score [4.36859041e-01 9.99833584e-01 9.91096258e-01 5.23559093e-01\n"," 2.91880947e-02 4.26870584e-02 9.99738038e-01 7.13782012e-01\n"," 2.70467508e-03 1.85662042e-02 1.60112921e-02 3.00152637e-02\n"," 4.19906015e-03 1.35028556e-01 1.18036717e-02 8.20101559e-05\n"," 2.45936471e-03 1.06526511e-02 2.37393216e-03 1.36700511e-01\n"," 5.55076115e-02 9.98945892e-01 9.97202873e-01 9.99952555e-01\n"," 9.98021007e-01 9.92364645e-01 9.95338321e-01 3.43131041e-03\n"," 3.93817388e-03 7.97178876e-03 3.58144119e-02 8.52063239e-01\n"," 5.10905743e-01 4.03145648e-04 1.17186473e-05 6.72057649e-05\n"," 2.26338190e-04 1.74351141e-01 1.48738205e-01 2.68833937e-05\n"," 1.34205737e-04 1.16434461e-03 1.59446418e-01 5.50155819e-04\n"," 6.61309123e-01 1.49532724e-02 9.63490188e-01 9.40134525e-01\n"," 9.86165285e-01 8.76808286e-01 9.92194295e-01 1.69849479e-09\n"," 2.02239826e-05 4.13483867e-05 7.43710865e-13 7.70016868e-08\n"," 3.34530734e-02 1.83901418e-12 7.27863281e-08 9.67797416e-04\n"," 9.99725282e-01 9.98537779e-01 9.99243259e-01 9.99753416e-01\n"," 9.99819696e-01 9.96912479e-01 9.99327779e-01 9.99980569e-01\n"," 9.99966860e-01 9.97305155e-01 9.92402077e-01 9.97281909e-01\n"," 9.99963403e-01 9.99732316e-01 8.67008209e-01 9.51408446e-01\n"," 9.99988794e-01 9.99994040e-01 9.99989986e-01 9.99971151e-01\n"," 9.99980330e-01 9.99919534e-01 9.99951482e-01 3.76738518e-01\n"," 7.29269385e-01 9.99666214e-01 9.96512115e-01 9.90588188e-01\n"," 1.96033105e-01 9.99143481e-01 9.92965937e-01 7.44329751e-01\n"," 3.01042616e-01 9.99985576e-01 9.99653935e-01 9.93683577e-01\n"," 4.20927368e-02 9.96454239e-01 9.93905723e-01 9.93590832e-01\n"," 9.79498208e-01 9.99367535e-01 2.71545481e-02 5.25571704e-01\n"," 9.02679205e-01 5.75003684e-01 9.99068797e-01 9.38994996e-03\n"," 1.54610984e-02 9.80469515e-04 1.21585943e-01 3.81912403e-02\n"," 7.57843614e-01 9.99345958e-01 9.84283566e-01 6.66800082e-01\n"," 9.99930620e-01 9.97384608e-01]\n","predict [0. 1. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1.\n"," 1. 1. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1.\n"," 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1.\n"," 0. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1.]\n","Train Epoch: 60 [0/107 (0%)]\tTrain Loss: 0.012823\n","Train Epoch: 60 [4/107 (4%)]\tTrain Loss: 0.014497\n","Train Epoch: 60 [8/107 (7%)]\tTrain Loss: 0.003234\n","Train Epoch: 60 [12/107 (11%)]\tTrain Loss: 0.000378\n","Train Epoch: 60 [16/107 (15%)]\tTrain Loss: 0.019713\n","Train Epoch: 60 [20/107 (19%)]\tTrain Loss: 0.056257\n","Train Epoch: 60 [24/107 (22%)]\tTrain Loss: 0.015587\n","Train Epoch: 60 [28/107 (26%)]\tTrain Loss: 0.002700\n","Train Epoch: 60 [32/107 (30%)]\tTrain Loss: 0.016651\n","Train Epoch: 60 [36/107 (34%)]\tTrain Loss: 0.007774\n","Train Epoch: 60 [40/107 (37%)]\tTrain Loss: 0.001705\n","Train Epoch: 60 [44/107 (41%)]\tTrain Loss: 0.003459\n","Train Epoch: 60 [48/107 (45%)]\tTrain Loss: 0.000350\n","Train Epoch: 60 [52/107 (49%)]\tTrain Loss: 0.002010\n","Train Epoch: 60 [56/107 (52%)]\tTrain Loss: 0.000755\n","Train Epoch: 60 [60/107 (56%)]\tTrain Loss: 0.000120\n","Train Epoch: 60 [64/107 (60%)]\tTrain Loss: 0.000218\n","Train Epoch: 60 [68/107 (64%)]\tTrain Loss: 0.034678\n","Train Epoch: 60 [72/107 (67%)]\tTrain Loss: 0.013627\n","Train Epoch: 60 [76/107 (71%)]\tTrain Loss: 0.000389\n","Train Epoch: 60 [80/107 (75%)]\tTrain Loss: 0.000488\n","Train Epoch: 60 [84/107 (79%)]\tTrain Loss: 0.000720\n","Train Epoch: 60 [88/107 (82%)]\tTrain Loss: 0.000545\n","Train Epoch: 60 [92/107 (86%)]\tTrain Loss: 0.000434\n","Train Epoch: 60 [96/107 (90%)]\tTrain Loss: 0.000291\n","Train Epoch: 60 [100/107 (93%)]\tTrain Loss: 0.002580\n","Train Epoch: 60 [104/107 (97%)]\tTrain Loss: 0.000451\n","target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n","score [8.12446952e-01 9.99041140e-01 7.55338490e-01 4.08653647e-01\n"," 1.65230203e-02 4.91761565e-02 9.97360885e-01 5.95806599e-01\n"," 1.18169806e-03 8.98069963e-02 5.52366450e-02 4.23712190e-03\n"," 3.61268572e-03 5.02671339e-02 3.69639620e-02 1.61284770e-04\n"," 1.16746733e-03 7.21960270e-04 5.57406631e-04 6.70939917e-03\n"," 6.89000450e-03 9.91172135e-01 9.96747613e-01 9.98156846e-01\n"," 9.92442131e-01 9.88523901e-01 9.11873877e-01 5.81237394e-03\n"," 5.77687693e-04 9.59654048e-04 1.42928392e-01 6.80020213e-01\n"," 8.73024523e-01 1.93896522e-05 1.42632791e-06 1.31336776e-06\n"," 9.69270695e-06 6.99323177e-01 2.66377511e-03 5.19696357e-07\n"," 1.27679539e-06 6.02633081e-05 5.45614421e-01 7.21283443e-03\n"," 6.97859704e-01 1.08888960e-02 9.68464136e-01 6.67298615e-01\n"," 9.90285695e-01 9.37656641e-01 9.94553328e-01 1.02265236e-08\n"," 6.82761965e-05 5.22331211e-05 1.93718782e-12 5.53916198e-06\n"," 1.51776865e-01 8.20219847e-12 2.57049688e-08 4.15139413e-03\n"," 9.90976155e-01 8.43960583e-01 9.87353981e-01 9.95520711e-01\n"," 9.99880075e-01 9.98493791e-01 9.99193370e-01 9.99980807e-01\n"," 9.99913573e-01 9.86222923e-01 9.92147267e-01 9.90529835e-01\n"," 9.99541879e-01 9.98099506e-01 7.52954245e-01 9.39891219e-01\n"," 9.99975085e-01 9.99974012e-01 9.99981046e-01 9.99808848e-01\n"," 9.99926925e-01 9.99921083e-01 9.99896288e-01 1.03265822e-01\n"," 2.06212893e-01 9.99736845e-01 9.98833239e-01 9.94120657e-01\n"," 2.11347267e-02 9.69364166e-01 9.83094513e-01 9.15516555e-01\n"," 3.45952660e-01 9.99589264e-01 9.99411464e-01 9.97982025e-01\n"," 1.58912882e-01 5.20419553e-02 1.87786177e-01 9.99198020e-01\n"," 5.56094795e-02 9.96672988e-01 1.36755686e-03 9.77047801e-01\n"," 9.81203973e-01 9.39726353e-01 9.97345388e-01 8.43350664e-02\n"," 1.03899278e-02 2.49124435e-03 2.07279086e-01 5.19133955e-02\n"," 9.35525179e-01 9.98275638e-01 9.01408851e-01 1.91564694e-01\n"," 9.98991072e-01 9.90905762e-01]\n","predict [1. 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1.\n"," 1. 1. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 1. 1.\n"," 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1.\n"," 0. 0. 0. 1. 0. 1. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 1. 1. 0. 1. 1.]\n","vote_pred [0. 1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1.\n"," 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0.\n"," 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1.\n"," 0. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1.]\n","targetlist [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n","TP= 49 TN= 44 FN= 9 FP= 16\n","TP+FP 65\n","precision 0.7538461538461538\n","recall 0.8448275862068966\n","F1 0.7967479674796748\n","acc 0.788135593220339\n","AUCp 0.7890804597701151\n","AUC 0.8537356321839081\n","\n"," The epoch is 60, average recall: 0.8448, average precision: 0.7538,average F1: 0.7967, average accuracy: 0.7881, average AUC: 0.8537\n","Train Epoch: 61 [0/107 (0%)]\tTrain Loss: 0.000843\n","Train Epoch: 61 [4/107 (4%)]\tTrain Loss: 0.000729\n","Train Epoch: 61 [8/107 (7%)]\tTrain Loss: 0.008538\n","Train Epoch: 61 [12/107 (11%)]\tTrain Loss: 0.000863\n","Train Epoch: 61 [16/107 (15%)]\tTrain Loss: 0.006834\n","Train Epoch: 61 [20/107 (19%)]\tTrain Loss: 0.001334\n","Train Epoch: 61 [24/107 (22%)]\tTrain Loss: 0.010494\n","Train Epoch: 61 [28/107 (26%)]\tTrain Loss: 0.002479\n","Train Epoch: 61 [32/107 (30%)]\tTrain Loss: 0.003911\n","Train Epoch: 61 [36/107 (34%)]\tTrain Loss: 0.001644\n","Train Epoch: 61 [40/107 (37%)]\tTrain Loss: 0.005878\n","Train Epoch: 61 [44/107 (41%)]\tTrain Loss: 0.004515\n","Train Epoch: 61 [48/107 (45%)]\tTrain Loss: 0.000719\n","Train Epoch: 61 [52/107 (49%)]\tTrain Loss: 0.006050\n","Train Epoch: 61 [56/107 (52%)]\tTrain Loss: 0.000331\n","Train Epoch: 61 [60/107 (56%)]\tTrain Loss: 0.002158\n","Train Epoch: 61 [64/107 (60%)]\tTrain Loss: 0.000111\n","Train Epoch: 61 [68/107 (64%)]\tTrain Loss: 0.001260\n","Train Epoch: 61 [72/107 (67%)]\tTrain Loss: 0.000431\n","Train Epoch: 61 [76/107 (71%)]\tTrain Loss: 0.000659\n","Train Epoch: 61 [80/107 (75%)]\tTrain Loss: 0.001163\n","Train Epoch: 61 [84/107 (79%)]\tTrain Loss: 0.001078\n","Train Epoch: 61 [88/107 (82%)]\tTrain Loss: 0.000393\n","Train Epoch: 61 [92/107 (86%)]\tTrain Loss: 0.003499\n","Train Epoch: 61 [96/107 (90%)]\tTrain Loss: 0.001281\n","Train Epoch: 61 [100/107 (93%)]\tTrain Loss: 0.008513\n","Train Epoch: 61 [104/107 (97%)]\tTrain Loss: 0.001814\n","target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n","score [4.21968043e-01 9.74865675e-01 2.03887850e-01 9.74679887e-01\n"," 8.07775080e-01 7.45848492e-02 9.84776735e-01 9.96886194e-01\n"," 5.01930062e-03 3.11764807e-01 9.31630611e-01 5.49507380e-01\n"," 8.83714795e-01 4.20218796e-01 7.35245824e-01 3.19614969e-02\n"," 1.04735522e-02 3.90774727e-01 7.32724726e-01 9.73193407e-01\n"," 7.24792838e-01 9.93853331e-01 9.71050382e-01 9.91185963e-01\n"," 9.90063608e-01 8.21727514e-01 9.80383337e-01 1.99894775e-02\n"," 2.10573040e-02 4.00096983e-01 1.74833015e-01 6.01645410e-01\n"," 2.98281997e-01 2.47559324e-03 1.07500586e-03 2.26106611e-03\n"," 1.45823089e-02 9.93718147e-01 8.92827272e-01 2.07472080e-03\n"," 1.40316486e-02 5.07447645e-02 9.87770319e-01 9.55773175e-01\n"," 9.36855197e-01 9.33842659e-01 9.79823709e-01 9.39714611e-01\n"," 8.89979184e-01 9.05539751e-01 9.01999712e-01 5.32914719e-11\n"," 1.82730844e-04 2.51752226e-06 2.16687172e-12 2.39438691e-06\n"," 1.33133233e-01 1.44870332e-14 2.26571810e-08 2.71061901e-03\n"," 9.98688877e-01 9.96758759e-01 9.96692777e-01 9.98691857e-01\n"," 9.91347492e-01 9.73085701e-01 9.97837842e-01 9.98539209e-01\n"," 9.96061504e-01 9.73422110e-01 9.69803333e-01 9.84173417e-01\n"," 9.98441637e-01 9.97955441e-01 9.40656602e-01 9.96787429e-01\n"," 9.98300016e-01 9.98253405e-01 9.95792627e-01 9.89566982e-01\n"," 9.97142017e-01 9.98689592e-01 9.94849503e-01 9.91780221e-01\n"," 9.86003220e-01 9.95983839e-01 9.99898553e-01 9.99561369e-01\n"," 1.44624859e-01 9.71837163e-01 7.53529489e-01 8.83712649e-01\n"," 5.59972823e-01 9.94252861e-01 9.87433255e-01 9.92233753e-01\n"," 9.69100654e-01 9.96526182e-01 9.85206306e-01 9.96969521e-01\n"," 7.91982055e-01 9.98863220e-01 8.30748975e-01 9.82845247e-01\n"," 9.79545116e-01 8.73089194e-01 9.67873275e-01 1.37004524e-01\n"," 2.93744117e-01 1.42237712e-02 8.24876800e-02 1.03744961e-01\n"," 6.74976707e-01 9.53925073e-01 9.47767854e-01 9.87044036e-01\n"," 9.99974847e-01 9.95376348e-01]\n","predict [0. 1. 0. 1. 1. 0. 1. 1. 0. 0. 1. 1. 1. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1.]\n","Train Epoch: 62 [0/107 (0%)]\tTrain Loss: 0.010659\n","Train Epoch: 62 [4/107 (4%)]\tTrain Loss: 0.024893\n","Train Epoch: 62 [8/107 (7%)]\tTrain Loss: 0.001547\n","Train Epoch: 62 [12/107 (11%)]\tTrain Loss: 0.012004\n","Train Epoch: 62 [16/107 (15%)]\tTrain Loss: 0.010829\n","Train Epoch: 62 [20/107 (19%)]\tTrain Loss: 0.001808\n","Train Epoch: 62 [24/107 (22%)]\tTrain Loss: 0.020159\n","Train Epoch: 62 [28/107 (26%)]\tTrain Loss: 0.000631\n","Train Epoch: 62 [32/107 (30%)]\tTrain Loss: 0.000680\n","Train Epoch: 62 [36/107 (34%)]\tTrain Loss: 0.001863\n","Train Epoch: 62 [40/107 (37%)]\tTrain Loss: 0.035026\n","Train Epoch: 62 [44/107 (41%)]\tTrain Loss: 0.014591\n","Train Epoch: 62 [48/107 (45%)]\tTrain Loss: 0.003635\n","Train Epoch: 62 [52/107 (49%)]\tTrain Loss: 0.001160\n","Train Epoch: 62 [56/107 (52%)]\tTrain Loss: 0.001606\n","Train Epoch: 62 [60/107 (56%)]\tTrain Loss: 0.014876\n","Train Epoch: 62 [64/107 (60%)]\tTrain Loss: 0.000730\n","Train Epoch: 62 [68/107 (64%)]\tTrain Loss: 0.009530\n","Train Epoch: 62 [72/107 (67%)]\tTrain Loss: 0.004405\n","Train Epoch: 62 [76/107 (71%)]\tTrain Loss: 0.002814\n","Train Epoch: 62 [80/107 (75%)]\tTrain Loss: 0.000984\n","Train Epoch: 62 [84/107 (79%)]\tTrain Loss: 0.001523\n","Train Epoch: 62 [88/107 (82%)]\tTrain Loss: 0.048206\n","Train Epoch: 62 [92/107 (86%)]\tTrain Loss: 0.003962\n","Train Epoch: 62 [96/107 (90%)]\tTrain Loss: 0.004054\n","Train Epoch: 62 [100/107 (93%)]\tTrain Loss: 0.000298\n","Train Epoch: 62 [104/107 (97%)]\tTrain Loss: 0.000733\n","target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n","score [9.82819378e-01 9.99344647e-01 6.21260285e-01 9.97117519e-01\n"," 8.21216166e-01 4.49505448e-01 9.95480061e-01 9.99139428e-01\n"," 1.02661073e-01 3.13603461e-01 9.87623811e-01 3.08922417e-02\n"," 9.96263564e-01 8.94009590e-01 9.61633265e-01 6.12171087e-03\n"," 7.30361324e-03 2.02391557e-02 4.74381231e-04 6.78744540e-02\n"," 1.02213092e-01 9.96654153e-01 9.99954820e-01 9.99984026e-01\n"," 9.85865235e-01 9.99939680e-01 9.99992967e-01 6.49221987e-03\n"," 1.14478394e-02 1.00562116e-02 5.99313021e-01 9.65705752e-01\n"," 6.49263620e-01 2.02540285e-03 1.09803642e-03 3.00483359e-03\n"," 4.94251586e-03 9.36481178e-01 5.44766605e-01 4.30879118e-05\n"," 1.32230169e-04 2.46121362e-03 9.28324938e-01 8.39553654e-01\n"," 9.99533772e-01 9.82306898e-01 9.99975443e-01 9.99720991e-01\n"," 9.99796450e-01 9.99579966e-01 9.99989986e-01 7.24519000e-10\n"," 1.43150419e-06 1.31271814e-03 9.13323846e-14 5.50273747e-08\n"," 3.62833679e-01 7.19120215e-15 9.16762166e-10 1.13447476e-02\n"," 9.99999762e-01 9.99999046e-01 9.99999523e-01 9.99999642e-01\n"," 9.99997139e-01 9.99934077e-01 9.99978065e-01 1.00000000e+00\n"," 9.99999523e-01 9.99908686e-01 9.99997616e-01 9.99997973e-01\n"," 9.99996185e-01 9.99994278e-01 9.99802768e-01 9.99980450e-01\n"," 9.99999523e-01 9.99999523e-01 1.00000000e+00 9.99998569e-01\n"," 9.99999881e-01 9.99999762e-01 9.99998808e-01 9.97927904e-01\n"," 9.97717023e-01 9.99991417e-01 9.99997973e-01 9.99982834e-01\n"," 9.52248573e-02 9.99976754e-01 9.99710619e-01 9.92731512e-01\n"," 8.60157549e-01 9.99993205e-01 9.99981999e-01 9.99884963e-01\n"," 9.37058449e-01 9.95843232e-01 9.45355058e-01 9.99991894e-01\n"," 9.60070908e-01 9.99994159e-01 9.28099096e-01 9.98318315e-01\n"," 9.98543739e-01 9.86496508e-01 9.97533560e-01 3.33725244e-01\n"," 6.92785252e-03 4.74511720e-02 7.82024115e-03 1.08735665e-04\n"," 9.93107259e-01 9.99868989e-01 9.99818027e-01 9.99621749e-01\n"," 9.99220967e-01 9.99920964e-01]\n","predict [1. 1. 1. 1. 1. 0. 1. 1. 0. 0. 1. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1.\n"," 1. 1. 1. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1.]\n","Train Epoch: 63 [0/107 (0%)]\tTrain Loss: 0.000313\n","Train Epoch: 63 [4/107 (4%)]\tTrain Loss: 0.001799\n","Train Epoch: 63 [8/107 (7%)]\tTrain Loss: 0.010596\n","Train Epoch: 63 [12/107 (11%)]\tTrain Loss: 0.000149\n","Train Epoch: 63 [16/107 (15%)]\tTrain Loss: 0.000315\n","Train Epoch: 63 [20/107 (19%)]\tTrain Loss: 0.000404\n","Train Epoch: 63 [24/107 (22%)]\tTrain Loss: 0.007172\n","Train Epoch: 63 [28/107 (26%)]\tTrain Loss: 0.000538\n","Train Epoch: 63 [32/107 (30%)]\tTrain Loss: 0.000068\n","Train Epoch: 63 [36/107 (34%)]\tTrain Loss: 0.001138\n","Train Epoch: 63 [40/107 (37%)]\tTrain Loss: 0.000435\n","Train Epoch: 63 [44/107 (41%)]\tTrain Loss: 0.001377\n","Train Epoch: 63 [48/107 (45%)]\tTrain Loss: 0.004845\n","Train Epoch: 63 [52/107 (49%)]\tTrain Loss: 0.005230\n","Train Epoch: 63 [56/107 (52%)]\tTrain Loss: 0.009357\n","Train Epoch: 63 [60/107 (56%)]\tTrain Loss: 0.001001\n","Train Epoch: 63 [64/107 (60%)]\tTrain Loss: 0.000330\n","Train Epoch: 63 [68/107 (64%)]\tTrain Loss: 0.014241\n","Train Epoch: 63 [72/107 (67%)]\tTrain Loss: 0.002435\n","Train Epoch: 63 [76/107 (71%)]\tTrain Loss: 0.027803\n","Train Epoch: 63 [80/107 (75%)]\tTrain Loss: 0.001487\n","Train Epoch: 63 [84/107 (79%)]\tTrain Loss: 0.000996\n","Train Epoch: 63 [88/107 (82%)]\tTrain Loss: 0.000294\n","Train Epoch: 63 [92/107 (86%)]\tTrain Loss: 0.000735\n","Train Epoch: 63 [96/107 (90%)]\tTrain Loss: 0.000355\n","Train Epoch: 63 [100/107 (93%)]\tTrain Loss: 0.005282\n","Train Epoch: 63 [104/107 (97%)]\tTrain Loss: 0.001448\n","target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n","score [3.64680476e-02 9.34419990e-01 1.37775928e-01 1.05417795e-01\n"," 2.96148728e-03 5.99406324e-02 9.11069334e-01 5.80816902e-02\n"," 8.60208459e-03 3.85151431e-02 8.85968935e-03 2.97991931e-03\n"," 2.01239344e-03 2.84796674e-03 8.26902688e-03 1.67934966e-04\n"," 2.00620678e-04 2.71877442e-02 4.52820968e-05 6.58146630e-04\n"," 2.20934302e-03 1.38001993e-01 6.68947041e-01 9.99904752e-01\n"," 7.30773155e-03 4.05111581e-01 9.99798477e-01 3.51486873e-04\n"," 8.67210838e-05 2.45783187e-04 2.30530836e-03 3.50828079e-04\n"," 1.00125708e-02 2.46633008e-05 2.95461268e-06 2.13463372e-05\n"," 2.47373846e-05 2.17083097e-01 1.71411666e-03 5.97653343e-05\n"," 5.37233755e-05 1.55055779e-03 7.30559707e-01 2.91129283e-04\n"," 6.79882709e-03 3.74963810e-03 5.04261315e-01 1.67971000e-01\n"," 5.17275929e-01 8.82696390e-01 9.75023508e-01 1.29869548e-07\n"," 1.73391396e-04 1.58775321e-04 4.05480272e-08 8.08086952e-06\n"," 1.27824277e-01 2.16215201e-09 1.03996297e-06 1.95504035e-04\n"," 9.78982091e-01 9.84016776e-01 9.98608053e-01 9.93564904e-01\n"," 7.36152709e-01 8.79003823e-01 2.40115061e-01 9.99963522e-01\n"," 9.91186202e-01 9.73981798e-01 6.31837070e-01 4.18988943e-01\n"," 9.82570946e-01 8.07537913e-01 3.56227279e-01 1.19311996e-02\n"," 9.99354184e-01 9.99254525e-01 9.69494879e-01 9.87391651e-01\n"," 9.98816252e-01 9.45590973e-01 9.98789370e-01 6.35991544e-02\n"," 7.34689534e-01 9.96705592e-01 3.80918145e-01 7.50300810e-02\n"," 9.20257706e-04 8.13852772e-02 9.80054855e-01 1.42403051e-01\n"," 9.00714174e-02 9.99987364e-01 9.99796331e-01 9.84419405e-01\n"," 4.88943711e-04 4.47626226e-03 2.52073202e-02 8.48346472e-01\n"," 2.30534319e-02 5.26739955e-01 2.67661433e-03 4.45271611e-01\n"," 2.46936470e-01 2.14995459e-01 9.91921902e-01 2.84273122e-02\n"," 7.12443469e-03 8.47093668e-03 1.18964240e-01 3.83707017e-01\n"," 8.71077001e-01 1.80786140e-02 1.22246154e-01 2.05187630e-02\n"," 9.99946833e-01 9.97099519e-01]\n","predict [0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.\n"," 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0.\n"," 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0.\n"," 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1.\n"," 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1.]\n","Train Epoch: 64 [0/107 (0%)]\tTrain Loss: 0.000965\n","Train Epoch: 64 [4/107 (4%)]\tTrain Loss: 0.002012\n","Train Epoch: 64 [8/107 (7%)]\tTrain Loss: 0.002424\n","Train Epoch: 64 [12/107 (11%)]\tTrain Loss: 0.034691\n","Train Epoch: 64 [16/107 (15%)]\tTrain Loss: 0.032872\n","Train Epoch: 64 [20/107 (19%)]\tTrain Loss: 0.006144\n","Train Epoch: 64 [24/107 (22%)]\tTrain Loss: 0.037070\n","Train Epoch: 64 [28/107 (26%)]\tTrain Loss: 0.164709\n","Train Epoch: 64 [32/107 (30%)]\tTrain Loss: 0.007292\n","Train Epoch: 64 [36/107 (34%)]\tTrain Loss: 0.000907\n","Train Epoch: 64 [40/107 (37%)]\tTrain Loss: 0.001870\n","Train Epoch: 64 [44/107 (41%)]\tTrain Loss: 0.009178\n","Train Epoch: 64 [48/107 (45%)]\tTrain Loss: 0.020418\n","Train Epoch: 64 [52/107 (49%)]\tTrain Loss: 0.023927\n","Train Epoch: 64 [56/107 (52%)]\tTrain Loss: 0.074287\n","Train Epoch: 64 [60/107 (56%)]\tTrain Loss: 0.014414\n","Train Epoch: 64 [64/107 (60%)]\tTrain Loss: 0.209042\n","Train Epoch: 64 [68/107 (64%)]\tTrain Loss: 0.004176\n","Train Epoch: 64 [72/107 (67%)]\tTrain Loss: 0.000687\n","Train Epoch: 64 [76/107 (71%)]\tTrain Loss: 0.121900\n","Train Epoch: 64 [80/107 (75%)]\tTrain Loss: 0.008286\n","Train Epoch: 64 [84/107 (79%)]\tTrain Loss: 0.001733\n","Train Epoch: 64 [88/107 (82%)]\tTrain Loss: 0.007138\n","Train Epoch: 64 [92/107 (86%)]\tTrain Loss: 0.065585\n","Train Epoch: 64 [96/107 (90%)]\tTrain Loss: 0.043243\n","Train Epoch: 64 [100/107 (93%)]\tTrain Loss: 0.005520\n","Train Epoch: 64 [104/107 (97%)]\tTrain Loss: 0.001121\n","target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n","score [5.87063849e-01 9.95191813e-01 4.71545100e-01 9.99505162e-01\n"," 4.16331664e-02 9.46149826e-02 9.97577727e-01 9.99213099e-01\n"," 1.08095534e-01 2.47940898e-01 2.02992037e-02 2.73289345e-02\n"," 2.31484231e-02 7.45502472e-01 7.73005545e-01 1.24448150e-06\n"," 1.09595230e-05 4.44245478e-03 4.29033004e-02 5.54927550e-02\n"," 1.58519037e-02 9.94372845e-01 9.99895811e-01 9.99994397e-01\n"," 6.67048514e-01 9.96756613e-01 9.99924421e-01 1.86936965e-03\n"," 7.66878016e-03 1.10263042e-02 4.14220057e-02 1.79637656e-01\n"," 3.59251291e-01 7.48769025e-06 4.42813146e-08 5.19140049e-05\n"," 3.39392827e-05 6.32627070e-01 2.14667781e-03 1.26565070e-07\n"," 9.63843490e-07 3.29610771e-07 7.76770949e-01 3.51005176e-04\n"," 9.94052470e-01 3.82997543e-02 9.43720102e-01 7.77824998e-01\n"," 9.05009866e-01 8.09335291e-01 9.79515612e-01 8.71486438e-04\n"," 3.13771452e-04 8.10593308e-04 2.01450967e-09 5.87451505e-04\n"," 7.02099979e-01 4.92607599e-09 2.88491037e-06 6.61714822e-02\n"," 9.99941468e-01 9.99963284e-01 9.99849558e-01 9.99068439e-01\n"," 9.99765933e-01 9.90182817e-01 9.93852615e-01 9.99984741e-01\n"," 9.99976397e-01 9.97972906e-01 9.92499292e-01 9.97761846e-01\n"," 9.99953866e-01 9.99334633e-01 9.95841563e-01 9.82529402e-01\n"," 9.99996424e-01 9.99989867e-01 9.99997020e-01 9.99432147e-01\n"," 9.99986768e-01 9.99827385e-01 9.99871969e-01 7.74556935e-01\n"," 8.71619761e-01 9.99800384e-01 9.99826252e-01 9.98297513e-01\n"," 3.77793014e-02 9.73034680e-01 7.63345659e-01 9.26169395e-01\n"," 9.65666056e-01 9.99945045e-01 9.99485612e-01 9.96771157e-01\n"," 9.38659012e-01 9.87819910e-01 2.35817373e-01 7.98374355e-01\n"," 2.50991955e-02 9.86961246e-01 1.17909126e-01 6.75478399e-01\n"," 9.81208444e-01 5.23415267e-01 9.52898204e-01 6.41216012e-03\n"," 8.39705050e-01 9.23885033e-02 5.68127930e-02 2.55817920e-01\n"," 9.54922020e-01 9.94393468e-01 8.53145421e-01 9.76690233e-01\n"," 9.99998808e-01 9.99901056e-01]\n","predict [1. 1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1.\n"," 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 1. 1.\n"," 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 0. 1. 0. 1. 0. 1. 1. 1. 1. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1.]\n","Train Epoch: 65 [0/107 (0%)]\tTrain Loss: 0.051954\n","Train Epoch: 65 [4/107 (4%)]\tTrain Loss: 0.020155\n","Train Epoch: 65 [8/107 (7%)]\tTrain Loss: 0.008357\n","Train Epoch: 65 [12/107 (11%)]\tTrain Loss: 0.000569\n","Train Epoch: 65 [16/107 (15%)]\tTrain Loss: 0.004668\n","Train Epoch: 65 [20/107 (19%)]\tTrain Loss: 0.062321\n","Train Epoch: 65 [24/107 (22%)]\tTrain Loss: 0.016122\n","Train Epoch: 65 [28/107 (26%)]\tTrain Loss: 0.000228\n","Train Epoch: 65 [32/107 (30%)]\tTrain Loss: 0.023921\n","Train Epoch: 65 [36/107 (34%)]\tTrain Loss: 0.014701\n","Train Epoch: 65 [40/107 (37%)]\tTrain Loss: 0.009068\n","Train Epoch: 65 [44/107 (41%)]\tTrain Loss: 0.003487\n","Train Epoch: 65 [48/107 (45%)]\tTrain Loss: 0.069751\n","Train Epoch: 65 [52/107 (49%)]\tTrain Loss: 0.044361\n","Train Epoch: 65 [56/107 (52%)]\tTrain Loss: 0.115822\n","Train Epoch: 65 [60/107 (56%)]\tTrain Loss: 0.001952\n","Train Epoch: 65 [64/107 (60%)]\tTrain Loss: 0.205422\n","Train Epoch: 65 [68/107 (64%)]\tTrain Loss: 0.001972\n","Train Epoch: 65 [72/107 (67%)]\tTrain Loss: 0.105329\n","Train Epoch: 65 [76/107 (71%)]\tTrain Loss: 0.002158\n","Train Epoch: 65 [80/107 (75%)]\tTrain Loss: 0.000673\n","Train Epoch: 65 [84/107 (79%)]\tTrain Loss: 0.000286\n","Train Epoch: 65 [88/107 (82%)]\tTrain Loss: 0.022828\n","Train Epoch: 65 [92/107 (86%)]\tTrain Loss: 0.071141\n","Train Epoch: 65 [96/107 (90%)]\tTrain Loss: 0.000961\n","Train Epoch: 65 [100/107 (93%)]\tTrain Loss: 0.073527\n","Train Epoch: 65 [104/107 (97%)]\tTrain Loss: 0.003242\n","target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n","score [6.35929585e-01 8.78647447e-01 2.13901460e-01 8.94603372e-01\n"," 1.70276150e-01 1.41052762e-02 9.19970632e-01 3.19824785e-01\n"," 1.26161263e-03 5.77804625e-01 8.56398791e-02 2.77168974e-02\n"," 2.05850117e-02 1.16042994e-01 6.66534354e-04 1.02558297e-04\n"," 4.44786187e-04 7.77733745e-03 5.27507927e-06 5.75892366e-02\n"," 1.68455541e-02 5.19863702e-02 1.21932132e-02 9.99806583e-01\n"," 6.61978960e-01 7.16405883e-02 1.00000000e+00 3.95124880e-06\n"," 4.63622474e-09 2.88862534e-09 5.72825298e-02 3.92623806e-05\n"," 5.13024488e-03 1.47655024e-03 3.01495107e-04 3.52694909e-03\n"," 2.65042204e-03 6.52207911e-01 1.53404623e-01 2.23090428e-05\n"," 9.33603660e-05 2.24043368e-04 7.16481730e-03 9.30479146e-05\n"," 4.29384634e-02 3.53279114e-01 8.30003738e-01 6.95234179e-01\n"," 9.32542503e-01 9.10831928e-01 9.74948287e-01 2.83417376e-05\n"," 8.21481168e-04 2.48336801e-05 1.92095177e-11 2.99747262e-06\n"," 8.09336841e-01 1.38624653e-10 2.39241227e-09 5.56875486e-03\n"," 9.99412775e-01 9.99398470e-01 9.99941468e-01 9.98442948e-01\n"," 9.84112680e-01 4.19843979e-02 9.33534861e-01 9.99227881e-01\n"," 9.99885559e-01 9.99009967e-01 9.94696259e-01 9.90009189e-01\n"," 9.99851942e-01 9.99986768e-01 9.67848241e-01 1.48514345e-01\n"," 9.99738395e-01 9.99703467e-01 8.21091890e-01 9.96283472e-01\n"," 9.99964356e-01 8.94951463e-01 9.99665618e-01 7.48595595e-01\n"," 4.48460966e-01 9.99977827e-01 9.98657703e-01 9.99729812e-01\n"," 8.09360572e-05 8.09803486e-01 9.80720699e-01 9.68491018e-01\n"," 6.93762481e-01 9.99999881e-01 9.99937415e-01 9.99967933e-01\n"," 1.10209733e-02 7.04279840e-01 5.20702183e-01 9.92456436e-01\n"," 1.65324464e-01 8.91647756e-01 1.94285244e-01 6.65886581e-01\n"," 9.70616221e-01 9.60481048e-01 9.99998450e-01 1.47314712e-01\n"," 2.43902892e-01 9.09496188e-01 3.17188382e-01 9.99996066e-01\n"," 9.99292254e-01 2.40902632e-01 2.64679402e-01 6.20356739e-01\n"," 9.99994755e-01 9.99796450e-01]\n","predict [1. 1. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n"," 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.\n"," 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1.\n"," 0. 1. 1. 1. 0. 1. 0. 1. 1. 1. 1. 0. 0. 1. 0. 1. 1. 0. 0. 1. 1. 1.]\n","Train Epoch: 66 [0/107 (0%)]\tTrain Loss: 0.042348\n","Train Epoch: 66 [4/107 (4%)]\tTrain Loss: 0.014657\n","Train Epoch: 66 [8/107 (7%)]\tTrain Loss: 0.003452\n","Train Epoch: 66 [12/107 (11%)]\tTrain Loss: 0.006592\n","Train Epoch: 66 [16/107 (15%)]\tTrain Loss: 0.005336\n","Train Epoch: 66 [20/107 (19%)]\tTrain Loss: 0.010828\n","Train Epoch: 66 [24/107 (22%)]\tTrain Loss: 0.011323\n","Train Epoch: 66 [28/107 (26%)]\tTrain Loss: 0.023874\n","Train Epoch: 66 [32/107 (30%)]\tTrain Loss: 0.004290\n","Train Epoch: 66 [36/107 (34%)]\tTrain Loss: 0.307629\n","Train Epoch: 66 [40/107 (37%)]\tTrain Loss: 0.000413\n","Train Epoch: 66 [44/107 (41%)]\tTrain Loss: 0.001328\n","Train Epoch: 66 [48/107 (45%)]\tTrain Loss: 0.000375\n","Train Epoch: 66 [52/107 (49%)]\tTrain Loss: 0.002162\n","Train Epoch: 66 [56/107 (52%)]\tTrain Loss: 0.298677\n","Train Epoch: 66 [60/107 (56%)]\tTrain Loss: 0.002432\n","Train Epoch: 66 [64/107 (60%)]\tTrain Loss: 0.051097\n","Train Epoch: 66 [68/107 (64%)]\tTrain Loss: 0.002320\n","Train Epoch: 66 [72/107 (67%)]\tTrain Loss: 0.005882\n","Train Epoch: 66 [76/107 (71%)]\tTrain Loss: 0.026450\n","Train Epoch: 66 [80/107 (75%)]\tTrain Loss: 0.000303\n","Train Epoch: 66 [84/107 (79%)]\tTrain Loss: 0.002737\n","Train Epoch: 66 [88/107 (82%)]\tTrain Loss: 0.009518\n","Train Epoch: 66 [92/107 (86%)]\tTrain Loss: 0.010603\n","Train Epoch: 66 [96/107 (90%)]\tTrain Loss: 0.028576\n","Train Epoch: 66 [100/107 (93%)]\tTrain Loss: 0.014405\n","Train Epoch: 66 [104/107 (97%)]\tTrain Loss: 0.000080\n","target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n","score [3.11049279e-02 3.79572839e-01 3.06219999e-02 7.82794833e-01\n"," 2.90225958e-04 1.41639262e-02 5.06587744e-01 7.46294737e-01\n"," 2.82013346e-03 3.62346061e-02 1.14434131e-03 2.59538386e-02\n"," 1.00902596e-03 1.02172844e-01 7.40206167e-02 9.15219971e-07\n"," 2.28224217e-05 6.86383108e-04 1.82148851e-05 1.26226130e-03\n"," 4.58076800e-04 2.89661944e-01 8.95063341e-01 9.97826636e-01\n"," 1.61511913e-01 8.71578753e-01 9.75721538e-01 1.13067734e-04\n"," 9.00174473e-06 2.67430842e-05 9.92924441e-04 2.20413160e-04\n"," 2.02233475e-02 8.36260369e-06 2.25331223e-06 1.31742427e-05\n"," 7.55039218e-05 1.88625589e-01 6.19394630e-02 9.78364710e-07\n"," 5.70010707e-06 8.72727287e-06 1.54041022e-01 4.57553433e-05\n"," 1.56046346e-01 5.39860986e-02 3.39086913e-02 3.78773920e-02\n"," 9.56095934e-01 3.64491433e-01 9.60011482e-01 3.03379267e-07\n"," 2.68950989e-05 7.93663436e-04 1.65834448e-14 5.20534559e-06\n"," 6.88923180e-01 2.33509184e-11 3.79889862e-05 1.63833120e-05\n"," 9.92137730e-01 9.62382376e-01 9.95254397e-01 9.92220104e-01\n"," 9.99589860e-01 9.85516965e-01 9.26983595e-01 9.99658823e-01\n"," 9.99764502e-01 9.79301214e-01 9.62087691e-01 8.45785141e-01\n"," 9.97132182e-01 9.98339057e-01 9.62165833e-01 5.64988792e-01\n"," 9.99651790e-01 9.99488354e-01 9.94943202e-01 9.82588112e-01\n"," 9.99587357e-01 9.98477399e-01 9.95949388e-01 1.56245023e-01\n"," 1.78642944e-01 9.80369270e-01 9.94680464e-01 9.93929625e-01\n"," 3.70408245e-03 6.58763289e-01 9.47939515e-01 6.46710873e-01\n"," 3.33184600e-01 9.99996781e-01 9.95975435e-01 9.99725044e-01\n"," 6.23798445e-02 2.25195110e-01 7.18734926e-03 9.76231754e-01\n"," 1.77710816e-01 9.88420844e-01 1.76693276e-02 1.05922678e-02\n"," 1.07356369e-01 1.19594028e-02 6.69202209e-01 6.94381027e-03\n"," 2.87569482e-02 1.28916616e-03 6.69706380e-03 1.87384069e-01\n"," 9.91487265e-01 8.45945239e-01 5.65450132e-01 4.55873534e-02\n"," 9.99979615e-01 9.87517178e-01]\n","predict [0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.\n"," 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1.\n"," 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 1. 0. 1. 1.]\n","Train Epoch: 67 [0/107 (0%)]\tTrain Loss: 0.001073\n","Train Epoch: 67 [4/107 (4%)]\tTrain Loss: 0.000085\n","Train Epoch: 67 [8/107 (7%)]\tTrain Loss: 0.000095\n","Train Epoch: 67 [12/107 (11%)]\tTrain Loss: 0.001838\n","Train Epoch: 67 [16/107 (15%)]\tTrain Loss: 0.001349\n","Train Epoch: 67 [20/107 (19%)]\tTrain Loss: 0.001704\n","Train Epoch: 67 [24/107 (22%)]\tTrain Loss: 0.003843\n","Train Epoch: 67 [28/107 (26%)]\tTrain Loss: 0.009432\n","Train Epoch: 67 [32/107 (30%)]\tTrain Loss: 0.001612\n","Train Epoch: 67 [36/107 (34%)]\tTrain Loss: 0.003615\n","Train Epoch: 67 [40/107 (37%)]\tTrain Loss: 0.006092\n","Train Epoch: 67 [44/107 (41%)]\tTrain Loss: 0.006772\n","Train Epoch: 67 [48/107 (45%)]\tTrain Loss: 0.001321\n","Train Epoch: 67 [52/107 (49%)]\tTrain Loss: 0.001653\n","Train Epoch: 67 [56/107 (52%)]\tTrain Loss: 0.033478\n","Train Epoch: 67 [60/107 (56%)]\tTrain Loss: 0.000310\n","Train Epoch: 67 [64/107 (60%)]\tTrain Loss: 0.000299\n","Train Epoch: 67 [68/107 (64%)]\tTrain Loss: 0.006709\n","Train Epoch: 67 [72/107 (67%)]\tTrain Loss: 0.001584\n","Train Epoch: 67 [76/107 (71%)]\tTrain Loss: 0.002992\n","Train Epoch: 67 [80/107 (75%)]\tTrain Loss: 0.000736\n","Train Epoch: 67 [84/107 (79%)]\tTrain Loss: 0.064144\n","Train Epoch: 67 [88/107 (82%)]\tTrain Loss: 0.000937\n","Train Epoch: 67 [92/107 (86%)]\tTrain Loss: 0.009071\n","Train Epoch: 67 [96/107 (90%)]\tTrain Loss: 0.003871\n","Train Epoch: 67 [100/107 (93%)]\tTrain Loss: 0.000463\n","Train Epoch: 67 [104/107 (97%)]\tTrain Loss: 0.001952\n","target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n","score [6.28479868e-02 9.18997765e-01 4.75001782e-01 3.16826075e-01\n"," 2.43741870e-02 3.59935611e-02 9.58956242e-01 6.44397616e-01\n"," 1.60781406e-02 9.11759257e-01 3.44211668e-01 2.59184837e-01\n"," 4.95728105e-02 5.78534007e-01 3.16178322e-01 2.00264585e-05\n"," 1.34932518e-04 1.46393046e-01 3.01137334e-03 7.27785155e-02\n"," 1.24144189e-01 9.61791635e-01 9.98233914e-01 9.98214602e-01\n"," 9.71362293e-01 9.98724759e-01 9.99659419e-01 1.13238923e-01\n"," 3.76415555e-04 2.38612425e-04 5.60020804e-01 4.93757939e-03\n"," 1.60566479e-01 1.34443297e-04 4.65681718e-04 5.52876946e-03\n"," 2.49675326e-02 9.92616236e-01 9.88436460e-01 1.73859895e-04\n"," 9.50075395e-04 8.76681320e-03 7.64358044e-01 2.07191378e-01\n"," 7.12799788e-01 7.99104273e-01 8.78085792e-01 3.56860310e-01\n"," 8.43459845e-01 7.64199018e-01 9.86778736e-01 1.00399091e-04\n"," 2.02017464e-03 1.66372612e-01 8.11760614e-09 1.53530939e-02\n"," 8.75954032e-01 2.21823086e-07 6.40155804e-06 3.07253632e-03\n"," 9.99854565e-01 9.99228358e-01 9.99869585e-01 9.99550760e-01\n"," 9.95619953e-01 9.92947102e-01 9.33991075e-01 9.99765456e-01\n"," 9.87382352e-01 9.97061431e-01 9.82642055e-01 9.02476072e-01\n"," 9.97523248e-01 9.99368966e-01 9.90841746e-01 9.93673325e-01\n"," 9.98627186e-01 9.99232054e-01 9.99720871e-01 9.94231343e-01\n"," 9.97867942e-01 9.97798979e-01 9.95988429e-01 9.92824435e-01\n"," 9.75506783e-01 9.99813020e-01 9.98655200e-01 9.98971820e-01\n"," 4.08091545e-02 9.95877266e-01 9.83946443e-01 9.79694784e-01\n"," 8.99839520e-01 9.99334276e-01 9.97574747e-01 9.98531818e-01\n"," 7.42082417e-01 9.46096718e-01 9.80360687e-01 9.98599470e-01\n"," 9.15250421e-01 9.97594535e-01 6.70681894e-01 4.67878193e-01\n"," 8.46442044e-01 8.84704947e-01 9.94659603e-01 9.13133144e-01\n"," 7.12699890e-01 1.76799759e-01 7.75960907e-02 9.52026665e-01\n"," 9.94202077e-01 9.89991307e-01 9.11861241e-01 9.20911849e-01\n"," 9.99932408e-01 9.98761773e-01]\n","predict [0. 1. 0. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1.\n"," 1. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 1. 1. 1. 0.\n"," 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1.]\n","Train Epoch: 68 [0/107 (0%)]\tTrain Loss: 0.000626\n","Train Epoch: 68 [4/107 (4%)]\tTrain Loss: 0.000243\n","Train Epoch: 68 [8/107 (7%)]\tTrain Loss: 0.000878\n","Train Epoch: 68 [12/107 (11%)]\tTrain Loss: 0.006017\n","Train Epoch: 68 [16/107 (15%)]\tTrain Loss: 0.001171\n","Train Epoch: 68 [20/107 (19%)]\tTrain Loss: 0.002023\n","Train Epoch: 68 [24/107 (22%)]\tTrain Loss: 0.001053\n","Train Epoch: 68 [28/107 (26%)]\tTrain Loss: 0.000495\n","Train Epoch: 68 [32/107 (30%)]\tTrain Loss: 0.000118\n","Train Epoch: 68 [36/107 (34%)]\tTrain Loss: 0.002320\n","Train Epoch: 68 [40/107 (37%)]\tTrain Loss: 0.000336\n","Train Epoch: 68 [44/107 (41%)]\tTrain Loss: 0.080006\n","Train Epoch: 68 [48/107 (45%)]\tTrain Loss: 0.002699\n","Train Epoch: 68 [52/107 (49%)]\tTrain Loss: 0.011964\n","Train Epoch: 68 [56/107 (52%)]\tTrain Loss: 0.000877\n","Train Epoch: 68 [60/107 (56%)]\tTrain Loss: 0.000132\n","Train Epoch: 68 [64/107 (60%)]\tTrain Loss: 0.000228\n","Train Epoch: 68 [68/107 (64%)]\tTrain Loss: 0.000368\n","Train Epoch: 68 [72/107 (67%)]\tTrain Loss: 0.005831\n","Train Epoch: 68 [76/107 (71%)]\tTrain Loss: 0.000307\n","Train Epoch: 68 [80/107 (75%)]\tTrain Loss: 0.000407\n","Train Epoch: 68 [84/107 (79%)]\tTrain Loss: 0.001161\n","Train Epoch: 68 [88/107 (82%)]\tTrain Loss: 0.064928\n","Train Epoch: 68 [92/107 (86%)]\tTrain Loss: 0.000348\n","Train Epoch: 68 [96/107 (90%)]\tTrain Loss: 0.000343\n","Train Epoch: 68 [100/107 (93%)]\tTrain Loss: 0.054273\n","Train Epoch: 68 [104/107 (97%)]\tTrain Loss: 0.002509\n","target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n","score [3.01968418e-02 8.41800869e-01 5.84678948e-01 3.89122702e-02\n"," 6.77670445e-03 5.18293260e-03 8.38992596e-01 6.20224178e-01\n"," 1.51776813e-03 8.47702265e-01 4.27924186e-01 4.48535949e-01\n"," 6.37478977e-02 1.56230956e-01 5.74616134e-01 2.93317189e-05\n"," 8.97119171e-05 7.53027439e-01 4.47179511e-04 3.41065391e-03\n"," 8.16621352e-04 8.57473135e-01 2.19196364e-01 9.94060338e-01\n"," 9.31821942e-01 6.66387916e-01 9.99833345e-01 2.13359279e-04\n"," 5.63860158e-05 1.35439535e-04 3.07063060e-03 7.77583616e-03\n"," 1.94284003e-02 5.25952100e-06 7.28988482e-07 2.87907169e-04\n"," 1.17729208e-03 8.02873671e-01 9.75722253e-01 3.77068383e-04\n"," 1.57409988e-03 1.33635001e-02 5.23846745e-02 2.84537673e-01\n"," 8.63185450e-02 2.63509989e-01 5.39150119e-01 3.64822656e-01\n"," 9.41403270e-01 8.33924890e-01 9.89307225e-01 2.11827427e-07\n"," 3.81051564e-06 4.58893803e-04 6.60961899e-13 2.73966834e-05\n"," 5.00602245e-01 2.04542855e-10 4.81580775e-10 5.52262245e-05\n"," 9.99963045e-01 9.99882340e-01 9.99995828e-01 9.99972224e-01\n"," 9.90411282e-01 9.82206941e-01 8.64900470e-01 9.99794543e-01\n"," 9.98305440e-01 7.29940534e-01 9.91767466e-01 9.89703834e-01\n"," 9.91551280e-01 9.99319315e-01 8.09315264e-01 9.39770043e-01\n"," 9.99912143e-01 9.99871850e-01 9.92290437e-01 9.97825265e-01\n"," 9.99721706e-01 9.97321784e-01 9.99534249e-01 9.91406322e-01\n"," 9.98974085e-01 9.99763429e-01 9.87652063e-01 9.91636455e-01\n"," 9.43702107e-05 9.59329903e-01 9.65947747e-01 9.31084454e-01\n"," 8.87814611e-02 9.99790251e-01 9.98839080e-01 9.93577719e-01\n"," 3.23139154e-03 9.93017554e-01 2.54282773e-01 9.69919741e-01\n"," 9.41537201e-01 9.99566257e-01 9.29478824e-01 3.05820763e-01\n"," 3.98780942e-01 7.67347634e-01 9.95118499e-01 6.47294056e-03\n"," 2.02057451e-01 8.80773589e-02 1.36573822e-03 9.77457106e-01\n"," 9.40106511e-01 2.55580068e-01 5.54241478e-01 8.93197179e-01\n"," 9.99896646e-01 9.99887943e-01]\n","predict [0. 1. 1. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0. 1.\n"," 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0.\n"," 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1.\n"," 0. 1. 0. 1. 1. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 1. 1. 0. 1. 1. 1. 1.]\n","Train Epoch: 69 [0/107 (0%)]\tTrain Loss: 0.000538\n","Train Epoch: 69 [4/107 (4%)]\tTrain Loss: 0.001383\n","Train Epoch: 69 [8/107 (7%)]\tTrain Loss: 0.001372\n","Train Epoch: 69 [12/107 (11%)]\tTrain Loss: 0.015716\n","Train Epoch: 69 [16/107 (15%)]\tTrain Loss: 0.001241\n","Train Epoch: 69 [20/107 (19%)]\tTrain Loss: 0.004250\n","Train Epoch: 69 [24/107 (22%)]\tTrain Loss: 0.004825\n","Train Epoch: 69 [28/107 (26%)]\tTrain Loss: 0.006003\n","Train Epoch: 69 [32/107 (30%)]\tTrain Loss: 0.024006\n","Train Epoch: 69 [36/107 (34%)]\tTrain Loss: 0.016888\n","Train Epoch: 69 [40/107 (37%)]\tTrain Loss: 0.003409\n","Train Epoch: 69 [44/107 (41%)]\tTrain Loss: 0.004405\n","Train Epoch: 69 [48/107 (45%)]\tTrain Loss: 0.000915\n","Train Epoch: 69 [52/107 (49%)]\tTrain Loss: 0.035713\n","Train Epoch: 69 [56/107 (52%)]\tTrain Loss: 0.050256\n","Train Epoch: 69 [60/107 (56%)]\tTrain Loss: 0.006119\n","Train Epoch: 69 [64/107 (60%)]\tTrain Loss: 0.000533\n","Train Epoch: 69 [68/107 (64%)]\tTrain Loss: 0.000882\n","Train Epoch: 69 [72/107 (67%)]\tTrain Loss: 0.000675\n","Train Epoch: 69 [76/107 (71%)]\tTrain Loss: 0.009898\n","Train Epoch: 69 [80/107 (75%)]\tTrain Loss: 0.004225\n","Train Epoch: 69 [84/107 (79%)]\tTrain Loss: 0.001671\n","Train Epoch: 69 [88/107 (82%)]\tTrain Loss: 0.003487\n","Train Epoch: 69 [92/107 (86%)]\tTrain Loss: 0.002612\n","Train Epoch: 69 [96/107 (90%)]\tTrain Loss: 0.000694\n","Train Epoch: 69 [100/107 (93%)]\tTrain Loss: 0.148777\n","Train Epoch: 69 [104/107 (97%)]\tTrain Loss: 0.000084\n","target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n","score [2.03236029e-01 5.40975392e-01 2.18964461e-02 2.33759075e-01\n"," 6.36945770e-05 1.91238169e-02 3.94994229e-01 2.49221280e-01\n"," 4.63506253e-03 1.42434826e-02 1.35533102e-02 3.14616133e-04\n"," 9.36123193e-04 4.12635185e-04 3.83347459e-02 2.10733120e-07\n"," 3.32997593e-06 9.32124481e-02 4.65025769e-05 1.36829185e-04\n"," 4.72619839e-04 9.90873575e-01 9.00019765e-01 9.99968886e-01\n"," 9.40083742e-01 7.47045457e-01 9.99419332e-01 6.42368514e-06\n"," 8.40182838e-06 2.15112850e-05 1.52226549e-03 1.94165169e-03\n"," 2.52759014e-03 5.40744232e-08 4.11063183e-10 4.31755467e-07\n"," 2.21941468e-06 5.96137464e-01 2.97612157e-02 5.81863491e-09\n"," 2.94540285e-08 5.61865306e-07 3.80816728e-01 2.67700016e-01\n"," 8.17361593e-01 5.01239439e-04 9.40980166e-02 1.03524877e-02\n"," 3.62788737e-01 5.96013814e-02 9.97698724e-01 1.08314040e-07\n"," 2.66338293e-06 2.24991440e-04 2.84889197e-19 6.71630960e-06\n"," 2.96590090e-01 2.82051384e-14 4.83860209e-11 2.02805222e-06\n"," 9.99812901e-01 9.99174774e-01 9.99847531e-01 9.99449551e-01\n"," 9.99878049e-01 9.97406900e-01 8.72777164e-01 9.99981403e-01\n"," 9.80457008e-01 9.84339476e-01 9.50665832e-01 4.97558266e-01\n"," 9.99421954e-01 9.99124587e-01 8.83854032e-01 9.09784913e-01\n"," 9.99702632e-01 9.98708487e-01 9.99245286e-01 9.78267550e-01\n"," 9.95179772e-01 7.27415681e-01 9.94258285e-01 9.40536797e-01\n"," 9.28062022e-01 9.93539274e-01 9.99787271e-01 9.98911858e-01\n"," 2.62326876e-06 6.26292467e-01 7.07771629e-02 9.31368589e-01\n"," 5.28419353e-02 9.99848127e-01 9.73877430e-01 9.70917523e-01\n"," 2.30043203e-01 1.05253816e-01 1.73709891e-03 6.91395581e-01\n"," 6.46509416e-03 9.98004019e-01 5.78821986e-04 1.59792285e-02\n"," 3.68626602e-02 2.08978001e-02 9.98441994e-01 1.35129867e-02\n"," 3.12034041e-02 4.32722550e-03 1.06716540e-03 1.52950928e-01\n"," 9.81311500e-01 4.84825313e-01 2.05102144e-03 1.35811299e-01\n"," 9.99810159e-01 9.99902129e-01]\n","predict [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1.\n"," 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n"," 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0. 1. 0. 1. 1. 1.\n"," 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1.]\n","Train Epoch: 70 [0/107 (0%)]\tTrain Loss: 0.000045\n","Train Epoch: 70 [4/107 (4%)]\tTrain Loss: 0.049173\n","Train Epoch: 70 [8/107 (7%)]\tTrain Loss: 0.000141\n","Train Epoch: 70 [12/107 (11%)]\tTrain Loss: 0.000029\n","Train Epoch: 70 [16/107 (15%)]\tTrain Loss: 0.002465\n","Train Epoch: 70 [20/107 (19%)]\tTrain Loss: 0.002028\n","Train Epoch: 70 [24/107 (22%)]\tTrain Loss: 0.004730\n","Train Epoch: 70 [28/107 (26%)]\tTrain Loss: 0.000146\n","Train Epoch: 70 [32/107 (30%)]\tTrain Loss: 0.000085\n","Train Epoch: 70 [36/107 (34%)]\tTrain Loss: 0.002106\n","Train Epoch: 70 [40/107 (37%)]\tTrain Loss: 0.000453\n","Train Epoch: 70 [44/107 (41%)]\tTrain Loss: 0.046889\n","Train Epoch: 70 [48/107 (45%)]\tTrain Loss: 0.006230\n","Train Epoch: 70 [52/107 (49%)]\tTrain Loss: 0.033633\n","Train Epoch: 70 [56/107 (52%)]\tTrain Loss: 0.010729\n","Train Epoch: 70 [60/107 (56%)]\tTrain Loss: 0.000403\n","Train Epoch: 70 [64/107 (60%)]\tTrain Loss: 0.001764\n","Train Epoch: 70 [68/107 (64%)]\tTrain Loss: 0.104435\n","Train Epoch: 70 [72/107 (67%)]\tTrain Loss: 0.012181\n","Train Epoch: 70 [76/107 (71%)]\tTrain Loss: 0.073976\n","Train Epoch: 70 [80/107 (75%)]\tTrain Loss: 0.084233\n","Train Epoch: 70 [84/107 (79%)]\tTrain Loss: 0.011499\n","Train Epoch: 70 [88/107 (82%)]\tTrain Loss: 0.000406\n","Train Epoch: 70 [92/107 (86%)]\tTrain Loss: 0.030537\n","Train Epoch: 70 [96/107 (90%)]\tTrain Loss: 0.045833\n","Train Epoch: 70 [100/107 (93%)]\tTrain Loss: 0.013682\n","Train Epoch: 70 [104/107 (97%)]\tTrain Loss: 0.002500\n","target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n","score [9.29346621e-01 9.87098038e-01 6.70558751e-01 9.71891999e-01\n"," 2.05739006e-01 3.46783638e-01 9.74153221e-01 9.10156429e-01\n"," 1.65702794e-02 5.85959196e-01 9.58192050e-01 5.62425375e-01\n"," 2.03377619e-01 6.51569068e-02 2.03886315e-01 8.65048729e-04\n"," 2.70575448e-03 8.55570495e-01 2.66723800e-03 2.82370210e-01\n"," 5.72146237e-01 9.89901543e-01 9.98034894e-01 9.99972582e-01\n"," 6.46885872e-01 9.98485625e-01 9.99748051e-01 7.85444456e-04\n"," 1.57847870e-02 2.20010076e-02 9.38628837e-02 1.60478041e-01\n"," 5.32914340e-01 2.37486747e-04 2.30762998e-05 8.16286600e-04\n"," 3.46811526e-02 9.98190105e-01 3.93640965e-01 2.10795668e-03\n"," 2.67403573e-03 8.41905456e-03 8.06607723e-01 7.28775740e-01\n"," 9.88545477e-01 9.40159082e-01 9.94625628e-01 9.70100582e-01\n"," 9.89549279e-01 7.12269664e-01 9.98775303e-01 5.17899537e-08\n"," 1.29079461e-04 1.23677164e-01 2.04017749e-20 1.16961985e-06\n"," 2.97384024e-01 1.46053002e-15 4.56394655e-09 5.12994919e-03\n"," 9.99922514e-01 9.99816239e-01 9.99950647e-01 9.99889612e-01\n"," 9.99088287e-01 9.69002724e-01 9.86551523e-01 9.99965191e-01\n"," 9.99610364e-01 9.99172866e-01 9.91356373e-01 9.97059107e-01\n"," 9.99947071e-01 9.99811471e-01 9.99556243e-01 9.89225209e-01\n"," 9.99691248e-01 9.99498725e-01 9.96727228e-01 9.96892631e-01\n"," 9.99036193e-01 9.75382566e-01 9.97765183e-01 9.95400846e-01\n"," 9.91801798e-01 9.99975324e-01 9.99779165e-01 9.95465875e-01\n"," 3.37028019e-02 9.77047086e-01 9.70918775e-01 9.03426051e-01\n"," 8.44367087e-01 9.99999166e-01 9.99936342e-01 9.99810755e-01\n"," 9.18151855e-01 7.26539612e-01 9.04708445e-01 5.68374634e-01\n"," 3.22097689e-01 9.98980463e-01 1.82391405e-01 8.50805163e-01\n"," 9.76471603e-01 9.75588799e-01 9.93962586e-01 4.77459572e-10\n"," 8.34716614e-14 6.03554489e-12 1.28390910e-13 8.81888904e-04\n"," 3.08669951e-05 9.98327315e-01 8.41352105e-01 6.05224431e-01\n"," 9.99999762e-01 2.52742529e-01]\n","predict [1. 1. 1. 1. 0. 0. 1. 1. 0. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 1.\n"," 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 0. 1. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0.]\n","vote_pred [0. 1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1.\n"," 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0.\n"," 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1.\n"," 0. 1. 0. 1. 0. 1. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1.]\n","targetlist [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n","TP= 47 TN= 43 FN= 11 FP= 17\n","TP+FP 64\n","precision 0.734375\n","recall 0.8103448275862069\n","F1 0.7704918032786885\n","acc 0.7627118644067796\n","AUCp 0.7635057471264367\n","AUC 0.8560344827586207\n","\n"," The epoch is 70, average recall: 0.8103, average precision: 0.7344,average F1: 0.7705, average accuracy: 0.7627, average AUC: 0.8560\n","Train Epoch: 71 [0/107 (0%)]\tTrain Loss: 0.031021\n","Train Epoch: 71 [4/107 (4%)]\tTrain Loss: 0.070881\n","Train Epoch: 71 [8/107 (7%)]\tTrain Loss: 0.001388\n","Train Epoch: 71 [12/107 (11%)]\tTrain Loss: 0.001236\n","Train Epoch: 71 [16/107 (15%)]\tTrain Loss: 0.000826\n","Train Epoch: 71 [20/107 (19%)]\tTrain Loss: 0.010158\n","Train Epoch: 71 [24/107 (22%)]\tTrain Loss: 0.021915\n","Train Epoch: 71 [28/107 (26%)]\tTrain Loss: 0.098646\n","Train Epoch: 71 [32/107 (30%)]\tTrain Loss: 0.001615\n","Train Epoch: 71 [36/107 (34%)]\tTrain Loss: 0.003943\n","Train Epoch: 71 [40/107 (37%)]\tTrain Loss: 0.001846\n","Train Epoch: 71 [44/107 (41%)]\tTrain Loss: 0.016758\n","Train Epoch: 71 [48/107 (45%)]\tTrain Loss: 0.021819\n","Train Epoch: 71 [52/107 (49%)]\tTrain Loss: 0.027738\n","Train Epoch: 71 [56/107 (52%)]\tTrain Loss: 0.002409\n","Train Epoch: 71 [60/107 (56%)]\tTrain Loss: 0.009102\n","Train Epoch: 71 [64/107 (60%)]\tTrain Loss: 0.000671\n","Train Epoch: 71 [68/107 (64%)]\tTrain Loss: 0.001920\n","Train Epoch: 71 [72/107 (67%)]\tTrain Loss: 0.000406\n","Train Epoch: 71 [76/107 (71%)]\tTrain Loss: 0.109070\n","Train Epoch: 71 [80/107 (75%)]\tTrain Loss: 0.000314\n","Train Epoch: 71 [84/107 (79%)]\tTrain Loss: 0.000751\n","Train Epoch: 71 [88/107 (82%)]\tTrain Loss: 0.000556\n","Train Epoch: 71 [92/107 (86%)]\tTrain Loss: 0.122309\n","Train Epoch: 71 [96/107 (90%)]\tTrain Loss: 0.006150\n","Train Epoch: 71 [100/107 (93%)]\tTrain Loss: 0.000606\n","Train Epoch: 71 [104/107 (97%)]\tTrain Loss: 0.000330\n","target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n","score [9.58703220e-01 9.98449922e-01 7.08778679e-01 9.93115962e-01\n"," 5.95241189e-01 5.00981152e-01 9.88782823e-01 8.76958609e-01\n"," 1.75023843e-02 8.73114049e-01 9.97984529e-01 7.39069700e-01\n"," 9.94464457e-01 7.53486812e-01 9.31920409e-01 1.26000505e-03\n"," 4.10258397e-03 6.02114677e-01 8.33685102e-04 3.52928430e-01\n"," 1.11110114e-01 9.66010809e-01 9.98729169e-01 9.99949813e-01\n"," 9.34668720e-01 9.97642696e-01 9.99578297e-01 4.80548531e-01\n"," 5.31957075e-02 4.92605716e-02 4.22049344e-01 2.28725076e-01\n"," 7.97285140e-01 4.02156584e-04 1.78821982e-04 2.74608117e-02\n"," 2.71121208e-02 6.36000156e-01 7.05403745e-01 7.87883939e-04\n"," 4.81537048e-04 9.74282995e-03 9.23340976e-01 9.50769961e-01\n"," 9.97477591e-01 7.92815447e-01 9.91414249e-01 9.56188381e-01\n"," 9.96917963e-01 7.77249038e-01 9.99579608e-01 1.45310185e-06\n"," 1.20954274e-03 2.00124100e-01 5.75424264e-18 7.95558299e-06\n"," 6.07636809e-01 1.17109640e-14 2.24069208e-09 5.04944175e-02\n"," 9.99982238e-01 9.99890327e-01 9.99912739e-01 9.99943733e-01\n"," 9.99983191e-01 9.98555958e-01 9.83185530e-01 9.99982834e-01\n"," 9.98597801e-01 9.99734581e-01 9.72365618e-01 9.88868117e-01\n"," 9.99983668e-01 9.99943495e-01 9.99544919e-01 9.99136150e-01\n"," 9.99744833e-01 9.99838352e-01 9.99979377e-01 9.85081911e-01\n"," 9.97033119e-01 9.99019265e-01 9.99533534e-01 9.89347339e-01\n"," 9.79821920e-01 9.96478975e-01 9.99891520e-01 9.99468744e-01\n"," 7.91961476e-02 9.71825838e-01 9.78293002e-01 9.98041987e-01\n"," 9.91511762e-01 9.99951482e-01 9.99778092e-01 9.99077439e-01\n"," 8.73233557e-01 4.76996452e-01 9.86397207e-01 9.91600275e-01\n"," 9.63629425e-01 9.99452889e-01 4.40285891e-01 7.02371240e-01\n"," 9.60207283e-01 9.78136480e-01 9.99655128e-01 3.62765864e-02\n"," 2.43015209e-04 1.58612132e-01 1.06097572e-03 1.38338476e-01\n"," 9.86916304e-01 9.95223582e-01 3.86825562e-01 5.88405967e-01\n"," 9.99994874e-01 9.99678493e-01]\n","predict [1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 0. 1. 0. 0. 0. 1. 1. 1.\n"," 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 1. 0. 1. 1. 1.]\n","Train Epoch: 72 [0/107 (0%)]\tTrain Loss: 0.000652\n","Train Epoch: 72 [4/107 (4%)]\tTrain Loss: 0.000293\n","Train Epoch: 72 [8/107 (7%)]\tTrain Loss: 0.001132\n","Train Epoch: 72 [12/107 (11%)]\tTrain Loss: 0.011056\n","Train Epoch: 72 [16/107 (15%)]\tTrain Loss: 0.000576\n","Train Epoch: 72 [20/107 (19%)]\tTrain Loss: 0.000136\n","Train Epoch: 72 [24/107 (22%)]\tTrain Loss: 0.002801\n","Train Epoch: 72 [28/107 (26%)]\tTrain Loss: 0.001736\n","Train Epoch: 72 [32/107 (30%)]\tTrain Loss: 0.007751\n","Train Epoch: 72 [36/107 (34%)]\tTrain Loss: 0.020619\n","Train Epoch: 72 [40/107 (37%)]\tTrain Loss: 0.000635\n","Train Epoch: 72 [44/107 (41%)]\tTrain Loss: 0.000539\n","Train Epoch: 72 [48/107 (45%)]\tTrain Loss: 0.002133\n","Train Epoch: 72 [52/107 (49%)]\tTrain Loss: 0.000331\n","Train Epoch: 72 [56/107 (52%)]\tTrain Loss: 0.134661\n","Train Epoch: 72 [60/107 (56%)]\tTrain Loss: 0.008769\n","Train Epoch: 72 [64/107 (60%)]\tTrain Loss: 0.000311\n","Train Epoch: 72 [68/107 (64%)]\tTrain Loss: 0.071942\n","Train Epoch: 72 [72/107 (67%)]\tTrain Loss: 0.005065\n","Train Epoch: 72 [76/107 (71%)]\tTrain Loss: 0.002982\n","Train Epoch: 72 [80/107 (75%)]\tTrain Loss: 0.001360\n","Train Epoch: 72 [84/107 (79%)]\tTrain Loss: 0.011288\n","Train Epoch: 72 [88/107 (82%)]\tTrain Loss: 0.001633\n","Train Epoch: 72 [92/107 (86%)]\tTrain Loss: 0.002448\n","Train Epoch: 72 [96/107 (90%)]\tTrain Loss: 0.005275\n","Train Epoch: 72 [100/107 (93%)]\tTrain Loss: 0.013352\n","Train Epoch: 72 [104/107 (97%)]\tTrain Loss: 0.010262\n","target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n","score [7.43855014e-02 8.75811756e-01 4.99173366e-02 9.27810967e-01\n"," 2.43707839e-02 8.56260285e-02 8.78569424e-01 5.38809240e-01\n"," 4.82731592e-03 4.37467545e-01 8.66873562e-01 5.61202131e-02\n"," 7.32106864e-01 2.57301163e-02 5.69233298e-01 2.13667354e-05\n"," 1.63905454e-04 5.61840646e-02 8.20854111e-05 3.36690471e-02\n"," 3.42581645e-02 9.44783092e-01 9.96704638e-01 9.99709427e-01\n"," 3.19838494e-01 9.93337870e-01 9.99139667e-01 4.71149484e-04\n"," 7.88408623e-04 4.32523608e-04 1.66706042e-03 9.73419659e-03\n"," 6.50793165e-02 9.72707198e-07 4.48008102e-08 1.51225622e-05\n"," 5.78668769e-05 5.23228496e-02 1.98883638e-02 4.95618515e-05\n"," 3.30531293e-05 9.42948856e-04 1.25743553e-01 4.00049761e-02\n"," 9.82541382e-01 9.65037942e-02 4.44034487e-01 5.15302360e-01\n"," 1.19830839e-01 2.96500642e-02 8.91471028e-01 4.37080677e-10\n"," 3.25261935e-05 4.31956287e-04 6.78043919e-18 6.06775998e-08\n"," 3.83739412e-01 2.13408405e-15 1.31469735e-09 1.46526887e-04\n"," 9.99694228e-01 9.99423742e-01 9.99345005e-01 9.99291778e-01\n"," 9.99973774e-01 9.81223941e-01 8.50787401e-01 9.99967694e-01\n"," 9.98412609e-01 9.87556696e-01 5.80751359e-01 4.58536536e-01\n"," 9.99880672e-01 9.98079419e-01 9.98407423e-01 9.97002184e-01\n"," 9.92894590e-01 9.97091770e-01 9.97665882e-01 9.81766403e-01\n"," 9.98291790e-01 9.99441564e-01 9.97276008e-01 7.80795634e-01\n"," 9.30269659e-01 9.14725184e-01 9.97224569e-01 9.92987216e-01\n"," 2.27943156e-03 9.68885660e-01 8.63692582e-01 8.85572553e-01\n"," 8.10275078e-01 9.99715149e-01 9.96689558e-01 9.93585646e-01\n"," 2.99818814e-01 3.69910479e-01 7.40281224e-01 7.32496023e-01\n"," 8.35913897e-01 9.89687145e-01 1.63088202e-01 7.17059374e-01\n"," 9.37940836e-01 9.40601647e-01 9.83956516e-01 7.29896274e-05\n"," 1.98369321e-07 3.92201946e-05 2.89686659e-06 7.39510283e-02\n"," 5.46889305e-01 9.85475481e-01 6.18326366e-02 2.02747211e-01\n"," 9.99270976e-01 7.44413793e-01]\n","predict [0. 1. 0. 1. 0. 0. 1. 1. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1.\n"," 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1.\n"," 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1.\n"," 0. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 1.]\n","Train Epoch: 73 [0/107 (0%)]\tTrain Loss: 0.001805\n","Train Epoch: 73 [4/107 (4%)]\tTrain Loss: 0.004944\n","Train Epoch: 73 [8/107 (7%)]\tTrain Loss: 0.004249\n","Train Epoch: 73 [12/107 (11%)]\tTrain Loss: 0.000342\n","Train Epoch: 73 [16/107 (15%)]\tTrain Loss: 0.001264\n","Train Epoch: 73 [20/107 (19%)]\tTrain Loss: 0.000618\n","Train Epoch: 73 [24/107 (22%)]\tTrain Loss: 0.004357\n","Train Epoch: 73 [28/107 (26%)]\tTrain Loss: 0.000979\n","Train Epoch: 73 [32/107 (30%)]\tTrain Loss: 0.002716\n","Train Epoch: 73 [36/107 (34%)]\tTrain Loss: 0.000275\n","Train Epoch: 73 [40/107 (37%)]\tTrain Loss: 0.000163\n","Train Epoch: 73 [44/107 (41%)]\tTrain Loss: 0.002495\n","Train Epoch: 73 [48/107 (45%)]\tTrain Loss: 0.000095\n","Train Epoch: 73 [52/107 (49%)]\tTrain Loss: 0.000131\n","Train Epoch: 73 [56/107 (52%)]\tTrain Loss: 0.000793\n","Train Epoch: 73 [60/107 (56%)]\tTrain Loss: 0.000173\n","Train Epoch: 73 [64/107 (60%)]\tTrain Loss: 0.000308\n","Train Epoch: 73 [68/107 (64%)]\tTrain Loss: 0.000798\n","Train Epoch: 73 [72/107 (67%)]\tTrain Loss: 0.024318\n","Train Epoch: 73 [76/107 (71%)]\tTrain Loss: 0.001004\n","Train Epoch: 73 [80/107 (75%)]\tTrain Loss: 0.015941\n","Train Epoch: 73 [84/107 (79%)]\tTrain Loss: 0.006756\n","Train Epoch: 73 [88/107 (82%)]\tTrain Loss: 0.000202\n","Train Epoch: 73 [92/107 (86%)]\tTrain Loss: 0.000547\n","Train Epoch: 73 [96/107 (90%)]\tTrain Loss: 0.001909\n","Train Epoch: 73 [100/107 (93%)]\tTrain Loss: 0.010420\n","Train Epoch: 73 [104/107 (97%)]\tTrain Loss: 0.000247\n","target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n","score [1.97409958e-01 9.27988529e-01 6.34568453e-01 8.96242619e-01\n"," 6.12274073e-02 7.45614171e-02 9.94243443e-01 8.10743511e-01\n"," 2.96008866e-03 9.17572975e-01 9.39787388e-01 5.98413825e-01\n"," 1.84590429e-01 5.00171222e-02 3.24926645e-01 4.61540258e-05\n"," 2.53946497e-03 3.10881156e-02 2.89090880e-04 6.50846064e-02\n"," 1.96387812e-01 9.92256880e-01 9.98739302e-01 9.99853730e-01\n"," 8.63027632e-01 9.99595940e-01 9.99715030e-01 1.51205463e-02\n"," 3.98679171e-03 5.41306427e-03 3.80065292e-02 6.78080842e-02\n"," 1.52491421e-01 7.38972221e-06 1.67418796e-06 8.66533883e-05\n"," 3.21222877e-04 2.12524056e-01 2.85621751e-02 1.86634134e-04\n"," 1.76933245e-04 2.40006787e-03 1.82008877e-01 8.47732201e-02\n"," 9.92321312e-01 3.74872297e-01 9.59559679e-01 8.96902859e-01\n"," 8.20971012e-01 4.68537867e-01 9.92332339e-01 1.76342174e-09\n"," 9.58432574e-05 1.20171322e-03 3.13582913e-17 1.20826925e-07\n"," 5.85388184e-01 6.75550915e-15 1.72010661e-09 4.24619939e-04\n"," 9.99925733e-01 9.99748170e-01 9.99879718e-01 9.99891520e-01\n"," 9.99992967e-01 9.94770348e-01 9.51279402e-01 9.99995470e-01\n"," 9.99989867e-01 9.94963527e-01 6.50587440e-01 3.38977426e-01\n"," 9.99990702e-01 9.99735653e-01 9.99892116e-01 9.99642491e-01\n"," 9.99485016e-01 9.99409556e-01 9.99873757e-01 9.95897710e-01\n"," 9.99894261e-01 9.99935150e-01 9.99194801e-01 8.99087787e-01\n"," 9.43869829e-01 9.97619092e-01 9.99738157e-01 9.99449790e-01\n"," 1.18750958e-02 9.74081218e-01 9.81542647e-01 9.19671297e-01\n"," 8.91757846e-01 9.99993682e-01 9.99751508e-01 9.99512672e-01\n"," 4.54849631e-01 9.58371997e-01 7.07561553e-01 9.93945539e-01\n"," 6.07932150e-01 9.99073744e-01 4.55372602e-01 8.90074730e-01\n"," 9.87244427e-01 9.90118623e-01 9.88791227e-01 2.75502516e-05\n"," 1.20046435e-08 3.08093917e-07 1.25838060e-08 5.13982236e-01\n"," 2.03723162e-02 9.93559480e-01 3.83818209e-01 3.07223618e-01\n"," 9.99962807e-01 9.79135215e-01]\n","predict [0. 1. 1. 1. 0. 0. 1. 1. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1.\n"," 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1.\n"," 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1.\n"," 0. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 1.]\n","Train Epoch: 74 [0/107 (0%)]\tTrain Loss: 0.000445\n","Train Epoch: 74 [4/107 (4%)]\tTrain Loss: 0.001169\n","Train Epoch: 74 [8/107 (7%)]\tTrain Loss: 0.005179\n","Train Epoch: 74 [12/107 (11%)]\tTrain Loss: 0.000538\n","Train Epoch: 74 [16/107 (15%)]\tTrain Loss: 0.000259\n","Train Epoch: 74 [20/107 (19%)]\tTrain Loss: 0.002058\n","Train Epoch: 74 [24/107 (22%)]\tTrain Loss: 0.012133\n","Train Epoch: 74 [28/107 (26%)]\tTrain Loss: 0.093314\n","Train Epoch: 74 [32/107 (30%)]\tTrain Loss: 0.002454\n","Train Epoch: 74 [36/107 (34%)]\tTrain Loss: 0.007633\n","Train Epoch: 74 [40/107 (37%)]\tTrain Loss: 0.000364\n","Train Epoch: 74 [44/107 (41%)]\tTrain Loss: 0.059319\n","Train Epoch: 74 [48/107 (45%)]\tTrain Loss: 0.001208\n","Train Epoch: 74 [52/107 (49%)]\tTrain Loss: 0.011818\n","Train Epoch: 74 [56/107 (52%)]\tTrain Loss: 0.000425\n","Train Epoch: 74 [60/107 (56%)]\tTrain Loss: 0.000281\n","Train Epoch: 74 [64/107 (60%)]\tTrain Loss: 0.005133\n","Train Epoch: 74 [68/107 (64%)]\tTrain Loss: 0.029515\n","Train Epoch: 74 [72/107 (67%)]\tTrain Loss: 0.000452\n","Train Epoch: 74 [76/107 (71%)]\tTrain Loss: 0.000229\n","Train Epoch: 74 [80/107 (75%)]\tTrain Loss: 0.000168\n","Train Epoch: 74 [84/107 (79%)]\tTrain Loss: 0.000457\n","Train Epoch: 74 [88/107 (82%)]\tTrain Loss: 0.001152\n","Train Epoch: 74 [92/107 (86%)]\tTrain Loss: 0.013388\n","Train Epoch: 74 [96/107 (90%)]\tTrain Loss: 0.002430\n","Train Epoch: 74 [100/107 (93%)]\tTrain Loss: 0.000249\n","Train Epoch: 74 [104/107 (97%)]\tTrain Loss: 0.000416\n","target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n","score [7.12247670e-01 9.92741227e-01 2.21558958e-01 7.87203729e-01\n"," 4.42035347e-02 1.76992744e-01 9.99318004e-01 9.52083230e-01\n"," 2.15987721e-03 6.68923482e-02 5.59511244e-01 4.89609204e-02\n"," 1.07537642e-01 3.19590606e-02 1.44674827e-03 3.57056269e-05\n"," 1.35732625e-04 2.30165645e-02 1.95734920e-05 1.20527518e-03\n"," 2.71851942e-03 7.65821517e-01 8.06865990e-01 9.99402881e-01\n"," 3.71121056e-02 9.48009610e-01 9.99912620e-01 4.20314376e-04\n"," 1.00432313e-03 5.67799085e-04 1.85769913e-03 5.60662970e-02\n"," 1.50659293e-01 1.24157286e-05 3.75017516e-05 9.44573258e-05\n"," 6.28915324e-04 9.97378111e-01 1.86464801e-01 7.07372208e-04\n"," 1.30994082e-03 3.25728282e-02 2.14259818e-01 6.13018796e-02\n"," 9.73619103e-01 8.27328801e-01 9.74022388e-01 9.84918833e-01\n"," 7.92051435e-01 1.12319514e-01 9.78363514e-01 1.00952838e-15\n"," 9.22739787e-08 3.65640381e-06 1.64285195e-29 2.13746954e-11\n"," 3.23892600e-05 4.55296926e-26 1.91766396e-15 2.29428497e-05\n"," 9.99990106e-01 9.99887347e-01 9.99987721e-01 9.99958038e-01\n"," 9.97940600e-01 9.65113699e-01 9.97797489e-01 9.99998212e-01\n"," 9.99943018e-01 9.99626994e-01 4.72396046e-01 5.75839281e-01\n"," 9.99999642e-01 9.99938846e-01 9.99922514e-01 9.58059192e-01\n"," 9.99997973e-01 9.99909401e-01 9.98374104e-01 9.99157190e-01\n"," 9.99488473e-01 9.92029548e-01 9.99146938e-01 9.51573193e-01\n"," 9.60883021e-01 9.99813497e-01 9.99030113e-01 9.93671060e-01\n"," 9.70957535e-06 3.01433921e-01 6.98141241e-03 9.17167783e-01\n"," 4.48799908e-01 9.99992013e-01 9.99957323e-01 9.96596873e-01\n"," 3.55539843e-03 9.45764780e-01 4.24828023e-01 7.10158125e-02\n"," 6.51826978e-01 9.99857306e-01 7.29251564e-01 1.13142706e-01\n"," 8.67017210e-01 4.05032784e-01 9.84064877e-01 2.49333228e-11\n"," 1.91209293e-20 1.50405608e-14 2.37475040e-20 1.67599916e-02\n"," 3.59208675e-13 1.84653744e-01 5.31913117e-02 2.25884765e-01\n"," 1.00000000e+00 9.40342247e-01]\n","predict [1. 1. 0. 1. 0. 0. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1.\n"," 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1.\n"," 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 1. 0. 1. 1. 1.\n"," 0. 1. 0. 0. 1. 1. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.]\n","Train Epoch: 75 [0/107 (0%)]\tTrain Loss: 0.000271\n","Train Epoch: 75 [4/107 (4%)]\tTrain Loss: 0.000510\n","Train Epoch: 75 [8/107 (7%)]\tTrain Loss: 0.000523\n","Train Epoch: 75 [12/107 (11%)]\tTrain Loss: 0.001927\n","Train Epoch: 75 [16/107 (15%)]\tTrain Loss: 0.000615\n","Train Epoch: 75 [20/107 (19%)]\tTrain Loss: 0.000560\n","Train Epoch: 75 [24/107 (22%)]\tTrain Loss: 0.000240\n","Train Epoch: 75 [28/107 (26%)]\tTrain Loss: 0.000285\n","Train Epoch: 75 [32/107 (30%)]\tTrain Loss: 0.002781\n","Train Epoch: 75 [36/107 (34%)]\tTrain Loss: 0.000198\n","Train Epoch: 75 [40/107 (37%)]\tTrain Loss: 0.001841\n","Train Epoch: 75 [44/107 (41%)]\tTrain Loss: 0.001027\n","Train Epoch: 75 [48/107 (45%)]\tTrain Loss: 0.001400\n","Train Epoch: 75 [52/107 (49%)]\tTrain Loss: 0.000850\n","Train Epoch: 75 [56/107 (52%)]\tTrain Loss: 0.001310\n","Train Epoch: 75 [60/107 (56%)]\tTrain Loss: 0.001517\n","Train Epoch: 75 [64/107 (60%)]\tTrain Loss: 0.005107\n","Train Epoch: 75 [68/107 (64%)]\tTrain Loss: 0.000308\n","Train Epoch: 75 [72/107 (67%)]\tTrain Loss: 0.000131\n","Train Epoch: 75 [76/107 (71%)]\tTrain Loss: 0.000611\n","Train Epoch: 75 [80/107 (75%)]\tTrain Loss: 0.003539\n","Train Epoch: 75 [84/107 (79%)]\tTrain Loss: 0.000366\n","Train Epoch: 75 [88/107 (82%)]\tTrain Loss: 0.000527\n","Train Epoch: 75 [92/107 (86%)]\tTrain Loss: 0.000329\n","Train Epoch: 75 [96/107 (90%)]\tTrain Loss: 0.000363\n","Train Epoch: 75 [100/107 (93%)]\tTrain Loss: 0.001409\n","Train Epoch: 75 [104/107 (97%)]\tTrain Loss: 0.000979\n","target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n","score [9.92666930e-02 9.97746885e-01 9.72065091e-01 9.79694068e-01\n"," 3.88005376e-02 1.33914128e-01 9.99554932e-01 9.90790367e-01\n"," 2.80028861e-03 6.02726817e-01 8.83576393e-01 1.57599613e-01\n"," 2.71936990e-02 2.08541248e-02 2.84306221e-02 1.89789228e-06\n"," 3.79445664e-05 2.51375586e-01 2.85804243e-04 5.38323820e-02\n"," 5.85019104e-02 9.99762475e-01 9.99046624e-01 9.99858141e-01\n"," 9.83193040e-01 9.98701692e-01 9.99502063e-01 1.79724246e-02\n"," 6.90351473e-03 6.79818168e-03 4.24636714e-03 1.20324112e-01\n"," 8.05655047e-02 1.95305129e-05 1.11270228e-05 2.63080583e-05\n"," 1.46626926e-03 9.93997574e-01 6.56098545e-01 1.16084004e-04\n"," 3.07045993e-04 4.91622137e-03 9.54709649e-01 1.90187380e-01\n"," 9.47426617e-01 3.01393032e-01 6.20594382e-01 4.50437039e-01\n"," 4.76322412e-01 9.48411971e-02 9.37273383e-01 4.90595079e-14\n"," 4.88739806e-06 4.21872710e-06 1.29958502e-26 2.47859799e-09\n"," 4.41679207e-04 5.76798291e-24 9.19497429e-14 2.09466743e-04\n"," 9.99956250e-01 9.99396920e-01 9.99889851e-01 9.99844670e-01\n"," 9.99908566e-01 9.99257028e-01 9.95458007e-01 9.99998093e-01\n"," 9.99900222e-01 9.97817278e-01 8.19194540e-02 6.53510988e-02\n"," 9.99954104e-01 9.97856915e-01 9.99492884e-01 8.43961120e-01\n"," 9.99559820e-01 9.99401093e-01 9.98363793e-01 9.97728646e-01\n"," 9.99753177e-01 9.98702645e-01 9.98955250e-01 9.76006150e-01\n"," 8.91658127e-01 9.99165654e-01 9.99102354e-01 9.98387456e-01\n"," 1.75909707e-04 7.23803699e-01 5.29357612e-01 8.57277691e-01\n"," 8.90447080e-01 9.99998808e-01 9.99931335e-01 9.98855710e-01\n"," 8.89833689e-01 9.98625159e-01 1.01892151e-01 3.66365850e-01\n"," 3.42236012e-01 9.99337018e-01 9.22526360e-01 2.79527575e-01\n"," 9.00190771e-01 3.39378715e-01 8.55271578e-01 1.32071546e-06\n"," 2.95472495e-13 6.98763998e-08 8.17638600e-14 1.25419244e-01\n"," 2.71378269e-07 9.98851180e-01 9.32119966e-01 6.69684529e-01\n"," 9.99995828e-01 9.15632665e-01]\n","predict [0. 1. 1. 1. 0. 0. 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1.\n"," 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 1. 0. 1. 0.\n"," 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 0. 0. 0. 1. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1.]\n","Train Epoch: 76 [0/107 (0%)]\tTrain Loss: 0.036857\n","Train Epoch: 76 [4/107 (4%)]\tTrain Loss: 0.000041\n","Train Epoch: 76 [8/107 (7%)]\tTrain Loss: 0.001668\n","Train Epoch: 76 [12/107 (11%)]\tTrain Loss: 0.004872\n","Train Epoch: 76 [16/107 (15%)]\tTrain Loss: 0.001223\n","Train Epoch: 76 [20/107 (19%)]\tTrain Loss: 0.005065\n","Train Epoch: 76 [24/107 (22%)]\tTrain Loss: 0.004991\n","Train Epoch: 76 [28/107 (26%)]\tTrain Loss: 0.004652\n","Train Epoch: 76 [32/107 (30%)]\tTrain Loss: 0.000525\n","Train Epoch: 76 [36/107 (34%)]\tTrain Loss: 0.011972\n","Train Epoch: 76 [40/107 (37%)]\tTrain Loss: 0.000206\n","Train Epoch: 76 [44/107 (41%)]\tTrain Loss: 0.000242\n","Train Epoch: 76 [48/107 (45%)]\tTrain Loss: 0.003034\n","Train Epoch: 76 [52/107 (49%)]\tTrain Loss: 0.000069\n","Train Epoch: 76 [56/107 (52%)]\tTrain Loss: 0.000353\n","Train Epoch: 76 [60/107 (56%)]\tTrain Loss: 0.010439\n","Train Epoch: 76 [64/107 (60%)]\tTrain Loss: 0.000116\n","Train Epoch: 76 [68/107 (64%)]\tTrain Loss: 0.000984\n","Train Epoch: 76 [72/107 (67%)]\tTrain Loss: 0.000090\n","Train Epoch: 76 [76/107 (71%)]\tTrain Loss: 0.000052\n","Train Epoch: 76 [80/107 (75%)]\tTrain Loss: 0.000205\n","Train Epoch: 76 [84/107 (79%)]\tTrain Loss: 0.020102\n","Train Epoch: 76 [88/107 (82%)]\tTrain Loss: 0.000603\n","Train Epoch: 76 [92/107 (86%)]\tTrain Loss: 0.000247\n","Train Epoch: 76 [96/107 (90%)]\tTrain Loss: 0.000987\n","Train Epoch: 76 [100/107 (93%)]\tTrain Loss: 0.000491\n","Train Epoch: 76 [104/107 (97%)]\tTrain Loss: 0.000122\n","target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n","score [6.15092032e-02 9.88377631e-01 9.10171151e-01 9.24298942e-01\n"," 2.63912547e-02 1.40489608e-01 9.99043167e-01 9.93531823e-01\n"," 4.21703880e-04 6.71297967e-01 4.83847886e-01 2.82059550e-01\n"," 5.76423900e-03 1.69505016e-03 1.44793268e-03 2.01905141e-06\n"," 1.17949066e-05 8.72533768e-02 2.21826558e-04 7.92588200e-03\n"," 1.02395313e-02 9.96752262e-01 9.98010457e-01 9.99467075e-01\n"," 6.83176935e-01 9.90864933e-01 9.99676943e-01 1.52305677e-03\n"," 9.32472700e-04 2.92427139e-04 1.66368193e-03 2.47058626e-02\n"," 3.17577161e-02 1.22614483e-06 3.94974592e-07 9.73584247e-06\n"," 8.49684729e-05 9.96118426e-01 5.90880275e-01 1.66283244e-05\n"," 6.18076738e-05 5.34914143e-04 7.51531303e-01 7.47128390e-03\n"," 8.94732177e-01 1.17735155e-01 5.75618505e-01 3.51235002e-01\n"," 2.71251619e-01 2.53701061e-02 8.84953916e-01 3.35165007e-09\n"," 3.36879602e-05 3.35217832e-04 1.85913690e-17 3.00623469e-06\n"," 9.94886011e-02 1.99968428e-15 4.94741026e-10 3.51885210e-05\n"," 9.99899745e-01 9.98678982e-01 9.99691010e-01 9.98986185e-01\n"," 9.99404192e-01 9.84642446e-01 9.89430070e-01 9.99992013e-01\n"," 9.99687791e-01 9.97655034e-01 1.42758548e-01 1.42255619e-01\n"," 9.99985456e-01 9.98903751e-01 9.99419332e-01 4.42113727e-01\n"," 9.97902274e-01 9.94524002e-01 9.97106731e-01 9.96706426e-01\n"," 9.99844074e-01 9.97195959e-01 9.98819530e-01 8.45569372e-01\n"," 5.59791386e-01 9.99776423e-01 9.98544335e-01 9.95326757e-01\n"," 2.65351046e-05 2.71900356e-01 7.25081086e-01 8.92682672e-01\n"," 8.09267342e-01 9.99999762e-01 9.99968171e-01 9.99825656e-01\n"," 9.54842985e-01 9.99790967e-01 5.81201315e-01 9.31816697e-02\n"," 9.14050579e-01 9.98907447e-01 9.11047041e-01 3.15578729e-01\n"," 9.20329690e-01 9.42036629e-01 9.90528226e-01 7.10844542e-05\n"," 1.51914301e-05 7.98472378e-04 2.09605992e-06 9.80262578e-01\n"," 7.35762298e-01 9.18586254e-01 6.11214563e-02 1.33979376e-02\n"," 9.99999881e-01 6.74593270e-01]\n","predict [0. 1. 1. 1. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1.\n"," 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 1. 0. 1. 0.\n"," 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0.\n"," 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1. 0. 0. 0. 0. 1. 1. 1. 0. 0. 1. 1.]\n","Train Epoch: 77 [0/107 (0%)]\tTrain Loss: 0.000333\n","Train Epoch: 77 [4/107 (4%)]\tTrain Loss: 0.029922\n","Train Epoch: 77 [8/107 (7%)]\tTrain Loss: 0.000014\n","Train Epoch: 77 [12/107 (11%)]\tTrain Loss: 0.000413\n","Train Epoch: 77 [16/107 (15%)]\tTrain Loss: 0.002148\n","Train Epoch: 77 [20/107 (19%)]\tTrain Loss: 0.000646\n","Train Epoch: 77 [24/107 (22%)]\tTrain Loss: 0.000181\n","Train Epoch: 77 [28/107 (26%)]\tTrain Loss: 0.000458\n","Train Epoch: 77 [32/107 (30%)]\tTrain Loss: 0.000123\n","Train Epoch: 77 [36/107 (34%)]\tTrain Loss: 0.000083\n","Train Epoch: 77 [40/107 (37%)]\tTrain Loss: 0.020547\n","Train Epoch: 77 [44/107 (41%)]\tTrain Loss: 0.000027\n","Train Epoch: 77 [48/107 (45%)]\tTrain Loss: 0.002239\n","Train Epoch: 77 [52/107 (49%)]\tTrain Loss: 0.000677\n","Train Epoch: 77 [56/107 (52%)]\tTrain Loss: 0.021257\n","Train Epoch: 77 [60/107 (56%)]\tTrain Loss: 0.008005\n","Train Epoch: 77 [64/107 (60%)]\tTrain Loss: 0.000542\n","Train Epoch: 77 [68/107 (64%)]\tTrain Loss: 0.001983\n","Train Epoch: 77 [72/107 (67%)]\tTrain Loss: 0.000225\n","Train Epoch: 77 [76/107 (71%)]\tTrain Loss: 0.036732\n","Train Epoch: 77 [80/107 (75%)]\tTrain Loss: 0.000210\n","Train Epoch: 77 [84/107 (79%)]\tTrain Loss: 0.001630\n","Train Epoch: 77 [88/107 (82%)]\tTrain Loss: 0.053225\n","Train Epoch: 77 [92/107 (86%)]\tTrain Loss: 0.098153\n","Train Epoch: 77 [96/107 (90%)]\tTrain Loss: 0.000158\n","Train Epoch: 77 [100/107 (93%)]\tTrain Loss: 0.001606\n","Train Epoch: 77 [104/107 (97%)]\tTrain Loss: 0.050077\n","target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n","score [9.67272639e-01 9.87256408e-01 5.90348542e-01 9.99408960e-01\n"," 8.73396814e-01 7.98947096e-01 9.89945114e-01 9.99985814e-01\n"," 2.07285836e-01 5.05945265e-01 9.99904513e-01 9.22657102e-02\n"," 9.99797285e-01 7.67005980e-01 9.59273815e-01 6.70825914e-02\n"," 1.02213612e-02 2.34952390e-01 2.01436970e-02 8.23397040e-02\n"," 2.44107977e-01 9.97956753e-01 9.99929547e-01 9.99982476e-01\n"," 8.76604199e-01 9.99465048e-01 9.99894977e-01 9.83862132e-02\n"," 4.20202464e-02 2.95594726e-02 5.74035287e-01 4.88781303e-01\n"," 4.67870563e-01 5.10364953e-05 7.90050763e-05 1.69723007e-05\n"," 3.29487986e-04 9.99492645e-01 7.34747410e-01 9.60564503e-05\n"," 4.88783662e-05 2.40168930e-03 9.95536208e-01 9.99420404e-01\n"," 9.99941945e-01 9.97677386e-01 9.97404158e-01 9.99250829e-01\n"," 9.97149527e-01 9.89388347e-01 9.99782741e-01 4.62052668e-07\n"," 1.98333897e-03 3.43290687e-01 2.88197412e-15 4.59545554e-04\n"," 6.79860830e-01 4.87241676e-14 3.31437064e-07 1.11340536e-02\n"," 9.99984980e-01 9.99783099e-01 9.99992967e-01 9.99993324e-01\n"," 9.99980092e-01 9.99873996e-01 9.98602211e-01 9.99999881e-01\n"," 9.99994874e-01 9.99976993e-01 9.98138189e-01 9.77379978e-01\n"," 9.99997020e-01 9.99987841e-01 9.99841690e-01 9.99944448e-01\n"," 9.99813974e-01 9.99780118e-01 9.99998212e-01 9.99800861e-01\n"," 9.99996781e-01 9.99983668e-01 9.99948263e-01 9.97338474e-01\n"," 9.99460757e-01 9.99991179e-01 9.99945879e-01 9.99930382e-01\n"," 2.95862905e-03 9.98739660e-01 9.93547142e-01 9.99990344e-01\n"," 9.98891056e-01 9.99999642e-01 9.99999881e-01 9.99975681e-01\n"," 9.35473621e-01 9.66004670e-01 9.21142220e-01 9.99972701e-01\n"," 9.86361384e-01 9.99991775e-01 9.87876177e-01 9.80317950e-01\n"," 9.94803727e-01 9.99238372e-01 9.99832630e-01 8.70260003e-04\n"," 3.98560091e-12 1.04099908e-03 6.68979254e-08 9.22544837e-01\n"," 2.08170675e-02 9.99547303e-01 8.77020121e-01 9.65697765e-01\n"," 9.99965191e-01 9.99990344e-01]\n","predict [1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1.\n"," 1. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 1. 0. 1. 1. 1. 1. 1.]\n","Train Epoch: 78 [0/107 (0%)]\tTrain Loss: 0.034187\n","Train Epoch: 78 [4/107 (4%)]\tTrain Loss: 0.051577\n","Train Epoch: 78 [8/107 (7%)]\tTrain Loss: 0.001075\n","Train Epoch: 78 [12/107 (11%)]\tTrain Loss: 0.000437\n","Train Epoch: 78 [16/107 (15%)]\tTrain Loss: 0.034625\n","Train Epoch: 78 [20/107 (19%)]\tTrain Loss: 0.012004\n","Train Epoch: 78 [24/107 (22%)]\tTrain Loss: 0.006094\n","Train Epoch: 78 [28/107 (26%)]\tTrain Loss: 0.005241\n","Train Epoch: 78 [32/107 (30%)]\tTrain Loss: 0.018188\n","Train Epoch: 78 [36/107 (34%)]\tTrain Loss: 0.001640\n","Train Epoch: 78 [40/107 (37%)]\tTrain Loss: 0.001487\n","Train Epoch: 78 [44/107 (41%)]\tTrain Loss: 0.004756\n","Train Epoch: 78 [48/107 (45%)]\tTrain Loss: 0.005765\n","Train Epoch: 78 [52/107 (49%)]\tTrain Loss: 0.029065\n","Train Epoch: 78 [56/107 (52%)]\tTrain Loss: 0.000809\n","Train Epoch: 78 [60/107 (56%)]\tTrain Loss: 0.005723\n","Train Epoch: 78 [64/107 (60%)]\tTrain Loss: 0.017412\n","Train Epoch: 78 [68/107 (64%)]\tTrain Loss: 0.016232\n","Train Epoch: 78 [72/107 (67%)]\tTrain Loss: 0.009553\n","Train Epoch: 78 [76/107 (71%)]\tTrain Loss: 0.005039\n","Train Epoch: 78 [80/107 (75%)]\tTrain Loss: 0.000870\n","Train Epoch: 78 [84/107 (79%)]\tTrain Loss: 0.001059\n","Train Epoch: 78 [88/107 (82%)]\tTrain Loss: 0.002285\n","Train Epoch: 78 [92/107 (86%)]\tTrain Loss: 0.000341\n","Train Epoch: 78 [96/107 (90%)]\tTrain Loss: 0.000860\n","Train Epoch: 78 [100/107 (93%)]\tTrain Loss: 0.145644\n","Train Epoch: 78 [104/107 (97%)]\tTrain Loss: 0.001686\n","target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n","score [6.55313879e-02 8.48958313e-01 5.51508367e-01 9.97260809e-01\n"," 1.46860749e-01 3.85989621e-02 9.95422304e-01 9.97663140e-01\n"," 1.44540640e-02 9.66954768e-01 9.74617720e-01 6.80649579e-01\n"," 9.31774855e-01 3.12381424e-03 2.77000993e-01 9.63830644e-06\n"," 1.87604048e-04 2.75964826e-01 4.61395277e-04 4.69919920e-01\n"," 1.76953256e-01 9.99708593e-01 9.99174654e-01 9.99863982e-01\n"," 9.94308293e-01 9.96220052e-01 9.97287154e-01 5.55992010e-05\n"," 7.48281542e-04 1.17103564e-05 1.88409537e-03 2.35079706e-01\n"," 6.26910850e-02 1.23444639e-04 8.88862542e-06 5.22004964e-04\n"," 4.92366171e-03 9.96284842e-01 9.50128853e-01 3.80078709e-04\n"," 1.76874967e-03 1.83697836e-03 9.98474896e-01 9.95366350e-02\n"," 9.97569382e-01 5.07837594e-01 5.54699004e-01 6.77776873e-01\n"," 7.68945694e-01 7.94179618e-01 9.90422010e-01 1.62845201e-13\n"," 5.12994011e-05 4.36195296e-05 6.92653165e-24 2.64775451e-07\n"," 1.56419441e-01 1.57294950e-24 1.04480175e-11 1.11263733e-04\n"," 9.99967337e-01 9.99922752e-01 9.99985814e-01 9.99973774e-01\n"," 9.99602020e-01 9.99514341e-01 9.94324744e-01 9.99992013e-01\n"," 9.99304175e-01 9.89088178e-01 8.64563584e-01 6.92308247e-01\n"," 9.99535799e-01 9.97638464e-01 9.98157680e-01 9.98835981e-01\n"," 9.99683619e-01 9.99310970e-01 9.99914408e-01 9.90833461e-01\n"," 9.99730527e-01 9.99312758e-01 9.99193847e-01 9.99248683e-01\n"," 9.99305010e-01 9.99850869e-01 9.89899218e-01 9.99662280e-01\n"," 5.13456180e-04 9.99271810e-01 9.41726089e-01 9.91244078e-01\n"," 9.89177048e-01 9.99981999e-01 9.99474585e-01 9.99511361e-01\n"," 9.81760442e-01 9.99443829e-01 9.77991223e-01 9.99944091e-01\n"," 9.97615814e-01 9.99635220e-01 9.95899618e-01 9.90252256e-01\n"," 9.89436984e-01 9.97889578e-01 9.78061914e-01 1.78396988e-07\n"," 2.87066944e-17 4.38798433e-12 7.89497440e-16 3.84756774e-02\n"," 2.67142836e-10 9.97544825e-01 9.18313861e-01 9.97218847e-01\n"," 9.99988198e-01 9.99973059e-01]\n","predict [0. 1. 1. 1. 0. 0. 1. 1. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1.\n"," 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 1. 1. 1. 1.\n"," 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1.]\n","Train Epoch: 79 [0/107 (0%)]\tTrain Loss: 0.000118\n","Train Epoch: 79 [4/107 (4%)]\tTrain Loss: 0.000277\n","Train Epoch: 79 [8/107 (7%)]\tTrain Loss: 0.003361\n","Train Epoch: 79 [12/107 (11%)]\tTrain Loss: 0.000616\n","Train Epoch: 79 [16/107 (15%)]\tTrain Loss: 0.054172\n","Train Epoch: 79 [20/107 (19%)]\tTrain Loss: 0.000579\n","Train Epoch: 79 [24/107 (22%)]\tTrain Loss: 0.000320\n","Train Epoch: 79 [28/107 (26%)]\tTrain Loss: 0.005956\n","Train Epoch: 79 [32/107 (30%)]\tTrain Loss: 0.000435\n","Train Epoch: 79 [36/107 (34%)]\tTrain Loss: 0.014079\n","Train Epoch: 79 [40/107 (37%)]\tTrain Loss: 0.000423\n","Train Epoch: 79 [44/107 (41%)]\tTrain Loss: 0.018184\n","Train Epoch: 79 [48/107 (45%)]\tTrain Loss: 0.002952\n","Train Epoch: 79 [52/107 (49%)]\tTrain Loss: 0.000930\n","Train Epoch: 79 [56/107 (52%)]\tTrain Loss: 0.004252\n","Train Epoch: 79 [60/107 (56%)]\tTrain Loss: 0.000216\n","Train Epoch: 79 [64/107 (60%)]\tTrain Loss: 0.003819\n","Train Epoch: 79 [68/107 (64%)]\tTrain Loss: 0.001912\n","Train Epoch: 79 [72/107 (67%)]\tTrain Loss: 0.000298\n","Train Epoch: 79 [76/107 (71%)]\tTrain Loss: 0.007754\n","Train Epoch: 79 [80/107 (75%)]\tTrain Loss: 0.000203\n","Train Epoch: 79 [84/107 (79%)]\tTrain Loss: 0.006694\n","Train Epoch: 79 [88/107 (82%)]\tTrain Loss: 0.001373\n","Train Epoch: 79 [92/107 (86%)]\tTrain Loss: 0.000248\n","Train Epoch: 79 [96/107 (90%)]\tTrain Loss: 0.001317\n","Train Epoch: 79 [100/107 (93%)]\tTrain Loss: 0.273933\n","Train Epoch: 79 [104/107 (97%)]\tTrain Loss: 0.004713\n","target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n","score [4.58528474e-02 7.82556832e-01 8.63203555e-02 9.91141915e-01\n"," 7.57737458e-01 7.68479556e-02 9.63414848e-01 9.96618748e-01\n"," 4.12022546e-02 7.95369685e-01 9.99726593e-01 5.39236844e-01\n"," 9.99537945e-01 7.35998273e-01 6.49441302e-01 2.76938435e-02\n"," 1.31841060e-02 7.51658201e-01 1.97641589e-02 8.60509694e-01\n"," 8.99349689e-01 9.95842874e-01 9.99493837e-01 9.99603331e-01\n"," 9.84229505e-01 9.98662114e-01 9.99521971e-01 6.51310921e-01\n"," 1.19837262e-02 5.38189895e-03 2.92463839e-01 6.69874400e-02\n"," 5.87555878e-02 8.90325848e-03 3.42382267e-02 2.27847487e-01\n"," 6.05912983e-01 9.98680055e-01 9.78327453e-01 4.27085198e-02\n"," 8.65379795e-02 2.69443542e-01 9.91504729e-01 9.97459471e-01\n"," 9.99241590e-01 4.98912632e-01 9.15988922e-01 9.16006684e-01\n"," 4.41360384e-01 4.33293343e-01 9.59488690e-01 3.72135159e-15\n"," 4.98854369e-03 3.76321375e-02 2.08416982e-31 1.56185820e-06\n"," 3.95487070e-01 2.18941582e-29 1.20521676e-10 3.02957036e-02\n"," 9.99970794e-01 9.99962330e-01 9.99850154e-01 9.99646783e-01\n"," 9.99941468e-01 9.99661922e-01 9.94480014e-01 9.99999166e-01\n"," 9.99930978e-01 9.98070180e-01 9.97806609e-01 9.92253840e-01\n"," 9.99707639e-01 9.94533658e-01 9.99094844e-01 9.99556005e-01\n"," 9.95769918e-01 9.95566070e-01 9.99899030e-01 9.93643045e-01\n"," 9.99749720e-01 9.99819934e-01 9.96156037e-01 9.93870020e-01\n"," 9.52995539e-01 9.97461319e-01 9.98845458e-01 9.99589264e-01\n"," 1.27475232e-01 9.99911427e-01 8.10354412e-01 9.93922412e-01\n"," 9.78659451e-01 9.99987364e-01 9.99355853e-01 9.98791516e-01\n"," 9.63769555e-01 9.95188951e-01 9.97390568e-01 9.99581158e-01\n"," 9.99446094e-01 9.99666929e-01 9.79089439e-01 8.85117829e-01\n"," 9.51586485e-01 9.63433206e-01 3.91696304e-01 2.25980813e-07\n"," 3.20238683e-19 1.65544037e-10 1.36658252e-15 2.64243099e-05\n"," 1.80191018e-09 9.92803514e-01 9.37103331e-01 9.39896882e-01\n"," 9.99973655e-01 9.96657133e-01]\n","predict [0. 1. 0. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 0. 1. 0. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 1. 1. 1. 0. 1. 1.\n"," 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1.]\n","Train Epoch: 80 [0/107 (0%)]\tTrain Loss: 0.000857\n","Train Epoch: 80 [4/107 (4%)]\tTrain Loss: 0.001888\n","Train Epoch: 80 [8/107 (7%)]\tTrain Loss: 0.003835\n","Train Epoch: 80 [12/107 (11%)]\tTrain Loss: 0.061138\n","Train Epoch: 80 [16/107 (15%)]\tTrain Loss: 0.001918\n","Train Epoch: 80 [20/107 (19%)]\tTrain Loss: 0.006197\n","Train Epoch: 80 [24/107 (22%)]\tTrain Loss: 0.001262\n","Train Epoch: 80 [28/107 (26%)]\tTrain Loss: 0.001212\n","Train Epoch: 80 [32/107 (30%)]\tTrain Loss: 0.000700\n","Train Epoch: 80 [36/107 (34%)]\tTrain Loss: 0.021254\n","Train Epoch: 80 [40/107 (37%)]\tTrain Loss: 0.008364\n","Train Epoch: 80 [44/107 (41%)]\tTrain Loss: 0.000519\n","Train Epoch: 80 [48/107 (45%)]\tTrain Loss: 0.000189\n","Train Epoch: 80 [52/107 (49%)]\tTrain Loss: 0.000168\n","Train Epoch: 80 [56/107 (52%)]\tTrain Loss: 0.011262\n","Train Epoch: 80 [60/107 (56%)]\tTrain Loss: 0.004607\n","Train Epoch: 80 [64/107 (60%)]\tTrain Loss: 0.105527\n","Train Epoch: 80 [68/107 (64%)]\tTrain Loss: 0.007402\n","Train Epoch: 80 [72/107 (67%)]\tTrain Loss: 0.010296\n","Train Epoch: 80 [76/107 (71%)]\tTrain Loss: 0.006093\n","Train Epoch: 80 [80/107 (75%)]\tTrain Loss: 0.027689\n","Train Epoch: 80 [84/107 (79%)]\tTrain Loss: 0.001174\n","Train Epoch: 80 [88/107 (82%)]\tTrain Loss: 0.001461\n","Train Epoch: 80 [92/107 (86%)]\tTrain Loss: 0.000986\n","Train Epoch: 80 [96/107 (90%)]\tTrain Loss: 0.177026\n","Train Epoch: 80 [100/107 (93%)]\tTrain Loss: 0.002224\n","Train Epoch: 80 [104/107 (97%)]\tTrain Loss: 0.000441\n","target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n","score [4.44653706e-04 9.89809871e-01 3.80415499e-01 1.72763646e-01\n"," 2.04607216e-03 2.81987037e-03 9.99795258e-01 2.74681440e-03\n"," 1.95316934e-05 3.03684086e-01 5.59592605e-01 5.85757978e-02\n"," 5.95121924e-03 3.02029890e-03 5.90611110e-03 8.76296497e-07\n"," 2.88598085e-06 4.60410304e-03 8.91220793e-07 9.33772171e-05\n"," 1.66022128e-05 2.08860099e-01 3.31087172e-01 9.99462903e-01\n"," 7.92743638e-02 1.74956564e-02 9.99936938e-01 3.62453306e-06\n"," 1.09328129e-08 7.13785084e-08 9.26332723e-04 2.54760991e-04\n"," 1.40021117e-02 1.23919938e-06 2.31582999e-05 5.48566786e-05\n"," 8.18275657e-05 6.97206080e-01 8.20663750e-01 2.66705116e-04\n"," 3.67797096e-04 2.82548921e-04 8.30878317e-02 6.79827482e-03\n"," 3.58534208e-03 2.43335441e-02 6.15884483e-01 1.86541945e-01\n"," 7.92117774e-01 5.51979661e-01 9.61813986e-01 4.72362615e-07\n"," 4.82660835e-06 8.42687950e-05 5.24347787e-10 2.41174275e-05\n"," 7.47914985e-02 3.29331310e-12 5.97543249e-07 3.06658035e-06\n"," 9.98351097e-01 9.93910909e-01 9.97775614e-01 9.86288786e-01\n"," 5.16310940e-03 3.23686212e-01 1.73513636e-01 9.99609172e-01\n"," 5.70874989e-01 9.82173026e-01 9.08975720e-01 7.82645762e-01\n"," 9.96931911e-01 9.38659132e-01 6.01868153e-01 9.15464103e-01\n"," 9.99826014e-01 9.96969640e-01 9.03670609e-01 9.91868079e-01\n"," 9.99481142e-01 9.65106785e-01 9.89222050e-01 9.91507173e-01\n"," 3.96979034e-01 9.95623529e-01 9.87471879e-01 9.63367224e-01\n"," 1.49111452e-06 9.31954980e-01 2.64453180e-02 2.54580885e-01\n"," 8.58372226e-02 9.99758542e-01 9.92262602e-01 9.84771788e-01\n"," 3.41445004e-04 4.53074370e-03 2.64076889e-03 2.25876331e-01\n"," 7.72498488e-01 8.70169699e-01 2.51780497e-04 9.44738507e-01\n"," 9.30673540e-01 9.94372845e-01 9.14508164e-01 3.07773380e-03\n"," 1.25712925e-03 1.38158975e-02 1.63360115e-03 6.79067103e-03\n"," 9.77697134e-01 1.16138406e-01 2.80639604e-02 2.16492832e-01\n"," 1.00000000e+00 4.57357056e-02]\n","predict [0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n"," 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0.\n"," 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 0. 0. 0. 1. 1. 1.\n"," 0. 0. 0. 0. 1. 1. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0.]\n","vote_pred [0. 1. 1. 1. 0. 0. 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1.\n"," 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 1. 0. 1. 1.\n"," 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1.]\n","targetlist [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n","TP= 49 TN= 39 FN= 9 FP= 21\n","TP+FP 70\n","precision 0.7\n","recall 0.8448275862068966\n","F1 0.765625\n","acc 0.7457627118644068\n","AUCp 0.7474137931034482\n","AUC 0.8114942528735631\n","\n"," The epoch is 80, average recall: 0.8448, average precision: 0.7000,average F1: 0.7656, average accuracy: 0.7458, average AUC: 0.8115\n","Train Epoch: 81 [0/107 (0%)]\tTrain Loss: 0.000498\n","Train Epoch: 81 [4/107 (4%)]\tTrain Loss: 0.019136\n","Train Epoch: 81 [8/107 (7%)]\tTrain Loss: 0.000259\n","Train Epoch: 81 [12/107 (11%)]\tTrain Loss: 0.000734\n","Train Epoch: 81 [16/107 (15%)]\tTrain Loss: 0.003019\n","Train Epoch: 81 [20/107 (19%)]\tTrain Loss: 0.002839\n","Train Epoch: 81 [24/107 (22%)]\tTrain Loss: 0.005193\n","Train Epoch: 81 [28/107 (26%)]\tTrain Loss: 0.014104\n","Train Epoch: 81 [32/107 (30%)]\tTrain Loss: 0.000485\n","Train Epoch: 81 [36/107 (34%)]\tTrain Loss: 0.007230\n","Train Epoch: 81 [40/107 (37%)]\tTrain Loss: 0.000149\n","Train Epoch: 81 [44/107 (41%)]\tTrain Loss: 0.005055\n","Train Epoch: 81 [48/107 (45%)]\tTrain Loss: 0.004139\n","Train Epoch: 81 [52/107 (49%)]\tTrain Loss: 0.027952\n","Train Epoch: 81 [56/107 (52%)]\tTrain Loss: 0.004256\n","Train Epoch: 81 [60/107 (56%)]\tTrain Loss: 0.143505\n","Train Epoch: 81 [64/107 (60%)]\tTrain Loss: 0.008149\n","Train Epoch: 81 [68/107 (64%)]\tTrain Loss: 0.394566\n","Train Epoch: 81 [72/107 (67%)]\tTrain Loss: 0.000691\n","Train Epoch: 81 [76/107 (71%)]\tTrain Loss: 0.012030\n","Train Epoch: 81 [80/107 (75%)]\tTrain Loss: 0.000285\n","Train Epoch: 81 [84/107 (79%)]\tTrain Loss: 0.000718\n","Train Epoch: 81 [88/107 (82%)]\tTrain Loss: 0.005194\n","Train Epoch: 81 [92/107 (86%)]\tTrain Loss: 0.000794\n","Train Epoch: 81 [96/107 (90%)]\tTrain Loss: 0.004754\n","Train Epoch: 81 [100/107 (93%)]\tTrain Loss: 0.003932\n","Train Epoch: 81 [104/107 (97%)]\tTrain Loss: 0.004073\n","target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n","score [6.93061296e-03 9.03093994e-01 4.60397184e-01 4.49167162e-01\n"," 8.57167318e-03 2.59261373e-02 8.89512181e-01 6.35219961e-02\n"," 3.88334010e-04 1.36522660e-02 1.24856653e-02 7.08972337e-04\n"," 4.17884905e-03 9.22495208e-04 9.13245827e-02 4.20395554e-06\n"," 2.20160337e-06 2.91162659e-03 1.43703110e-05 1.50278956e-03\n"," 2.50432640e-03 3.69358696e-02 6.76521719e-01 9.98253882e-01\n"," 2.72916607e-03 2.07633916e-02 9.98376489e-01 2.07684076e-04\n"," 2.71810023e-07 7.24006384e-07 1.02558185e-03 1.53859239e-03\n"," 8.17637611e-03 9.83818822e-08 3.24562507e-07 1.82196970e-06\n"," 2.14000374e-06 4.35839295e-02 1.66998152e-02 6.13625707e-06\n"," 1.27123303e-05 1.05953717e-04 4.45479266e-02 6.03854947e-04\n"," 1.12621762e-01 2.32959562e-03 5.41897416e-01 1.09162174e-01\n"," 9.84231293e-01 2.84846395e-01 9.90256011e-01 5.02551467e-09\n"," 8.49683638e-05 2.47830205e-04 7.45936039e-12 2.59794160e-05\n"," 1.37981981e-01 3.76829441e-14 4.70155470e-08 8.23818031e-04\n"," 9.99677062e-01 9.99211788e-01 9.99682903e-01 9.92693186e-01\n"," 9.77879822e-01 8.76396537e-01 6.47779644e-01 9.99890447e-01\n"," 9.54217732e-01 9.99181926e-01 9.95264053e-01 9.98733222e-01\n"," 9.98571873e-01 9.85063672e-01 9.94777679e-01 9.94337976e-01\n"," 9.99380231e-01 9.99487996e-01 9.99417782e-01 9.94453728e-01\n"," 9.99094725e-01 9.99412537e-01 9.98117447e-01 5.31367123e-01\n"," 2.59449750e-01 9.99841690e-01 9.95460093e-01 9.99670744e-01\n"," 5.11259597e-04 7.17294356e-03 8.83279443e-01 1.83459356e-01\n"," 3.44662219e-01 9.82025921e-01 8.69598329e-01 8.52327228e-01\n"," 4.82979900e-04 6.88496744e-03 2.11527854e-01 1.72849789e-01\n"," 2.75475889e-01 9.98853445e-01 9.00913263e-04 9.05109942e-02\n"," 5.74900284e-02 5.50068736e-01 8.74584973e-01 1.58044963e-06\n"," 3.60652758e-11 2.91994677e-08 1.42963433e-10 1.89533530e-05\n"," 1.72693437e-09 6.20993227e-03 9.97046661e-03 2.00214330e-02\n"," 9.99789178e-01 9.54836607e-01]\n","predict [0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.\n"," 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.\n"," 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 0. 1. 0. 0. 1. 1. 1.\n"," 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.]\n","Train Epoch: 82 [0/107 (0%)]\tTrain Loss: 0.000617\n","Train Epoch: 82 [4/107 (4%)]\tTrain Loss: 0.003186\n","Train Epoch: 82 [8/107 (7%)]\tTrain Loss: 0.005114\n","Train Epoch: 82 [12/107 (11%)]\tTrain Loss: 0.007278\n","Train Epoch: 82 [16/107 (15%)]\tTrain Loss: 0.000395\n","Train Epoch: 82 [20/107 (19%)]\tTrain Loss: 0.003521\n","Train Epoch: 82 [24/107 (22%)]\tTrain Loss: 0.011651\n","Train Epoch: 82 [28/107 (26%)]\tTrain Loss: 0.007633\n","Train Epoch: 82 [32/107 (30%)]\tTrain Loss: 0.001612\n","Train Epoch: 82 [36/107 (34%)]\tTrain Loss: 0.000538\n","Train Epoch: 82 [40/107 (37%)]\tTrain Loss: 0.016047\n","Train Epoch: 82 [44/107 (41%)]\tTrain Loss: 0.004878\n","Train Epoch: 82 [48/107 (45%)]\tTrain Loss: 0.000119\n","Train Epoch: 82 [52/107 (49%)]\tTrain Loss: 0.000079\n","Train Epoch: 82 [56/107 (52%)]\tTrain Loss: 0.000409\n","Train Epoch: 82 [60/107 (56%)]\tTrain Loss: 0.000137\n","Train Epoch: 82 [64/107 (60%)]\tTrain Loss: 0.000331\n","Train Epoch: 82 [68/107 (64%)]\tTrain Loss: 0.001945\n","Train Epoch: 82 [72/107 (67%)]\tTrain Loss: 0.002857\n","Train Epoch: 82 [76/107 (71%)]\tTrain Loss: 0.000028\n","Train Epoch: 82 [80/107 (75%)]\tTrain Loss: 0.000513\n","Train Epoch: 82 [84/107 (79%)]\tTrain Loss: 0.000268\n","Train Epoch: 82 [88/107 (82%)]\tTrain Loss: 0.005808\n","Train Epoch: 82 [92/107 (86%)]\tTrain Loss: 0.007277\n","Train Epoch: 82 [96/107 (90%)]\tTrain Loss: 0.000486\n","Train Epoch: 82 [100/107 (93%)]\tTrain Loss: 0.020342\n","Train Epoch: 82 [104/107 (97%)]\tTrain Loss: 0.000504\n","target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n","score [4.89254622e-03 6.63440585e-01 2.61422694e-01 9.92501318e-01\n"," 1.26679763e-01 1.52444795e-01 8.06967020e-01 9.92853105e-01\n"," 2.57182796e-03 5.93190670e-01 9.79007721e-01 2.80531459e-02\n"," 2.94649482e-01 1.85217930e-03 1.73765257e-01 2.51561978e-05\n"," 1.46495891e-04 3.98441821e-01 9.03621200e-04 2.25637835e-02\n"," 2.51358431e-02 9.96306062e-01 9.97055292e-01 9.99959707e-01\n"," 9.84050453e-01 9.39585805e-01 9.99987721e-01 3.34553048e-03\n"," 1.06083098e-05 1.79365379e-05 1.04720287e-01 4.51457053e-02\n"," 1.99049134e-02 3.81651319e-07 1.67919939e-06 2.17844463e-05\n"," 9.07337817e-05 9.85279560e-01 9.65644538e-01 1.63420424e-04\n"," 9.66687454e-04 9.16246697e-03 9.88665342e-01 1.31256580e-01\n"," 8.59202683e-01 5.88816255e-02 9.94561136e-01 9.81598020e-01\n"," 9.88949537e-01 9.45949554e-01 9.98207450e-01 3.30684671e-08\n"," 2.30967489e-04 8.44444206e-04 3.77158461e-12 4.72897809e-04\n"," 4.77171987e-01 2.79910783e-15 2.46118816e-07 2.35174084e-03\n"," 9.99986053e-01 9.99922991e-01 9.99990940e-01 9.99869943e-01\n"," 9.80554461e-01 9.79964197e-01 9.89283085e-01 9.99998093e-01\n"," 9.99190509e-01 9.99879837e-01 9.99524593e-01 9.99533176e-01\n"," 9.99490738e-01 9.96577322e-01 9.97620761e-01 9.99784529e-01\n"," 9.99406338e-01 9.99294281e-01 9.99912143e-01 9.99600589e-01\n"," 9.99981523e-01 9.99940634e-01 9.99730647e-01 9.98969078e-01\n"," 9.87871051e-01 9.99994040e-01 9.99877930e-01 9.99973893e-01\n"," 1.04107731e-03 9.79105055e-01 9.98209834e-01 9.89648402e-01\n"," 9.90045488e-01 9.99984980e-01 9.99985337e-01 9.99223948e-01\n"," 2.89087519e-02 9.90894437e-01 6.37849808e-01 9.89378154e-01\n"," 3.33379090e-01 9.99953508e-01 1.60395384e-01 9.92887318e-01\n"," 9.90196824e-01 9.93531764e-01 9.97436702e-01 1.94445427e-04\n"," 5.78866093e-07 5.97508279e-06 8.70004158e-08 7.76709476e-03\n"," 5.07963523e-05 8.87598872e-01 6.61256433e-01 9.88258302e-01\n"," 9.99993920e-01 9.99950409e-01]\n","predict [0. 1. 0. 1. 0. 0. 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1.\n"," 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 1. 0. 1. 1.\n"," 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1.\n"," 0. 1. 1. 1. 0. 1. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1.]\n","Train Epoch: 83 [0/107 (0%)]\tTrain Loss: 0.011613\n","Train Epoch: 83 [4/107 (4%)]\tTrain Loss: 0.000076\n","Train Epoch: 83 [8/107 (7%)]\tTrain Loss: 0.006579\n","Train Epoch: 83 [12/107 (11%)]\tTrain Loss: 0.001542\n","Train Epoch: 83 [16/107 (15%)]\tTrain Loss: 0.000480\n","Train Epoch: 83 [20/107 (19%)]\tTrain Loss: 0.000374\n","Train Epoch: 83 [24/107 (22%)]\tTrain Loss: 0.000912\n","Train Epoch: 83 [28/107 (26%)]\tTrain Loss: 0.003549\n","Train Epoch: 83 [32/107 (30%)]\tTrain Loss: 0.000129\n","Train Epoch: 83 [36/107 (34%)]\tTrain Loss: 0.000284\n","Train Epoch: 83 [40/107 (37%)]\tTrain Loss: 0.000816\n","Train Epoch: 83 [44/107 (41%)]\tTrain Loss: 0.000225\n","Train Epoch: 83 [48/107 (45%)]\tTrain Loss: 0.000048\n","Train Epoch: 83 [52/107 (49%)]\tTrain Loss: 0.000448\n","Train Epoch: 83 [56/107 (52%)]\tTrain Loss: 0.017707\n","Train Epoch: 83 [60/107 (56%)]\tTrain Loss: 0.000167\n","Train Epoch: 83 [64/107 (60%)]\tTrain Loss: 0.002454\n","Train Epoch: 83 [68/107 (64%)]\tTrain Loss: 0.045773\n","Train Epoch: 83 [72/107 (67%)]\tTrain Loss: 0.000312\n","Train Epoch: 83 [76/107 (71%)]\tTrain Loss: 0.000203\n","Train Epoch: 83 [80/107 (75%)]\tTrain Loss: 0.001181\n","Train Epoch: 83 [84/107 (79%)]\tTrain Loss: 0.000525\n","Train Epoch: 83 [88/107 (82%)]\tTrain Loss: 0.000866\n","Train Epoch: 83 [92/107 (86%)]\tTrain Loss: 0.000203\n","Train Epoch: 83 [96/107 (90%)]\tTrain Loss: 0.000553\n","Train Epoch: 83 [100/107 (93%)]\tTrain Loss: 0.000364\n","Train Epoch: 83 [104/107 (97%)]\tTrain Loss: 0.000336\n","target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n","score [1.13310467e-03 4.96775180e-01 8.92742202e-02 9.66182172e-01\n"," 3.69682983e-02 7.05908835e-02 5.96603394e-01 8.54409873e-01\n"," 6.84277038e-04 2.90627480e-01 9.70882773e-01 2.90519744e-02\n"," 3.95355403e-01 1.08418008e-03 1.98020618e-02 1.18491371e-05\n"," 8.98559956e-05 2.23419771e-01 4.04942053e-04 1.58780068e-02\n"," 1.51337236e-02 9.92487431e-01 9.98126924e-01 9.99944687e-01\n"," 9.44774568e-01 9.95990455e-01 9.99949098e-01 2.92525329e-02\n"," 1.73441440e-05 3.99270684e-05 3.21981192e-01 7.12924404e-03\n"," 1.39833510e-01 2.42018154e-06 3.03741035e-05 8.25078678e-06\n"," 4.41645941e-04 9.74672556e-01 9.31916595e-01 3.00449174e-04\n"," 2.93059531e-03 1.59928370e-02 9.50418115e-01 1.06048524e-01\n"," 8.67601216e-01 1.93723738e-02 9.90694106e-01 9.83862996e-01\n"," 9.79555428e-01 9.44027245e-01 9.98777568e-01 1.17407422e-16\n"," 1.31849403e-04 2.76548940e-06 3.09124242e-25 1.10826392e-07\n"," 7.04969373e-03 1.80552743e-26 6.16577969e-13 8.15790426e-03\n"," 9.99941945e-01 9.99522209e-01 9.99955416e-01 9.98620033e-01\n"," 9.88004327e-01 9.92080271e-01 9.93081152e-01 9.99997139e-01\n"," 9.97150123e-01 9.99665022e-01 9.96851623e-01 9.84549642e-01\n"," 9.99255717e-01 9.90662754e-01 9.93579030e-01 9.92614508e-01\n"," 9.99010205e-01 9.98047233e-01 9.99898076e-01 9.89227891e-01\n"," 9.99894857e-01 9.99082923e-01 9.99866843e-01 9.85946596e-01\n"," 9.65382040e-01 9.99325395e-01 9.97843623e-01 9.99511003e-01\n"," 4.89302096e-04 7.85787225e-01 9.97967780e-01 9.87926662e-01\n"," 9.36159074e-01 9.99991417e-01 9.99962568e-01 9.99287307e-01\n"," 2.02758566e-01 7.99595416e-01 2.98662990e-01 9.93052244e-01\n"," 1.24634147e-01 9.99545634e-01 9.09989476e-02 8.32677066e-01\n"," 9.56502020e-01 9.20198739e-01 9.95441198e-01 3.99252881e-10\n"," 9.33705513e-20 1.73249144e-15 8.80195859e-19 1.80121913e-06\n"," 5.29541341e-12 9.49991345e-01 5.89398921e-01 8.75314653e-01\n"," 9.99734700e-01 9.99636292e-01]\n","predict [0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1.\n"," 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 1. 0. 1. 1.\n"," 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1.\n"," 0. 1. 0. 1. 0. 1. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1.]\n","Train Epoch: 84 [0/107 (0%)]\tTrain Loss: 0.000949\n","Train Epoch: 84 [4/107 (4%)]\tTrain Loss: 0.118210\n","Train Epoch: 84 [8/107 (7%)]\tTrain Loss: 0.001028\n","Train Epoch: 84 [12/107 (11%)]\tTrain Loss: 0.013249\n","Train Epoch: 84 [16/107 (15%)]\tTrain Loss: 0.021611\n","Train Epoch: 84 [20/107 (19%)]\tTrain Loss: 0.016357\n","Train Epoch: 84 [24/107 (22%)]\tTrain Loss: 0.002840\n","Train Epoch: 84 [28/107 (26%)]\tTrain Loss: 0.340826\n","Train Epoch: 84 [32/107 (30%)]\tTrain Loss: 0.023557\n","Train Epoch: 84 [36/107 (34%)]\tTrain Loss: 0.018538\n","Train Epoch: 84 [40/107 (37%)]\tTrain Loss: 0.022999\n","Train Epoch: 84 [44/107 (41%)]\tTrain Loss: 0.080624\n","Train Epoch: 84 [48/107 (45%)]\tTrain Loss: 0.001738\n","Train Epoch: 84 [52/107 (49%)]\tTrain Loss: 0.029661\n","Train Epoch: 84 [56/107 (52%)]\tTrain Loss: 0.003213\n","Train Epoch: 84 [60/107 (56%)]\tTrain Loss: 0.005252\n","Train Epoch: 84 [64/107 (60%)]\tTrain Loss: 0.006090\n","Train Epoch: 84 [68/107 (64%)]\tTrain Loss: 0.019731\n","Train Epoch: 84 [72/107 (67%)]\tTrain Loss: 0.004097\n","Train Epoch: 84 [76/107 (71%)]\tTrain Loss: 0.000495\n","Train Epoch: 84 [80/107 (75%)]\tTrain Loss: 0.000129\n","Train Epoch: 84 [84/107 (79%)]\tTrain Loss: 0.065462\n","Train Epoch: 84 [88/107 (82%)]\tTrain Loss: 0.000277\n","Train Epoch: 84 [92/107 (86%)]\tTrain Loss: 0.006186\n","Train Epoch: 84 [96/107 (90%)]\tTrain Loss: 0.028450\n","Train Epoch: 84 [100/107 (93%)]\tTrain Loss: 0.004363\n","Train Epoch: 84 [104/107 (97%)]\tTrain Loss: 0.041804\n","target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n","score [1.25250900e-02 8.23489845e-01 1.44781083e-01 9.80101466e-01\n"," 1.28598616e-01 6.25004154e-03 5.22795320e-01 7.84407556e-01\n"," 4.66144033e-04 4.51769233e-01 6.15011930e-01 9.87553373e-02\n"," 8.37089643e-02 2.74546370e-02 2.01578774e-02 5.24785764e-06\n"," 1.23855155e-02 5.09825051e-01 4.71341700e-05 3.70291397e-02\n"," 1.37856952e-03 9.46368337e-01 9.27270472e-01 9.99890327e-01\n"," 9.53079164e-01 8.96725237e-01 9.99736130e-01 3.57912928e-02\n"," 2.33288956e-05 4.34163521e-05 3.10793202e-02 3.03858280e-04\n"," 2.36317649e-01 2.34008592e-04 1.02022932e-05 1.32851666e-02\n"," 1.20443299e-01 7.67749369e-01 9.11803246e-01 3.62412911e-03\n"," 1.16498470e-02 9.46039781e-02 7.58182168e-01 3.81469429e-02\n"," 8.70758533e-01 2.03350000e-02 8.06143414e-03 6.02725334e-03\n"," 7.26865828e-02 1.01773208e-02 2.97320902e-01 9.14358997e-18\n"," 1.95593202e-05 4.69184488e-06 2.65439248e-29 4.86061080e-10\n"," 1.05445995e-03 1.25222933e-31 3.09398329e-15 5.14513056e-04\n"," 9.89469409e-01 9.97245908e-01 9.99570906e-01 9.95108783e-01\n"," 9.91727948e-01 9.96334076e-01 9.99422789e-01 9.99960303e-01\n"," 9.97317612e-01 9.80348885e-01 4.65547442e-01 1.88045025e-01\n"," 9.97881353e-01 9.99689817e-01 9.98288333e-01 9.53867972e-01\n"," 9.95629430e-01 9.92272377e-01 9.89623845e-01 9.97496665e-01\n"," 9.91996348e-01 8.59727085e-01 9.88531232e-01 9.99354184e-01\n"," 9.77525532e-01 9.99435604e-01 6.93017781e-01 4.49230939e-01\n"," 6.76572993e-02 5.25615811e-01 9.95320380e-01 9.86262023e-01\n"," 9.71385896e-01 9.99990106e-01 9.96055245e-01 9.99759495e-01\n"," 8.59073624e-02 8.10920835e-01 1.71639130e-01 9.78179574e-01\n"," 9.97841477e-01 9.92980838e-01 9.61519897e-01 1.99439917e-02\n"," 1.09772682e-02 1.07833818e-02 9.18460846e-01 3.25616263e-08\n"," 2.60532406e-16 3.40557985e-13 2.02857411e-14 6.99531985e-04\n"," 2.55599347e-10 8.65969181e-01 7.13783979e-01 9.63029623e-01\n"," 9.99996305e-01 9.96464968e-01]\n","predict [0. 1. 0. 1. 0. 0. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1.\n"," 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1.\n"," 0. 1. 0. 1. 1. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1.]\n","Train Epoch: 85 [0/107 (0%)]\tTrain Loss: 0.002843\n","Train Epoch: 85 [4/107 (4%)]\tTrain Loss: 0.000569\n","Train Epoch: 85 [8/107 (7%)]\tTrain Loss: 0.000476\n","Train Epoch: 85 [12/107 (11%)]\tTrain Loss: 0.013366\n","Train Epoch: 85 [16/107 (15%)]\tTrain Loss: 0.008090\n","Train Epoch: 85 [20/107 (19%)]\tTrain Loss: 0.001076\n","Train Epoch: 85 [24/107 (22%)]\tTrain Loss: 0.008563\n","Train Epoch: 85 [28/107 (26%)]\tTrain Loss: 0.030557\n","Train Epoch: 85 [32/107 (30%)]\tTrain Loss: 0.042350\n","Train Epoch: 85 [36/107 (34%)]\tTrain Loss: 0.009184\n","Train Epoch: 85 [40/107 (37%)]\tTrain Loss: 0.002307\n","Train Epoch: 85 [44/107 (41%)]\tTrain Loss: 0.000645\n","Train Epoch: 85 [48/107 (45%)]\tTrain Loss: 0.000474\n","Train Epoch: 85 [52/107 (49%)]\tTrain Loss: 0.035298\n","Train Epoch: 85 [56/107 (52%)]\tTrain Loss: 0.022805\n","Train Epoch: 85 [60/107 (56%)]\tTrain Loss: 0.002225\n","Train Epoch: 85 [64/107 (60%)]\tTrain Loss: 0.002832\n","Train Epoch: 85 [68/107 (64%)]\tTrain Loss: 0.000357\n","Train Epoch: 85 [72/107 (67%)]\tTrain Loss: 0.031120\n","Train Epoch: 85 [76/107 (71%)]\tTrain Loss: 0.000296\n","Train Epoch: 85 [80/107 (75%)]\tTrain Loss: 0.024999\n","Train Epoch: 85 [84/107 (79%)]\tTrain Loss: 0.000637\n","Train Epoch: 85 [88/107 (82%)]\tTrain Loss: 0.000417\n","Train Epoch: 85 [92/107 (86%)]\tTrain Loss: 0.000227\n","Train Epoch: 85 [96/107 (90%)]\tTrain Loss: 0.001056\n","Train Epoch: 85 [100/107 (93%)]\tTrain Loss: 0.002822\n","Train Epoch: 85 [104/107 (97%)]\tTrain Loss: 0.000721\n","target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n","score [3.10826719e-01 6.21339142e-01 2.18862608e-01 8.89767408e-01\n"," 2.19738092e-02 4.80288360e-03 7.32144058e-01 6.31328404e-01\n"," 5.33064827e-03 5.55476129e-01 6.46795094e-01 1.21800132e-01\n"," 3.42394382e-01 1.17120683e-01 1.81563071e-03 5.22109513e-05\n"," 1.12492358e-02 6.05187342e-02 4.15207887e-06 1.21226991e-02\n"," 1.61534324e-02 5.19443214e-01 9.93824601e-01 9.99075890e-01\n"," 2.52956271e-01 7.51924992e-01 9.99357522e-01 4.06309254e-02\n"," 1.45048773e-06 1.22921608e-06 5.89025281e-02 8.67280702e-04\n"," 4.65053692e-02 2.06629265e-05 2.53616872e-06 3.67024877e-05\n"," 2.42774568e-05 9.52805459e-01 9.61141348e-01 7.90483726e-04\n"," 1.25983171e-03 5.55806840e-03 9.13332760e-01 1.10519421e-03\n"," 8.96732807e-01 7.14529634e-01 5.78237232e-03 3.45294992e-03\n"," 1.43159956e-01 3.71887274e-02 1.62237898e-01 4.85942037e-19\n"," 4.08073220e-08 4.61508769e-08 3.38884948e-28 1.30833039e-10\n"," 1.48771614e-05 1.14989575e-30 2.63734025e-14 2.49401695e-04\n"," 9.99579608e-01 9.99626160e-01 9.99837279e-01 9.97317374e-01\n"," 9.99588430e-01 9.99070108e-01 9.97763515e-01 9.99988198e-01\n"," 9.99254763e-01 9.58114624e-01 9.69002843e-01 9.65852857e-01\n"," 9.89722610e-01 9.99406576e-01 9.96134281e-01 9.82932568e-01\n"," 9.97059107e-01 9.92305577e-01 9.98653412e-01 9.97001112e-01\n"," 9.97736931e-01 9.94009495e-01 9.90773857e-01 9.98140812e-01\n"," 9.67858672e-01 9.97297943e-01 5.49880445e-01 5.26659369e-01\n"," 4.88210935e-03 9.88086462e-01 9.94202673e-01 9.40667212e-01\n"," 9.27790642e-01 9.99922514e-01 9.96057868e-01 9.98391807e-01\n"," 9.56781626e-01 7.31408536e-01 9.64025199e-01 9.98965025e-01\n"," 9.93630826e-01 9.96818542e-01 2.84538776e-01 2.47163065e-02\n"," 1.23341847e-03 1.25697358e-02 3.46588045e-01 3.87791630e-07\n"," 1.91219662e-11 3.86622956e-11 7.96985308e-12 7.66223809e-03\n"," 3.73339094e-06 9.86462295e-01 8.98326218e-01 9.19378877e-01\n"," 9.99997854e-01 9.99866366e-01]\n","predict [0. 1. 0. 1. 0. 0. 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1.\n"," 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 1. 1. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1.]\n","Train Epoch: 86 [0/107 (0%)]\tTrain Loss: 0.000442\n","Train Epoch: 86 [4/107 (4%)]\tTrain Loss: 0.000756\n","Train Epoch: 86 [8/107 (7%)]\tTrain Loss: 0.006379\n","Train Epoch: 86 [12/107 (11%)]\tTrain Loss: 0.000504\n","Train Epoch: 86 [16/107 (15%)]\tTrain Loss: 0.021371\n","Train Epoch: 86 [20/107 (19%)]\tTrain Loss: 0.000226\n","Train Epoch: 86 [24/107 (22%)]\tTrain Loss: 0.011846\n","Train Epoch: 86 [28/107 (26%)]\tTrain Loss: 0.002658\n","Train Epoch: 86 [32/107 (30%)]\tTrain Loss: 0.001809\n","Train Epoch: 86 [36/107 (34%)]\tTrain Loss: 0.086606\n","Train Epoch: 86 [40/107 (37%)]\tTrain Loss: 0.003223\n","Train Epoch: 86 [44/107 (41%)]\tTrain Loss: 0.004892\n","Train Epoch: 86 [48/107 (45%)]\tTrain Loss: 0.001064\n","Train Epoch: 86 [52/107 (49%)]\tTrain Loss: 0.000589\n","Train Epoch: 86 [56/107 (52%)]\tTrain Loss: 0.000210\n","Train Epoch: 86 [60/107 (56%)]\tTrain Loss: 0.000058\n","Train Epoch: 86 [64/107 (60%)]\tTrain Loss: 0.000523\n","Train Epoch: 86 [68/107 (64%)]\tTrain Loss: 0.000623\n","Train Epoch: 86 [72/107 (67%)]\tTrain Loss: 0.000771\n","Train Epoch: 86 [76/107 (71%)]\tTrain Loss: 0.002759\n","Train Epoch: 86 [80/107 (75%)]\tTrain Loss: 0.000049\n","Train Epoch: 86 [84/107 (79%)]\tTrain Loss: 0.000170\n","Train Epoch: 86 [88/107 (82%)]\tTrain Loss: 0.000303\n","Train Epoch: 86 [92/107 (86%)]\tTrain Loss: 0.000142\n","Train Epoch: 86 [96/107 (90%)]\tTrain Loss: 0.000303\n","Train Epoch: 86 [100/107 (93%)]\tTrain Loss: 0.052389\n","Train Epoch: 86 [104/107 (97%)]\tTrain Loss: 0.000194\n","target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n","score [1.93378842e-03 9.92780864e-01 7.45798707e-01 9.49406862e-01\n"," 6.10392950e-02 3.58509901e-03 8.07789505e-01 6.58478320e-01\n"," 3.37086203e-05 6.62977815e-01 5.25196552e-01 1.03002310e-01\n"," 3.17196511e-02 5.98431751e-02 3.93917300e-02 5.32030608e-05\n"," 4.83513512e-02 9.73385632e-01 1.38146803e-04 3.63029763e-02\n"," 3.84943141e-03 9.53538060e-01 9.95435297e-01 9.96875763e-01\n"," 5.12453556e-01 9.40065503e-01 9.99361694e-01 2.52692431e-01\n"," 1.34113885e-04 7.47018028e-04 4.71154422e-01 7.90011585e-02\n"," 2.55865008e-01 7.05858611e-06 9.30267561e-07 3.75083691e-05\n"," 5.26152835e-05 9.95540798e-01 9.72778678e-01 3.75324656e-04\n"," 8.17333930e-04 7.24994170e-04 9.83098447e-01 5.13668545e-02\n"," 9.93041515e-01 6.46180212e-01 8.48397911e-01 7.58044660e-01\n"," 8.45764637e-01 7.47125804e-01 9.74904060e-01 1.66381900e-18\n"," 7.59103287e-08 9.72143752e-08 6.38724122e-28 1.79047679e-10\n"," 5.96735736e-05 5.41748624e-31 7.25869882e-15 6.86498405e-03\n"," 9.99932766e-01 9.99950051e-01 9.99996305e-01 9.99930143e-01\n"," 9.99102592e-01 9.97700393e-01 9.93379831e-01 9.99998331e-01\n"," 9.99805987e-01 9.99119699e-01 9.24174070e-01 9.58592355e-01\n"," 9.98005331e-01 9.99661803e-01 9.98573184e-01 9.97042000e-01\n"," 9.99945164e-01 9.99897957e-01 9.99587119e-01 9.99699950e-01\n"," 9.99825537e-01 9.99300122e-01 9.98864889e-01 9.99823511e-01\n"," 9.85048592e-01 9.99784887e-01 9.90633726e-01 9.91230845e-01\n"," 1.97423273e-03 7.25819945e-01 9.55975652e-01 9.27784979e-01\n"," 7.69068480e-01 9.99444664e-01 9.88361180e-01 9.82033849e-01\n"," 3.37271452e-01 8.21411610e-01 9.80876148e-01 9.82994556e-01\n"," 8.90195608e-01 9.96411860e-01 2.52753407e-01 2.56037354e-01\n"," 1.89656898e-01 6.16087198e-01 9.87280071e-01 1.26227038e-19\n"," 2.34778490e-25 2.24773154e-22 3.46594382e-22 5.05825483e-06\n"," 3.24811269e-22 9.78973448e-01 7.42125869e-01 9.66369390e-01\n"," 1.00000000e+00 9.99993443e-01]\n","predict [0. 1. 1. 1. 0. 0. 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1.\n"," 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 1. 1. 1. 1.\n"," 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1.\n"," 0. 1. 1. 1. 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1.]\n","Train Epoch: 87 [0/107 (0%)]\tTrain Loss: 0.001061\n","Train Epoch: 87 [4/107 (4%)]\tTrain Loss: 0.008019\n","Train Epoch: 87 [8/107 (7%)]\tTrain Loss: 0.001065\n","Train Epoch: 87 [12/107 (11%)]\tTrain Loss: 0.001913\n","Train Epoch: 87 [16/107 (15%)]\tTrain Loss: 0.000890\n","Train Epoch: 87 [20/107 (19%)]\tTrain Loss: 0.002804\n","Train Epoch: 87 [24/107 (22%)]\tTrain Loss: 0.007314\n","Train Epoch: 87 [28/107 (26%)]\tTrain Loss: 0.010612\n","Train Epoch: 87 [32/107 (30%)]\tTrain Loss: 0.005134\n","Train Epoch: 87 [36/107 (34%)]\tTrain Loss: 0.019750\n","Train Epoch: 87 [40/107 (37%)]\tTrain Loss: 0.016428\n","Train Epoch: 87 [44/107 (41%)]\tTrain Loss: 0.027580\n","Train Epoch: 87 [48/107 (45%)]\tTrain Loss: 0.001069\n","Train Epoch: 87 [52/107 (49%)]\tTrain Loss: 0.001414\n","Train Epoch: 87 [56/107 (52%)]\tTrain Loss: 0.008766\n","Train Epoch: 87 [60/107 (56%)]\tTrain Loss: 0.000168\n","Train Epoch: 87 [64/107 (60%)]\tTrain Loss: 0.003494\n","Train Epoch: 87 [68/107 (64%)]\tTrain Loss: 0.004904\n","Train Epoch: 87 [72/107 (67%)]\tTrain Loss: 0.006997\n","Train Epoch: 87 [76/107 (71%)]\tTrain Loss: 0.000525\n","Train Epoch: 87 [80/107 (75%)]\tTrain Loss: 0.000274\n","Train Epoch: 87 [84/107 (79%)]\tTrain Loss: 0.003492\n","Train Epoch: 87 [88/107 (82%)]\tTrain Loss: 0.000448\n","Train Epoch: 87 [92/107 (86%)]\tTrain Loss: 0.002912\n","Train Epoch: 87 [96/107 (90%)]\tTrain Loss: 0.001686\n","Train Epoch: 87 [100/107 (93%)]\tTrain Loss: 0.000382\n","Train Epoch: 87 [104/107 (97%)]\tTrain Loss: 0.002832\n","target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n","score [6.50553405e-02 9.94463503e-01 8.28855693e-01 8.61241817e-01\n"," 6.39245212e-02 1.59019101e-02 9.07320976e-01 5.96784092e-02\n"," 5.73870086e-04 6.90305054e-01 4.45091456e-01 2.50565857e-01\n"," 5.52705467e-01 2.47414894e-02 4.20738151e-03 1.76824644e-04\n"," 3.78710357e-03 6.30864352e-02 2.55411778e-05 2.37642825e-01\n"," 1.37956729e-02 9.89162803e-01 9.93365586e-01 9.99759376e-01\n"," 9.60041344e-01 9.94464934e-01 9.43510354e-01 1.71292934e-03\n"," 9.12358064e-06 3.71999631e-05 7.84920994e-03 5.50804241e-03\n"," 9.49906558e-02 2.69903694e-06 1.45565164e-05 4.48387000e-05\n"," 1.20483350e-03 3.05847377e-01 8.12215984e-01 4.66053636e-04\n"," 1.74286473e-03 8.88945349e-03 2.15046108e-01 9.81768072e-02\n"," 9.61183488e-01 2.98944473e-01 7.82125741e-02 1.07552014e-01\n"," 7.38767862e-01 4.87438738e-02 9.82854724e-01 3.91197278e-27\n"," 5.71505259e-14 1.59156152e-12 1.66821780e-40 4.56098655e-17\n"," 2.22662444e-09 0.00000000e+00 8.48540252e-23 3.96662159e-03\n"," 9.99558866e-01 9.99078751e-01 9.99910474e-01 9.99169230e-01\n"," 9.98178244e-01 9.93779361e-01 9.87282872e-01 9.99992728e-01\n"," 9.99619842e-01 9.94059324e-01 9.07517314e-01 4.61445063e-01\n"," 9.91865337e-01 9.94093239e-01 9.96397734e-01 9.83064353e-01\n"," 9.99741733e-01 9.99570191e-01 9.99304056e-01 9.98305798e-01\n"," 9.99518871e-01 9.98050332e-01 9.98346806e-01 9.95386541e-01\n"," 9.31271851e-01 9.98886049e-01 9.78320718e-01 8.54053140e-01\n"," 6.70592394e-03 9.90601778e-01 9.78698850e-01 9.97531533e-01\n"," 9.32836533e-01 9.99991417e-01 9.98672128e-01 9.99820292e-01\n"," 7.87519395e-01 1.10020421e-01 8.34933698e-01 9.92735386e-01\n"," 6.64804280e-01 9.67882454e-01 7.39644349e-01 4.91110496e-02\n"," 2.60443147e-03 1.37326315e-01 9.73627806e-01 3.33611857e-21\n"," 5.10772327e-35 1.07337199e-29 1.55115466e-29 2.21377879e-04\n"," 1.27641764e-21 9.94815290e-01 9.00686026e-01 6.34504080e-01\n"," 9.99994755e-01 9.99919534e-01]\n","predict [0. 1. 1. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1.\n"," 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n"," 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 0. 1. 1. 1. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1.]\n","Train Epoch: 88 [0/107 (0%)]\tTrain Loss: 0.011965\n","Train Epoch: 88 [4/107 (4%)]\tTrain Loss: 0.000114\n","Train Epoch: 88 [8/107 (7%)]\tTrain Loss: 0.000751\n","Train Epoch: 88 [12/107 (11%)]\tTrain Loss: 0.000371\n","Train Epoch: 88 [16/107 (15%)]\tTrain Loss: 0.000298\n","Train Epoch: 88 [20/107 (19%)]\tTrain Loss: 0.000122\n","Train Epoch: 88 [24/107 (22%)]\tTrain Loss: 0.009131\n","Train Epoch: 88 [28/107 (26%)]\tTrain Loss: 0.004178\n","Train Epoch: 88 [32/107 (30%)]\tTrain Loss: 0.000868\n","Train Epoch: 88 [36/107 (34%)]\tTrain Loss: 0.001161\n","Train Epoch: 88 [40/107 (37%)]\tTrain Loss: 0.035903\n","Train Epoch: 88 [44/107 (41%)]\tTrain Loss: 0.000270\n","Train Epoch: 88 [48/107 (45%)]\tTrain Loss: 0.014891\n","Train Epoch: 88 [52/107 (49%)]\tTrain Loss: 0.006505\n","Train Epoch: 88 [56/107 (52%)]\tTrain Loss: 0.005328\n","Train Epoch: 88 [60/107 (56%)]\tTrain Loss: 0.194723\n","Train Epoch: 88 [64/107 (60%)]\tTrain Loss: 0.006282\n","Train Epoch: 88 [68/107 (64%)]\tTrain Loss: 0.027922\n","Train Epoch: 88 [72/107 (67%)]\tTrain Loss: 0.000180\n","Train Epoch: 88 [76/107 (71%)]\tTrain Loss: 0.001196\n","Train Epoch: 88 [80/107 (75%)]\tTrain Loss: 0.009112\n","Train Epoch: 88 [84/107 (79%)]\tTrain Loss: 0.000622\n","Train Epoch: 88 [88/107 (82%)]\tTrain Loss: 0.056998\n","Train Epoch: 88 [92/107 (86%)]\tTrain Loss: 0.000144\n","Train Epoch: 88 [96/107 (90%)]\tTrain Loss: 0.002578\n","Train Epoch: 88 [100/107 (93%)]\tTrain Loss: 0.002606\n","Train Epoch: 88 [104/107 (97%)]\tTrain Loss: 0.000631\n","target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n","score [3.11838686e-02 9.95050132e-01 3.32236767e-01 8.12586129e-01\n"," 6.20362498e-02 9.60179325e-03 9.21404541e-01 4.57598493e-02\n"," 4.48968640e-05 3.79987329e-01 8.66882682e-01 5.93570247e-02\n"," 6.50595427e-01 1.13847807e-01 6.41250685e-02 7.75559805e-04\n"," 6.61462871e-03 9.63951409e-01 1.38605057e-04 3.92666072e-01\n"," 8.92983191e-03 9.99652743e-01 9.99669790e-01 9.99861360e-01\n"," 9.97510791e-01 9.98791754e-01 9.99892831e-01 9.80958641e-01\n"," 3.06120813e-02 6.03426294e-03 9.68202055e-01 6.59079134e-01\n"," 4.86601532e-01 1.10421497e-05 6.25739412e-05 1.66398345e-03\n"," 1.13075564e-03 9.92845714e-01 9.96272445e-01 5.88300414e-02\n"," 6.61291480e-02 4.39726472e-01 9.74915206e-01 9.98409867e-01\n"," 9.99676943e-01 8.79817426e-01 9.41864431e-01 9.16424513e-01\n"," 8.77897680e-01 3.15259278e-01 9.82783794e-01 8.17851897e-11\n"," 3.65313499e-05 2.12784465e-02 1.33062378e-16 1.49755346e-04\n"," 3.58704515e-02 3.39445774e-17 2.77564531e-08 8.85730982e-03\n"," 9.99996424e-01 9.99996066e-01 9.99999642e-01 9.99998331e-01\n"," 9.98645127e-01 9.98281121e-01 9.85043049e-01 9.99998689e-01\n"," 9.97154117e-01 9.98835862e-01 9.97880816e-01 9.52477455e-01\n"," 9.98838365e-01 9.99208272e-01 9.96914506e-01 9.99966979e-01\n"," 9.99963164e-01 9.99974608e-01 9.99973297e-01 9.99528766e-01\n"," 9.99885321e-01 9.98343468e-01 9.99460757e-01 9.99956846e-01\n"," 9.99791205e-01 9.99956131e-01 9.99916434e-01 9.99967098e-01\n"," 1.36824825e-03 9.53423381e-01 9.12663817e-01 9.97130573e-01\n"," 9.62959647e-01 9.99632478e-01 8.91150832e-01 9.85337079e-01\n"," 5.00124276e-01 8.39909613e-01 9.94447947e-01 9.97772515e-01\n"," 9.94034946e-01 9.99260962e-01 7.79367447e-01 2.81256568e-02\n"," 1.02641180e-01 8.68068412e-02 9.92034733e-01 4.40085642e-02\n"," 4.29447666e-02 1.02816366e-05 5.22489194e-04 1.85342044e-01\n"," 9.77863729e-01 9.98676956e-01 8.53541374e-01 9.93555963e-01\n"," 1.00000000e+00 9.99999166e-01]\n","predict [0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1.\n"," 1. 1. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1.\n"," 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1.]\n","Train Epoch: 89 [0/107 (0%)]\tTrain Loss: 0.000520\n","Train Epoch: 89 [4/107 (4%)]\tTrain Loss: 0.000283\n","Train Epoch: 89 [8/107 (7%)]\tTrain Loss: 0.000433\n","Train Epoch: 89 [12/107 (11%)]\tTrain Loss: 0.000320\n","Train Epoch: 89 [16/107 (15%)]\tTrain Loss: 0.008159\n","Train Epoch: 89 [20/107 (19%)]\tTrain Loss: 0.002978\n","Train Epoch: 89 [24/107 (22%)]\tTrain Loss: 0.000331\n","Train Epoch: 89 [28/107 (26%)]\tTrain Loss: 0.003095\n","Train Epoch: 89 [32/107 (30%)]\tTrain Loss: 0.000027\n","Train Epoch: 89 [36/107 (34%)]\tTrain Loss: 0.000094\n","Train Epoch: 89 [40/107 (37%)]\tTrain Loss: 0.000228\n","Train Epoch: 89 [44/107 (41%)]\tTrain Loss: 0.001420\n","Train Epoch: 89 [48/107 (45%)]\tTrain Loss: 0.003898\n","Train Epoch: 89 [52/107 (49%)]\tTrain Loss: 0.000995\n","Train Epoch: 89 [56/107 (52%)]\tTrain Loss: 0.002364\n","Train Epoch: 89 [60/107 (56%)]\tTrain Loss: 0.000083\n","Train Epoch: 89 [64/107 (60%)]\tTrain Loss: 0.000039\n","Train Epoch: 89 [68/107 (64%)]\tTrain Loss: 0.000009\n","Train Epoch: 89 [72/107 (67%)]\tTrain Loss: 0.000088\n","Train Epoch: 89 [76/107 (71%)]\tTrain Loss: 0.000271\n","Train Epoch: 89 [80/107 (75%)]\tTrain Loss: 0.000150\n","Train Epoch: 89 [84/107 (79%)]\tTrain Loss: 0.001057\n","Train Epoch: 89 [88/107 (82%)]\tTrain Loss: 0.000345\n","Train Epoch: 89 [92/107 (86%)]\tTrain Loss: 0.000312\n","Train Epoch: 89 [96/107 (90%)]\tTrain Loss: 0.002772\n","Train Epoch: 89 [100/107 (93%)]\tTrain Loss: 0.000472\n","Train Epoch: 89 [104/107 (97%)]\tTrain Loss: 0.000927\n","target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n","score [2.16625747e-03 9.65517938e-01 1.57500401e-01 1.58294827e-01\n"," 1.95204327e-03 9.59662488e-04 8.83186340e-01 2.50123034e-04\n"," 1.20451814e-05 4.13158447e-01 2.18093619e-01 2.95857806e-02\n"," 3.33435923e-01 2.44205985e-02 5.22010378e-04 3.92134134e-05\n"," 3.11758812e-03 1.15757108e-01 1.56375954e-05 7.02870265e-03\n"," 4.45200549e-03 9.98583078e-01 9.99118149e-01 9.99953270e-01\n"," 9.87922728e-01 9.93342996e-01 9.99882102e-01 1.19144944e-02\n"," 7.04727427e-06 2.09185418e-06 3.68308164e-02 2.05587503e-03\n"," 2.64089485e-03 3.75699938e-06 3.79327093e-06 2.64819653e-04\n"," 3.01750115e-04 7.45111585e-01 8.80882800e-01 2.46086973e-04\n"," 5.60261658e-04 8.98506492e-03 1.76676378e-01 2.68431127e-01\n"," 9.01459396e-01 1.27536058e-01 2.14795947e-01 1.39839441e-01\n"," 5.56853294e-01 9.20114443e-02 9.67811286e-01 8.14607312e-19\n"," 5.31841904e-09 1.33946344e-06 5.05147110e-28 3.41262263e-09\n"," 4.72923821e-05 5.86388950e-32 2.65622241e-13 4.83079857e-05\n"," 9.99968529e-01 9.99971390e-01 9.99995947e-01 9.99951959e-01\n"," 9.42189813e-01 8.78539681e-01 7.23564684e-01 9.99989390e-01\n"," 9.88853216e-01 9.60959136e-01 9.97776091e-01 8.13519180e-01\n"," 8.69867921e-01 9.79046106e-01 7.09344327e-01 9.96833742e-01\n"," 9.99673963e-01 9.99460042e-01 9.99694824e-01 9.97612357e-01\n"," 9.99878883e-01 9.97631669e-01 9.99775589e-01 9.99852419e-01\n"," 9.99145627e-01 9.98650610e-01 9.98784602e-01 9.98805642e-01\n"," 1.11343041e-04 9.93835866e-01 9.77548540e-01 9.94481325e-01\n"," 5.68208277e-01 9.99929190e-01 9.96060431e-01 9.99527454e-01\n"," 2.98004568e-01 2.19680324e-01 9.82405782e-01 9.99490499e-01\n"," 9.98328507e-01 9.62242544e-01 9.67510566e-02 1.81757919e-02\n"," 7.38855544e-03 1.79737389e-01 9.80171204e-01 3.82004821e-23\n"," 1.52995913e-27 1.98496289e-30 9.70919402e-26 1.34059650e-04\n"," 1.62767536e-21 9.99481857e-01 4.00735378e-01 9.58578110e-01\n"," 1.00000000e+00 9.99996662e-01]\n","predict [0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1.\n"," 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n"," 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1.\n"," 0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1.]\n","Train Epoch: 90 [0/107 (0%)]\tTrain Loss: 0.003072\n","Train Epoch: 90 [4/107 (4%)]\tTrain Loss: 0.001171\n","Train Epoch: 90 [8/107 (7%)]\tTrain Loss: 0.000171\n","Train Epoch: 90 [12/107 (11%)]\tTrain Loss: 0.000622\n","Train Epoch: 90 [16/107 (15%)]\tTrain Loss: 0.001086\n","Train Epoch: 90 [20/107 (19%)]\tTrain Loss: 0.014579\n","Train Epoch: 90 [24/107 (22%)]\tTrain Loss: 0.000054\n","Train Epoch: 90 [28/107 (26%)]\tTrain Loss: 0.000040\n","Train Epoch: 90 [32/107 (30%)]\tTrain Loss: 0.000479\n","Train Epoch: 90 [36/107 (34%)]\tTrain Loss: 0.000169\n","Train Epoch: 90 [40/107 (37%)]\tTrain Loss: 0.000256\n","Train Epoch: 90 [44/107 (41%)]\tTrain Loss: 0.025965\n","Train Epoch: 90 [48/107 (45%)]\tTrain Loss: 0.001162\n","Train Epoch: 90 [52/107 (49%)]\tTrain Loss: 0.002365\n","Train Epoch: 90 [56/107 (52%)]\tTrain Loss: 0.001096\n","Train Epoch: 90 [60/107 (56%)]\tTrain Loss: 0.000043\n","Train Epoch: 90 [64/107 (60%)]\tTrain Loss: 0.002373\n","Train Epoch: 90 [68/107 (64%)]\tTrain Loss: 0.000087\n","Train Epoch: 90 [72/107 (67%)]\tTrain Loss: 0.000214\n","Train Epoch: 90 [76/107 (71%)]\tTrain Loss: 0.009609\n","Train Epoch: 90 [80/107 (75%)]\tTrain Loss: 0.000857\n","Train Epoch: 90 [84/107 (79%)]\tTrain Loss: 0.008890\n","Train Epoch: 90 [88/107 (82%)]\tTrain Loss: 0.002288\n","Train Epoch: 90 [92/107 (86%)]\tTrain Loss: 0.000841\n","Train Epoch: 90 [96/107 (90%)]\tTrain Loss: 0.001281\n","Train Epoch: 90 [100/107 (93%)]\tTrain Loss: 0.009676\n","Train Epoch: 90 [104/107 (97%)]\tTrain Loss: 0.000590\n","target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n","score [8.22358951e-03 9.83392894e-01 8.71788979e-01 6.53461397e-01\n"," 1.21257883e-02 1.43576013e-02 9.69640791e-01 5.79789042e-01\n"," 5.27470656e-05 3.21635038e-01 9.97720063e-01 1.74209818e-01\n"," 9.04948175e-01 8.99646282e-01 6.57029033e-01 2.42963037e-03\n"," 6.30504871e-03 6.98751152e-01 1.62914675e-02 4.83985335e-01\n"," 1.10427119e-01 9.98713136e-01 9.99799311e-01 9.99977469e-01\n"," 9.89886522e-01 9.93357837e-01 9.99938369e-01 6.79971278e-02\n"," 6.07854628e-04 2.66503484e-04 7.39314631e-02 1.41202405e-01\n"," 3.43210489e-01 1.48267327e-05 2.81230605e-05 1.69866369e-03\n"," 5.68928430e-03 9.88769293e-01 9.84305382e-01 8.88195485e-02\n"," 3.80966999e-02 3.42345089e-01 4.52206761e-01 9.89463151e-01\n"," 9.99645591e-01 7.02727079e-01 9.93219674e-01 9.49998975e-01\n"," 9.87359345e-01 9.66794729e-01 9.99286950e-01 1.15013702e-15\n"," 9.03530628e-04 6.12527085e-03 1.61895399e-24 5.46756564e-05\n"," 7.20667839e-02 1.04681255e-22 6.00712327e-11 2.38083541e-01\n"," 9.99993801e-01 9.99993682e-01 9.99999762e-01 9.99990702e-01\n"," 9.99634743e-01 9.99857306e-01 9.97009218e-01 9.99999881e-01\n"," 9.99941111e-01 9.60530758e-01 9.99347508e-01 9.96097922e-01\n"," 9.98284161e-01 9.93737578e-01 9.93800461e-01 9.99927402e-01\n"," 9.99982595e-01 9.99973536e-01 9.99995947e-01 9.98478711e-01\n"," 9.99961853e-01 9.99915242e-01 9.99985337e-01 9.99927282e-01\n"," 9.99234200e-01 9.99496698e-01 9.99989390e-01 9.99840498e-01\n"," 3.21526714e-02 9.53329444e-01 8.61021519e-01 9.97492194e-01\n"," 9.59567070e-01 9.99974728e-01 9.99660254e-01 9.98958468e-01\n"," 8.31995666e-01 5.87798297e-01 9.72365201e-01 9.90266860e-01\n"," 9.79667902e-01 9.99215126e-01 7.13738322e-01 2.13699397e-02\n"," 1.58800870e-01 6.84585273e-01 9.40913796e-01 1.48107279e-02\n"," 2.84985901e-04 7.97408647e-05 8.26673873e-04 9.95335355e-02\n"," 9.13117707e-01 9.95740771e-01 4.49409842e-01 9.87705112e-01\n"," 9.99999046e-01 9.99871969e-01]\n","predict [0. 1. 1. 1. 0. 0. 1. 1. 0. 0. 1. 0. 1. 1. 1. 0. 0. 1. 0. 0. 0. 1. 1. 1.\n"," 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1. 0. 1. 1. 1.]\n","vote_pred [0. 1. 0. 1. 0. 0. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1.\n"," 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 1. 0. 1. 0.\n"," 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1.\n"," 0. 1. 1. 1. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1.]\n","targetlist [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n","TP= 46 TN= 42 FN= 12 FP= 18\n","TP+FP 64\n","precision 0.71875\n","recall 0.7931034482758621\n","F1 0.7540983606557378\n","acc 0.7457627118644068\n","AUCp 0.7465517241379309\n","AUC 0.8307471264367817\n","\n"," The epoch is 90, average recall: 0.7931, average precision: 0.7188,average F1: 0.7541, average accuracy: 0.7458, average AUC: 0.8307\n","Train Epoch: 91 [0/107 (0%)]\tTrain Loss: 0.001459\n","Train Epoch: 91 [4/107 (4%)]\tTrain Loss: 0.000117\n","Train Epoch: 91 [8/107 (7%)]\tTrain Loss: 0.000058\n","Train Epoch: 91 [12/107 (11%)]\tTrain Loss: 0.001888\n","Train Epoch: 91 [16/107 (15%)]\tTrain Loss: 0.013068\n","Train Epoch: 91 [20/107 (19%)]\tTrain Loss: 0.004038\n","Train Epoch: 91 [24/107 (22%)]\tTrain Loss: 0.001006\n","Train Epoch: 91 [28/107 (26%)]\tTrain Loss: 0.022165\n","Train Epoch: 91 [32/107 (30%)]\tTrain Loss: 0.001325\n","Train Epoch: 91 [36/107 (34%)]\tTrain Loss: 0.000133\n","Train Epoch: 91 [40/107 (37%)]\tTrain Loss: 0.000505\n","Train Epoch: 91 [44/107 (41%)]\tTrain Loss: 0.009701\n","Train Epoch: 91 [48/107 (45%)]\tTrain Loss: 0.000621\n","Train Epoch: 91 [52/107 (49%)]\tTrain Loss: 0.000249\n","Train Epoch: 91 [56/107 (52%)]\tTrain Loss: 0.000109\n","Train Epoch: 91 [60/107 (56%)]\tTrain Loss: 0.000043\n","Train Epoch: 91 [64/107 (60%)]\tTrain Loss: 0.002172\n","Train Epoch: 91 [68/107 (64%)]\tTrain Loss: 0.003758\n","Train Epoch: 91 [72/107 (67%)]\tTrain Loss: 0.044056\n","Train Epoch: 91 [76/107 (71%)]\tTrain Loss: 0.003085\n","Train Epoch: 91 [80/107 (75%)]\tTrain Loss: 0.007999\n","Train Epoch: 91 [84/107 (79%)]\tTrain Loss: 0.009823\n","Train Epoch: 91 [88/107 (82%)]\tTrain Loss: 0.000875\n","Train Epoch: 91 [92/107 (86%)]\tTrain Loss: 0.000150\n","Train Epoch: 91 [96/107 (90%)]\tTrain Loss: 0.000537\n","Train Epoch: 91 [100/107 (93%)]\tTrain Loss: 0.002511\n","Train Epoch: 91 [104/107 (97%)]\tTrain Loss: 0.000272\n","target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n","score [4.90910094e-03 9.41793680e-01 1.13968797e-01 8.06062460e-01\n"," 8.36136639e-02 2.22535983e-01 9.32869494e-01 3.97711933e-01\n"," 2.22739400e-05 1.09467946e-01 9.65077937e-01 1.83124412e-02\n"," 9.24296200e-01 9.30165291e-01 8.17954361e-01 1.96912489e-03\n"," 8.55193064e-02 1.93803199e-02 2.37799381e-04 2.06799924e-01\n"," 1.82209015e-01 9.95998859e-01 9.98564780e-01 9.99722660e-01\n"," 9.00644064e-01 9.96811092e-01 9.99829531e-01 2.19378769e-02\n"," 1.83936581e-03 4.46051185e-04 1.03584349e-01 2.34979615e-01\n"," 6.80686459e-02 1.41822022e-06 1.17915306e-06 4.47066050e-05\n"," 4.78622169e-05 9.91901696e-01 9.59668279e-01 4.98861913e-03\n"," 6.75692316e-03 3.86707075e-02 8.96594405e-01 8.05781126e-01\n"," 9.98214006e-01 9.28267121e-01 9.56600070e-01 4.70766574e-01\n"," 9.83677983e-01 9.61095870e-01 9.97946560e-01 1.45847014e-15\n"," 6.25008484e-03 1.33871650e-02 7.48852638e-27 1.70512812e-05\n"," 1.39561361e-02 1.45413427e-26 3.78838833e-13 4.52898741e-01\n"," 9.99814332e-01 9.99581993e-01 9.99964118e-01 9.99637723e-01\n"," 9.98821080e-01 9.98584986e-01 9.98555839e-01 9.99995828e-01\n"," 9.99935508e-01 9.97869492e-01 9.96290684e-01 9.93161917e-01\n"," 9.98895645e-01 9.95915592e-01 9.98589098e-01 9.98827994e-01\n"," 9.99333322e-01 9.99502659e-01 9.99962211e-01 9.98841226e-01\n"," 9.99977231e-01 9.99877572e-01 9.99955773e-01 9.95438755e-01\n"," 9.74754214e-01 9.99498963e-01 9.99818504e-01 9.98857260e-01\n"," 2.09725216e-01 9.98579621e-01 9.97815013e-01 9.97543991e-01\n"," 9.06216621e-01 9.99992728e-01 9.99963284e-01 9.99884129e-01\n"," 7.59064019e-01 6.97735429e-01 9.19508398e-01 9.97339427e-01\n"," 4.17666018e-01 9.96136129e-01 2.82313675e-01 7.62975097e-01\n"," 6.78559780e-01 9.79301453e-01 9.96531785e-01 9.64764404e-05\n"," 2.77990324e-07 9.11135658e-07 1.26039055e-07 2.25509275e-02\n"," 9.87484396e-01 9.95999455e-01 8.89839888e-01 9.27526474e-01\n"," 9.99981403e-01 9.99871016e-01]\n","predict [0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1.\n"," 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 1. 1. 1. 1. 0.\n"," 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 0. 1. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1.]\n","Train Epoch: 92 [0/107 (0%)]\tTrain Loss: 0.002186\n","Train Epoch: 92 [4/107 (4%)]\tTrain Loss: 0.000292\n","Train Epoch: 92 [8/107 (7%)]\tTrain Loss: 0.000983\n","Train Epoch: 92 [12/107 (11%)]\tTrain Loss: 0.001045\n","Train Epoch: 92 [16/107 (15%)]\tTrain Loss: 0.009609\n","Train Epoch: 92 [20/107 (19%)]\tTrain Loss: 0.004773\n","Train Epoch: 92 [24/107 (22%)]\tTrain Loss: 0.000383\n","Train Epoch: 92 [28/107 (26%)]\tTrain Loss: 0.000409\n","Train Epoch: 92 [32/107 (30%)]\tTrain Loss: 0.000600\n","Train Epoch: 92 [36/107 (34%)]\tTrain Loss: 0.000217\n","Train Epoch: 92 [40/107 (37%)]\tTrain Loss: 0.000397\n","Train Epoch: 92 [44/107 (41%)]\tTrain Loss: 0.000270\n","Train Epoch: 92 [48/107 (45%)]\tTrain Loss: 0.000190\n","Train Epoch: 92 [52/107 (49%)]\tTrain Loss: 0.006634\n","Train Epoch: 92 [56/107 (52%)]\tTrain Loss: 0.000218\n","Train Epoch: 92 [60/107 (56%)]\tTrain Loss: 0.000160\n","Train Epoch: 92 [64/107 (60%)]\tTrain Loss: 0.000093\n","Train Epoch: 92 [68/107 (64%)]\tTrain Loss: 0.002374\n","Train Epoch: 92 [72/107 (67%)]\tTrain Loss: 0.000046\n","Train Epoch: 92 [76/107 (71%)]\tTrain Loss: 0.001010\n","Train Epoch: 92 [80/107 (75%)]\tTrain Loss: 0.001333\n","Train Epoch: 92 [84/107 (79%)]\tTrain Loss: 0.001587\n","Train Epoch: 92 [88/107 (82%)]\tTrain Loss: 0.000736\n","Train Epoch: 92 [92/107 (86%)]\tTrain Loss: 0.000241\n","Train Epoch: 92 [96/107 (90%)]\tTrain Loss: 0.001077\n","Train Epoch: 92 [100/107 (93%)]\tTrain Loss: 0.000120\n","Train Epoch: 92 [104/107 (97%)]\tTrain Loss: 0.000009\n","target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n","score [4.56738577e-04 9.86518443e-01 5.87878466e-01 7.11154222e-01\n"," 3.12296420e-01 3.36155333e-02 9.15351808e-01 5.42015851e-01\n"," 4.87352236e-06 3.33629027e-02 7.22906590e-01 2.47985870e-02\n"," 3.15978885e-01 6.36028767e-01 4.04887825e-01 3.41479172e-04\n"," 1.33585818e-02 1.91277802e-01 5.75976563e-04 6.93949699e-01\n"," 2.40624279e-01 9.90294099e-01 9.96800780e-01 9.99403238e-01\n"," 8.78011882e-01 9.58299100e-01 9.99526024e-01 2.82253847e-02\n"," 1.26751780e-03 2.44855200e-05 1.03548162e-01 1.09048724e-01\n"," 1.50142759e-01 1.28895067e-06 1.97553277e-06 5.15443608e-05\n"," 2.77703912e-05 9.09190893e-01 9.94833231e-01 1.42424613e-01\n"," 3.89010757e-01 6.27272010e-01 3.91725898e-01 4.67265218e-01\n"," 9.98800278e-01 9.51907098e-01 9.54112649e-01 9.50771689e-01\n"," 9.75402653e-01 8.42623711e-01 9.98838127e-01 6.13901915e-11\n"," 2.82110414e-03 7.75888562e-02 1.59239092e-15 2.79652333e-04\n"," 5.81368923e-01 1.80231426e-18 1.54436214e-10 8.55489299e-02\n"," 9.99991059e-01 9.99936104e-01 9.99997258e-01 9.99996543e-01\n"," 9.99675512e-01 9.99465644e-01 9.99073863e-01 9.99999285e-01\n"," 9.99983549e-01 9.98892009e-01 9.99628067e-01 9.99532580e-01\n"," 9.99899507e-01 9.99840021e-01 9.99793231e-01 9.99124825e-01\n"," 9.99629259e-01 9.99796093e-01 9.99983907e-01 9.99772727e-01\n"," 9.99997377e-01 9.99974608e-01 9.99981642e-01 9.98456717e-01\n"," 9.97012615e-01 9.99981880e-01 9.99585450e-01 9.99774992e-01\n"," 1.25811473e-01 9.94623065e-01 9.90112484e-01 9.97966826e-01\n"," 9.78536069e-01 9.99992251e-01 9.99873519e-01 9.99731004e-01\n"," 2.85958976e-01 8.87622058e-01 9.44241583e-01 9.98135090e-01\n"," 5.29182553e-01 9.99748170e-01 7.98860550e-01 4.05588746e-01\n"," 2.92991936e-01 9.68226850e-01 9.98212337e-01 1.50938155e-04\n"," 8.99268457e-07 1.06388143e-05 2.29431862e-05 3.59476805e-01\n"," 9.49975550e-01 9.86243546e-01 7.14173615e-01 9.49695945e-01\n"," 9.99973774e-01 9.99962091e-01]\n","predict [0. 1. 1. 1. 0. 0. 1. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1.\n"," 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 0. 1. 1. 1. 1.\n"," 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1.\n"," 0. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1.]\n","Train Epoch: 93 [0/107 (0%)]\tTrain Loss: 0.017092\n","Train Epoch: 93 [4/107 (4%)]\tTrain Loss: 0.000368\n","Train Epoch: 93 [8/107 (7%)]\tTrain Loss: 0.000065\n","Train Epoch: 93 [12/107 (11%)]\tTrain Loss: 0.000220\n","Train Epoch: 93 [16/107 (15%)]\tTrain Loss: 0.000064\n","Train Epoch: 93 [20/107 (19%)]\tTrain Loss: 0.001436\n","Train Epoch: 93 [24/107 (22%)]\tTrain Loss: 0.004069\n","Train Epoch: 93 [28/107 (26%)]\tTrain Loss: 0.000397\n","Train Epoch: 93 [32/107 (30%)]\tTrain Loss: 0.000096\n","Train Epoch: 93 [36/107 (34%)]\tTrain Loss: 0.006450\n","Train Epoch: 93 [40/107 (37%)]\tTrain Loss: 0.000166\n","Train Epoch: 93 [44/107 (41%)]\tTrain Loss: 0.000169\n","Train Epoch: 93 [48/107 (45%)]\tTrain Loss: 0.000196\n","Train Epoch: 93 [52/107 (49%)]\tTrain Loss: 0.005609\n","Train Epoch: 93 [56/107 (52%)]\tTrain Loss: 0.001031\n","Train Epoch: 93 [60/107 (56%)]\tTrain Loss: 0.001168\n","Train Epoch: 93 [64/107 (60%)]\tTrain Loss: 0.006670\n","Train Epoch: 93 [68/107 (64%)]\tTrain Loss: 0.000687\n","Train Epoch: 93 [72/107 (67%)]\tTrain Loss: 0.001151\n","Train Epoch: 93 [76/107 (71%)]\tTrain Loss: 0.000728\n","Train Epoch: 93 [80/107 (75%)]\tTrain Loss: 0.000080\n","Train Epoch: 93 [84/107 (79%)]\tTrain Loss: 0.000554\n","Train Epoch: 93 [88/107 (82%)]\tTrain Loss: 0.000355\n","Train Epoch: 93 [92/107 (86%)]\tTrain Loss: 0.000105\n","Train Epoch: 93 [96/107 (90%)]\tTrain Loss: 0.000042\n","Train Epoch: 93 [100/107 (93%)]\tTrain Loss: 0.012369\n","Train Epoch: 93 [104/107 (97%)]\tTrain Loss: 0.000338\n","target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n","score [3.11228819e-03 9.90127206e-01 4.05140847e-01 9.95678484e-01\n"," 5.77019334e-01 1.12670518e-01 9.82606292e-01 9.96153772e-01\n"," 3.67039524e-04 1.92484319e-01 9.87010479e-01 7.09968656e-02\n"," 9.79019225e-01 8.05961907e-01 5.72276592e-01 2.69772881e-03\n"," 9.01261032e-01 5.27703822e-01 3.35614681e-02 8.90897691e-01\n"," 4.71588135e-01 9.99995470e-01 9.99330759e-01 9.99888182e-01\n"," 9.99973893e-01 9.97317851e-01 9.98873770e-01 1.81937646e-02\n"," 1.59728481e-03 3.18731181e-04 1.09083705e-01 9.55959111e-02\n"," 8.65078345e-02 2.29646685e-04 1.34106167e-05 8.66685587e-04\n"," 6.02923939e-03 9.99985456e-01 9.99858618e-01 2.08004355e-01\n"," 9.24129188e-01 2.20552579e-01 9.99536872e-01 3.11320037e-01\n"," 9.99414921e-01 7.83449769e-01 3.66133392e-01 2.89511263e-01\n"," 9.29396808e-01 7.39556015e-01 9.93309736e-01 1.61291580e-13\n"," 4.23914241e-03 3.10166623e-03 2.73415633e-19 4.91908577e-05\n"," 3.62103164e-01 2.44563778e-23 5.22419564e-12 1.05868384e-01\n"," 9.99975204e-01 9.99794662e-01 9.99991894e-01 9.99981403e-01\n"," 9.98286307e-01 9.98637497e-01 9.99001920e-01 9.99998093e-01\n"," 9.99945760e-01 9.98849630e-01 9.96751785e-01 9.93567884e-01\n"," 9.99237776e-01 9.99376595e-01 9.99569833e-01 9.98958826e-01\n"," 9.99669433e-01 9.99849916e-01 9.99944329e-01 9.97829854e-01\n"," 9.99960899e-01 9.99484777e-01 9.99911427e-01 9.99708593e-01\n"," 9.98281837e-01 9.99885559e-01 9.99017954e-01 9.99449670e-01\n"," 3.80335838e-01 9.98323977e-01 9.84806120e-01 9.99532700e-01\n"," 9.95933712e-01 9.99984860e-01 9.99663591e-01 9.99665737e-01\n"," 9.46934938e-01 9.99558151e-01 9.88554418e-01 9.99843955e-01\n"," 9.91308033e-01 9.99187291e-01 9.99698281e-01 9.11555350e-01\n"," 6.20636821e-01 9.64778244e-01 9.99209404e-01 5.82287647e-03\n"," 4.20009196e-02 5.45878538e-05 5.07789991e-05 9.99096751e-01\n"," 9.68032658e-01 9.99486566e-01 8.75219882e-01 9.98281598e-01\n"," 1.00000000e+00 9.99995947e-01]\n","predict [0. 1. 0. 1. 1. 0. 1. 1. 0. 0. 1. 0. 1. 1. 1. 0. 1. 1. 0. 1. 0. 1. 1. 1.\n"," 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0.\n"," 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1.]\n","Train Epoch: 94 [0/107 (0%)]\tTrain Loss: 0.000364\n","Train Epoch: 94 [4/107 (4%)]\tTrain Loss: 0.000230\n","Train Epoch: 94 [8/107 (7%)]\tTrain Loss: 0.000177\n","Train Epoch: 94 [12/107 (11%)]\tTrain Loss: 0.000165\n","Train Epoch: 94 [16/107 (15%)]\tTrain Loss: 0.000461\n","Train Epoch: 94 [20/107 (19%)]\tTrain Loss: 0.002000\n","Train Epoch: 94 [24/107 (22%)]\tTrain Loss: 0.011630\n","Train Epoch: 94 [28/107 (26%)]\tTrain Loss: 0.000349\n","Train Epoch: 94 [32/107 (30%)]\tTrain Loss: 0.010041\n","Train Epoch: 94 [36/107 (34%)]\tTrain Loss: 0.000491\n","Train Epoch: 94 [40/107 (37%)]\tTrain Loss: 0.000826\n","Train Epoch: 94 [44/107 (41%)]\tTrain Loss: 0.009990\n","Train Epoch: 94 [48/107 (45%)]\tTrain Loss: 0.000041\n","Train Epoch: 94 [52/107 (49%)]\tTrain Loss: 0.000603\n","Train Epoch: 94 [56/107 (52%)]\tTrain Loss: 0.000159\n","Train Epoch: 94 [60/107 (56%)]\tTrain Loss: 0.002407\n","Train Epoch: 94 [64/107 (60%)]\tTrain Loss: 0.000296\n","Train Epoch: 94 [68/107 (64%)]\tTrain Loss: 0.001340\n","Train Epoch: 94 [72/107 (67%)]\tTrain Loss: 0.000414\n","Train Epoch: 94 [76/107 (71%)]\tTrain Loss: 0.000200\n","Train Epoch: 94 [80/107 (75%)]\tTrain Loss: 0.002147\n","Train Epoch: 94 [84/107 (79%)]\tTrain Loss: 0.001013\n","Train Epoch: 94 [88/107 (82%)]\tTrain Loss: 0.004429\n","Train Epoch: 94 [92/107 (86%)]\tTrain Loss: 0.031523\n","Train Epoch: 94 [96/107 (90%)]\tTrain Loss: 0.000680\n","Train Epoch: 94 [100/107 (93%)]\tTrain Loss: 0.001655\n","Train Epoch: 94 [104/107 (97%)]\tTrain Loss: 0.001920\n","target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n","score [1.25987446e-02 9.85326648e-01 6.07142210e-01 7.23411858e-01\n"," 4.75804545e-02 1.77309290e-02 9.46599960e-01 3.83601636e-01\n"," 2.42743234e-04 9.95445728e-01 9.99825180e-01 8.69402766e-01\n"," 9.78669524e-01 3.15223285e-03 1.81737840e-01 9.80544215e-08\n"," 3.44161595e-06 2.54321098e-01 4.39152308e-03 8.32089663e-01\n"," 1.25948414e-01 9.96121109e-01 9.98335898e-01 9.99666810e-01\n"," 5.12230873e-01 6.36289597e-01 9.27538514e-01 4.08254942e-04\n"," 2.16644803e-05 1.20683626e-05 4.42494750e-02 3.16414027e-03\n"," 5.12535311e-02 3.84850762e-10 1.67691069e-10 7.62629782e-08\n"," 5.45272378e-08 6.88054383e-01 3.17174375e-01 3.97271492e-07\n"," 4.13976068e-07 1.52870361e-03 2.93388695e-01 2.25715302e-02\n"," 9.98016477e-01 6.07966781e-01 2.41245165e-01 1.57735586e-01\n"," 7.94872880e-01 2.44680241e-01 9.06900883e-01 2.48105735e-13\n"," 5.12762286e-04 3.77974648e-04 2.14716350e-17 3.71802125e-05\n"," 8.11424613e-01 1.57495057e-22 8.76881623e-11 9.93023720e-03\n"," 9.99997497e-01 9.99965191e-01 9.99999762e-01 9.99999285e-01\n"," 9.98914599e-01 9.99035239e-01 9.92944777e-01 1.00000000e+00\n"," 9.99968290e-01 9.98516262e-01 9.99185383e-01 9.97192562e-01\n"," 9.99497056e-01 9.99850273e-01 9.95202184e-01 9.99678373e-01\n"," 9.99687910e-01 9.99917984e-01 9.99796093e-01 9.99559343e-01\n"," 9.99971747e-01 9.98501778e-01 9.99886036e-01 9.99951959e-01\n"," 9.99933362e-01 9.99992967e-01 9.99868155e-01 9.99869466e-01\n"," 1.82221666e-01 8.09641004e-01 9.27672565e-01 9.98552024e-01\n"," 9.54076707e-01 9.99998331e-01 9.99714553e-01 9.99970675e-01\n"," 9.38145578e-01 9.47158754e-01 9.87997293e-01 9.38696742e-01\n"," 9.33804870e-01 9.99843240e-01 8.31377387e-01 7.92815745e-01\n"," 6.52112782e-01 9.56614852e-01 9.96116519e-01 9.78285389e-05\n"," 3.48925333e-10 1.85595272e-04 2.55324153e-06 3.74389766e-03\n"," 4.86389659e-02 9.99185860e-01 5.57804883e-01 8.80335867e-01\n"," 1.00000000e+00 1.00000000e+00]\n","predict [0. 1. 1. 1. 0. 0. 1. 0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1.\n"," 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0.\n"," 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1.]\n","Train Epoch: 95 [0/107 (0%)]\tTrain Loss: 0.011410\n","Train Epoch: 95 [4/107 (4%)]\tTrain Loss: 0.000500\n","Train Epoch: 95 [8/107 (7%)]\tTrain Loss: 0.002594\n","Train Epoch: 95 [12/107 (11%)]\tTrain Loss: 0.001913\n","Train Epoch: 95 [16/107 (15%)]\tTrain Loss: 0.000684\n","Train Epoch: 95 [20/107 (19%)]\tTrain Loss: 0.000146\n","Train Epoch: 95 [24/107 (22%)]\tTrain Loss: 0.002132\n","Train Epoch: 95 [28/107 (26%)]\tTrain Loss: 0.000061\n","Train Epoch: 95 [32/107 (30%)]\tTrain Loss: 0.000484\n","Train Epoch: 95 [36/107 (34%)]\tTrain Loss: 0.000235\n","Train Epoch: 95 [40/107 (37%)]\tTrain Loss: 0.001090\n","Train Epoch: 95 [44/107 (41%)]\tTrain Loss: 0.000386\n","Train Epoch: 95 [48/107 (45%)]\tTrain Loss: 0.000067\n","Train Epoch: 95 [52/107 (49%)]\tTrain Loss: 0.017131\n","Train Epoch: 95 [56/107 (52%)]\tTrain Loss: 0.002123\n","Train Epoch: 95 [60/107 (56%)]\tTrain Loss: 0.000269\n","Train Epoch: 95 [64/107 (60%)]\tTrain Loss: 0.000028\n","Train Epoch: 95 [68/107 (64%)]\tTrain Loss: 0.000573\n","Train Epoch: 95 [72/107 (67%)]\tTrain Loss: 0.008076\n","Train Epoch: 95 [76/107 (71%)]\tTrain Loss: 0.000072\n","Train Epoch: 95 [80/107 (75%)]\tTrain Loss: 0.000326\n","Train Epoch: 95 [84/107 (79%)]\tTrain Loss: 0.001592\n","Train Epoch: 95 [88/107 (82%)]\tTrain Loss: 0.000123\n","Train Epoch: 95 [92/107 (86%)]\tTrain Loss: 0.045669\n","Train Epoch: 95 [96/107 (90%)]\tTrain Loss: 0.045340\n","Train Epoch: 95 [100/107 (93%)]\tTrain Loss: 0.000610\n","Train Epoch: 95 [104/107 (97%)]\tTrain Loss: 0.000137\n","target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n","score [1.72975268e-02 9.84701693e-01 6.26814842e-01 7.85756409e-01\n"," 1.92874312e-01 4.84648496e-02 9.68879938e-01 4.78008300e-01\n"," 1.89045662e-04 6.94306970e-01 8.21622312e-01 1.74915716e-01\n"," 1.70851305e-01 9.83108222e-01 9.37991500e-01 5.21026122e-05\n"," 8.00978960e-05 3.98747236e-01 3.95610550e-04 2.33445063e-01\n"," 1.47487327e-01 9.94709849e-01 9.97710586e-01 9.99939322e-01\n"," 8.73165190e-01 9.86152768e-01 9.98702645e-01 5.04407994e-02\n"," 3.27123748e-03 4.60324052e-04 2.17142612e-01 6.84424341e-02\n"," 3.20819318e-01 1.01351600e-06 5.64104141e-07 2.37415894e-04\n"," 2.34085787e-03 9.90333378e-01 9.93743837e-01 2.93788430e-03\n"," 3.22621944e-03 3.92450653e-02 7.86385298e-01 4.56142187e-01\n"," 9.80741322e-01 4.61943269e-01 5.88862896e-01 4.08158422e-01\n"," 8.54298592e-01 6.68129921e-01 9.91918147e-01 1.03457367e-14\n"," 1.83694030e-03 2.80491629e-04 1.51229644e-19 7.56645250e-06\n"," 4.07579213e-01 8.40819465e-24 2.64561203e-11 1.04318194e-01\n"," 9.99805272e-01 9.98773873e-01 9.99966860e-01 9.99981165e-01\n"," 9.97707844e-01 9.97888625e-01 9.97713566e-01 9.99999523e-01\n"," 9.99857545e-01 9.97383058e-01 9.96853054e-01 9.80112016e-01\n"," 9.98463273e-01 9.99454200e-01 9.95391130e-01 9.96786356e-01\n"," 9.99019146e-01 9.99007881e-01 9.99833822e-01 9.99689579e-01\n"," 9.99970555e-01 9.99599755e-01 9.99832988e-01 9.99719203e-01\n"," 9.98253763e-01 9.99949455e-01 9.98950183e-01 9.99597847e-01\n"," 4.33844209e-01 9.96884167e-01 9.98555124e-01 9.89769697e-01\n"," 9.59048331e-01 9.99985933e-01 9.99916077e-01 9.99690056e-01\n"," 3.68998796e-01 9.87320542e-01 7.03257799e-01 9.73818302e-01\n"," 9.90488529e-01 9.97292578e-01 4.70735878e-01 8.45048606e-01\n"," 6.60570025e-01 9.93791878e-01 9.98841703e-01 5.68045536e-03\n"," 9.24383343e-08 4.70258296e-03 1.47454368e-04 6.12788051e-02\n"," 9.70123112e-01 9.98027861e-01 9.42649305e-01 9.81177092e-01\n"," 9.99993563e-01 9.99951243e-01]\n","predict [0. 1. 1. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1.\n"," 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 1. 0. 1. 0.\n"," 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1.\n"," 0. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1.]\n","Train Epoch: 96 [0/107 (0%)]\tTrain Loss: 0.000060\n","Train Epoch: 96 [4/107 (4%)]\tTrain Loss: 0.000594\n","Train Epoch: 96 [8/107 (7%)]\tTrain Loss: 0.000701\n","Train Epoch: 96 [12/107 (11%)]\tTrain Loss: 0.000266\n","Train Epoch: 96 [16/107 (15%)]\tTrain Loss: 0.000870\n","Train Epoch: 96 [20/107 (19%)]\tTrain Loss: 0.001087\n","Train Epoch: 96 [24/107 (22%)]\tTrain Loss: 0.000075\n","Train Epoch: 96 [28/107 (26%)]\tTrain Loss: 0.001169\n","Train Epoch: 96 [32/107 (30%)]\tTrain Loss: 0.000780\n","Train Epoch: 96 [36/107 (34%)]\tTrain Loss: 0.000747\n","Train Epoch: 96 [40/107 (37%)]\tTrain Loss: 0.000095\n","Train Epoch: 96 [44/107 (41%)]\tTrain Loss: 0.001466\n","Train Epoch: 96 [48/107 (45%)]\tTrain Loss: 0.000290\n","Train Epoch: 96 [52/107 (49%)]\tTrain Loss: 0.019890\n","Train Epoch: 96 [56/107 (52%)]\tTrain Loss: 0.000373\n","Train Epoch: 96 [60/107 (56%)]\tTrain Loss: 0.000791\n","Train Epoch: 96 [64/107 (60%)]\tTrain Loss: 0.000807\n","Train Epoch: 96 [68/107 (64%)]\tTrain Loss: 0.001829\n","Train Epoch: 96 [72/107 (67%)]\tTrain Loss: 0.000076\n","Train Epoch: 96 [76/107 (71%)]\tTrain Loss: 0.000019\n","Train Epoch: 96 [80/107 (75%)]\tTrain Loss: 0.000036\n","Train Epoch: 96 [84/107 (79%)]\tTrain Loss: 0.000183\n","Train Epoch: 96 [88/107 (82%)]\tTrain Loss: 0.000235\n","Train Epoch: 96 [92/107 (86%)]\tTrain Loss: 0.000278\n","Train Epoch: 96 [96/107 (90%)]\tTrain Loss: 0.000555\n","Train Epoch: 96 [100/107 (93%)]\tTrain Loss: 0.000363\n","Train Epoch: 96 [104/107 (97%)]\tTrain Loss: 0.000684\n","target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n","score [7.36792805e-03 9.86476004e-01 5.21552205e-01 9.73993182e-01\n"," 4.79804054e-02 8.43765959e-03 9.94854808e-01 6.05417669e-01\n"," 1.03265559e-03 5.82763851e-01 9.95935678e-01 1.34035096e-01\n"," 9.68299389e-01 9.95205700e-01 9.91953790e-01 1.03907241e-05\n"," 1.78390663e-04 3.94466400e-01 3.08298832e-03 8.69046807e-01\n"," 9.71813202e-01 9.99669909e-01 9.98903394e-01 9.99985695e-01\n"," 9.87813354e-01 9.98650849e-01 9.99195755e-01 4.01075155e-01\n"," 1.66827667e-04 2.83425761e-05 1.23140328e-01 7.25384150e-03\n"," 1.61632180e-01 7.88151056e-08 7.85929331e-08 8.91695163e-05\n"," 3.83480545e-03 9.86309528e-01 9.68399763e-01 1.64135476e-03\n"," 1.08917442e-03 1.06436759e-02 7.91006267e-01 6.84794962e-01\n"," 9.99305964e-01 2.36317832e-02 2.13946681e-02 1.08612906e-02\n"," 3.40198427e-01 5.31727493e-01 8.96160901e-01 1.55504225e-07\n"," 7.54030235e-03 2.04640185e-03 2.22077237e-08 1.35656223e-02\n"," 9.15181935e-01 5.26390702e-11 6.06068625e-06 2.60465056e-01\n"," 9.99662519e-01 9.99726593e-01 9.99987721e-01 9.99966264e-01\n"," 9.99874711e-01 9.99783456e-01 9.99303579e-01 9.99995947e-01\n"," 9.99940276e-01 9.68237817e-01 9.63347316e-01 6.47702515e-01\n"," 9.99597132e-01 9.99317765e-01 9.99541998e-01 9.99897599e-01\n"," 9.99179900e-01 9.98452544e-01 9.99933958e-01 9.99247849e-01\n"," 9.99926329e-01 9.99593556e-01 9.99631643e-01 9.98170137e-01\n"," 9.91163492e-01 9.95476544e-01 9.99704182e-01 9.99536157e-01\n"," 9.23072100e-01 9.98950839e-01 9.99325514e-01 9.90353703e-01\n"," 9.83173430e-01 9.99997973e-01 9.99973416e-01 9.99953747e-01\n"," 9.92758036e-01 9.93197560e-01 9.50306237e-01 9.98581409e-01\n"," 9.92249012e-01 9.97847080e-01 8.87951732e-01 1.90774083e-01\n"," 3.05620015e-01 9.36623633e-01 9.86558378e-01 2.68102653e-04\n"," 5.30967895e-07 2.80238106e-03 1.62635988e-04 1.54017052e-02\n"," 6.02736771e-01 9.98844743e-01 9.90551949e-01 9.96848881e-01\n"," 9.99999881e-01 9.99996901e-01]\n","predict [0. 1. 1. 1. 0. 0. 1. 1. 0. 1. 1. 0. 1. 1. 1. 0. 0. 0. 0. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 0. 0.\n"," 0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1.]\n","Train Epoch: 97 [0/107 (0%)]\tTrain Loss: 0.000521\n","Train Epoch: 97 [4/107 (4%)]\tTrain Loss: 0.000137\n","Train Epoch: 97 [8/107 (7%)]\tTrain Loss: 0.000215\n","Train Epoch: 97 [12/107 (11%)]\tTrain Loss: 0.001259\n","Train Epoch: 97 [16/107 (15%)]\tTrain Loss: 0.000007\n","Train Epoch: 97 [20/107 (19%)]\tTrain Loss: 0.000243\n","Train Epoch: 97 [24/107 (22%)]\tTrain Loss: 0.001168\n","Train Epoch: 97 [28/107 (26%)]\tTrain Loss: 0.008466\n","Train Epoch: 97 [32/107 (30%)]\tTrain Loss: 0.000192\n","Train Epoch: 97 [36/107 (34%)]\tTrain Loss: 0.000078\n","Train Epoch: 97 [40/107 (37%)]\tTrain Loss: 0.000653\n","Train Epoch: 97 [44/107 (41%)]\tTrain Loss: 0.000117\n","Train Epoch: 97 [48/107 (45%)]\tTrain Loss: 0.000981\n","Train Epoch: 97 [52/107 (49%)]\tTrain Loss: 0.001670\n","Train Epoch: 97 [56/107 (52%)]\tTrain Loss: 0.014178\n","Train Epoch: 97 [60/107 (56%)]\tTrain Loss: 0.000254\n","Train Epoch: 97 [64/107 (60%)]\tTrain Loss: 0.004627\n","Train Epoch: 97 [68/107 (64%)]\tTrain Loss: 0.000259\n","Train Epoch: 97 [72/107 (67%)]\tTrain Loss: 0.000981\n","Train Epoch: 97 [76/107 (71%)]\tTrain Loss: 0.001247\n","Train Epoch: 97 [80/107 (75%)]\tTrain Loss: 0.000343\n","Train Epoch: 97 [84/107 (79%)]\tTrain Loss: 0.001551\n","Train Epoch: 97 [88/107 (82%)]\tTrain Loss: 0.001064\n","Train Epoch: 97 [92/107 (86%)]\tTrain Loss: 0.000687\n","Train Epoch: 97 [96/107 (90%)]\tTrain Loss: 0.000282\n","Train Epoch: 97 [100/107 (93%)]\tTrain Loss: 0.000146\n","Train Epoch: 97 [104/107 (97%)]\tTrain Loss: 0.001824\n","target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n","score [3.63681954e-03 9.61056411e-01 2.98264980e-01 9.01737690e-01\n"," 2.74731964e-02 1.44725619e-02 9.86268818e-01 5.71585536e-01\n"," 1.98564352e-03 7.11191952e-01 9.93471503e-01 3.35047066e-01\n"," 9.18937683e-01 9.82142746e-01 9.37696457e-01 4.69198676e-05\n"," 4.75970708e-04 4.98940110e-01 9.62039456e-04 2.67253757e-01\n"," 8.39474559e-01 9.93269026e-01 9.84508514e-01 9.99879122e-01\n"," 9.30804431e-01 9.94704068e-01 9.99852538e-01 3.20428163e-01\n"," 1.51836384e-05 3.45237549e-06 3.93983394e-01 2.12799152e-03\n"," 1.68722391e-01 1.68700225e-07 2.74153848e-07 6.26354522e-05\n"," 1.40627509e-03 9.85955060e-01 9.49149728e-01 3.10173095e-03\n"," 1.00121088e-03 3.39514092e-02 7.77008712e-01 7.62059867e-01\n"," 9.83503103e-01 3.87823507e-02 1.19845189e-01 5.30596599e-02\n"," 4.02688265e-01 7.90583074e-01 9.25742269e-01 3.92370993e-07\n"," 5.20267449e-02 4.17441246e-04 7.26064854e-07 1.45930387e-02\n"," 8.70697677e-01 4.61392147e-10 1.15314988e-05 5.10866404e-01\n"," 9.98989642e-01 9.99062836e-01 9.99966145e-01 9.99933124e-01\n"," 9.98228848e-01 9.97713327e-01 9.96240616e-01 9.99963760e-01\n"," 9.98078942e-01 9.88095164e-01 9.81617510e-01 7.32403636e-01\n"," 9.92872357e-01 9.96492922e-01 9.96763349e-01 9.99509692e-01\n"," 9.96598899e-01 9.97626364e-01 9.99748528e-01 9.92829144e-01\n"," 9.99483705e-01 9.98152554e-01 9.98897791e-01 9.98451710e-01\n"," 9.66781914e-01 9.88214374e-01 9.98769224e-01 9.98779595e-01\n"," 6.48908913e-01 9.97620523e-01 9.98673797e-01 9.82497633e-01\n"," 8.06734324e-01 9.99942660e-01 9.99842525e-01 9.99525189e-01\n"," 8.28909099e-01 9.97586489e-01 6.06338978e-01 9.98567343e-01\n"," 9.79696393e-01 9.97187316e-01 6.69736624e-01 5.85873306e-01\n"," 5.08451104e-01 9.75240052e-01 9.93998170e-01 8.05766496e-04\n"," 9.50129575e-08 7.74525804e-04 1.06415348e-04 1.93061113e-01\n"," 2.11811900e-01 9.75935638e-01 9.82176006e-01 9.89374518e-01\n"," 9.99999762e-01 9.99990344e-01]\n","predict [0. 1. 0. 1. 0. 0. 1. 1. 0. 1. 1. 0. 1. 1. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1.\n"," 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 0. 0.\n"," 0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1.]\n","Train Epoch: 98 [0/107 (0%)]\tTrain Loss: 0.001288\n","Train Epoch: 98 [4/107 (4%)]\tTrain Loss: 0.000018\n","Train Epoch: 98 [8/107 (7%)]\tTrain Loss: 0.000276\n","Train Epoch: 98 [12/107 (11%)]\tTrain Loss: 0.000659\n","Train Epoch: 98 [16/107 (15%)]\tTrain Loss: 0.007751\n","Train Epoch: 98 [20/107 (19%)]\tTrain Loss: 0.000075\n","Train Epoch: 98 [24/107 (22%)]\tTrain Loss: 0.000336\n","Train Epoch: 98 [28/107 (26%)]\tTrain Loss: 0.000909\n","Train Epoch: 98 [32/107 (30%)]\tTrain Loss: 0.000632\n","Train Epoch: 98 [36/107 (34%)]\tTrain Loss: 0.000878\n","Train Epoch: 98 [40/107 (37%)]\tTrain Loss: 0.001039\n","Train Epoch: 98 [44/107 (41%)]\tTrain Loss: 0.003180\n","Train Epoch: 98 [48/107 (45%)]\tTrain Loss: 0.000055\n","Train Epoch: 98 [52/107 (49%)]\tTrain Loss: 0.000806\n","Train Epoch: 98 [56/107 (52%)]\tTrain Loss: 0.000042\n","Train Epoch: 98 [60/107 (56%)]\tTrain Loss: 0.000106\n","Train Epoch: 98 [64/107 (60%)]\tTrain Loss: 0.000176\n","Train Epoch: 98 [68/107 (64%)]\tTrain Loss: 0.000097\n","Train Epoch: 98 [72/107 (67%)]\tTrain Loss: 0.149822\n","Train Epoch: 98 [76/107 (71%)]\tTrain Loss: 0.000034\n","Train Epoch: 98 [80/107 (75%)]\tTrain Loss: 0.002841\n","Train Epoch: 98 [84/107 (79%)]\tTrain Loss: 0.000019\n","Train Epoch: 98 [88/107 (82%)]\tTrain Loss: 0.000005\n","Train Epoch: 98 [92/107 (86%)]\tTrain Loss: 0.254562\n","Train Epoch: 98 [96/107 (90%)]\tTrain Loss: 0.003951\n","Train Epoch: 98 [100/107 (93%)]\tTrain Loss: 0.001504\n","Train Epoch: 98 [104/107 (97%)]\tTrain Loss: 0.000756\n","target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n","score [2.22229864e-02 9.90472078e-01 1.01449557e-01 7.83900738e-01\n"," 2.33465410e-03 4.30634730e-02 2.82311201e-01 8.75384957e-02\n"," 7.20869750e-04 9.91382539e-01 9.98318553e-01 5.24883687e-01\n"," 8.59595358e-01 1.58523351e-01 9.51339118e-03 9.35406277e-08\n"," 2.85891438e-05 6.12948279e-05 3.41994223e-07 1.47516176e-03\n"," 9.10350445e-06 9.57311541e-02 5.54728985e-01 9.99960423e-01\n"," 2.34943517e-02 8.56713176e-01 9.99861598e-01 1.05002599e-07\n"," 2.63201141e-08 8.07679923e-10 5.35435902e-05 4.27509331e-05\n"," 2.05046423e-02 5.69581671e-09 2.89800672e-09 6.12732975e-08\n"," 2.11089542e-07 8.17054927e-01 8.35425686e-03 4.39304308e-08\n"," 2.28360602e-08 4.03196827e-05 1.39379408e-02 9.29500093e-05\n"," 1.41216267e-03 1.95205864e-02 7.98569918e-02 3.94640863e-02\n"," 3.05313855e-01 2.34489217e-01 9.48126495e-01 2.08367948e-10\n"," 1.26017210e-06 2.63800484e-05 8.85227225e-10 5.84155543e-07\n"," 8.81324187e-02 1.94239512e-14 1.24074413e-08 1.08354709e-06\n"," 9.99989867e-01 9.99676943e-01 9.99999523e-01 9.99999046e-01\n"," 1.08479557e-03 2.80045066e-02 8.07597637e-01 9.99979496e-01\n"," 9.58959222e-01 9.97433007e-01 9.92583990e-01 9.93952215e-01\n"," 3.86297315e-01 9.84287977e-01 6.04004115e-02 4.08938676e-01\n"," 9.98441517e-01 9.99849558e-01 9.91756678e-01 9.85638082e-01\n"," 9.99759138e-01 5.95731795e-01 9.99818504e-01 9.98595893e-01\n"," 9.85981345e-01 9.99863386e-01 8.03210378e-01 1.02450028e-01\n"," 1.43407308e-03 9.36913371e-01 9.98870552e-01 8.20538849e-02\n"," 2.22611859e-01 9.99985695e-01 9.99967098e-01 9.99992967e-01\n"," 1.50862455e-01 9.97783363e-01 2.44495831e-02 7.85782278e-01\n"," 9.79802787e-01 9.97675717e-01 3.16818595e-01 1.82265744e-01\n"," 8.37921798e-01 4.91776317e-01 9.99943018e-01 5.86682124e-13\n"," 1.90250689e-24 1.17753303e-15 5.51388515e-19 4.13944790e-08\n"," 2.68778640e-19 6.83737993e-02 1.79638252e-01 7.78258592e-02\n"," 1.00000000e+00 9.99993920e-01]\n","predict [0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.\n"," 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1.\n"," 0. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 0. 0. 1. 1. 1.\n"," 0. 1. 0. 1. 1. 1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.]\n","Train Epoch: 99 [0/107 (0%)]\tTrain Loss: 0.000870\n","Train Epoch: 99 [4/107 (4%)]\tTrain Loss: 0.000422\n","Train Epoch: 99 [8/107 (7%)]\tTrain Loss: 0.002597\n","Train Epoch: 99 [12/107 (11%)]\tTrain Loss: 0.008948\n","Train Epoch: 99 [16/107 (15%)]\tTrain Loss: 0.032336\n","Train Epoch: 99 [20/107 (19%)]\tTrain Loss: 0.117623\n","Train Epoch: 99 [24/107 (22%)]\tTrain Loss: 0.019628\n","Train Epoch: 99 [28/107 (26%)]\tTrain Loss: 0.000420\n","Train Epoch: 99 [32/107 (30%)]\tTrain Loss: 0.023367\n","Train Epoch: 99 [36/107 (34%)]\tTrain Loss: 0.000239\n","Train Epoch: 99 [40/107 (37%)]\tTrain Loss: 0.001547\n","Train Epoch: 99 [44/107 (41%)]\tTrain Loss: 0.000370\n","Train Epoch: 99 [48/107 (45%)]\tTrain Loss: 0.001334\n","Train Epoch: 99 [52/107 (49%)]\tTrain Loss: 0.003904\n","Train Epoch: 99 [56/107 (52%)]\tTrain Loss: 0.081918\n","Train Epoch: 99 [60/107 (56%)]\tTrain Loss: 0.001630\n","Train Epoch: 99 [64/107 (60%)]\tTrain Loss: 0.001743\n","Train Epoch: 99 [68/107 (64%)]\tTrain Loss: 0.000433\n","Train Epoch: 99 [72/107 (67%)]\tTrain Loss: 0.000810\n","Train Epoch: 99 [76/107 (71%)]\tTrain Loss: 0.001932\n","Train Epoch: 99 [80/107 (75%)]\tTrain Loss: 0.000586\n","Train Epoch: 99 [84/107 (79%)]\tTrain Loss: 0.135458\n","Train Epoch: 99 [88/107 (82%)]\tTrain Loss: 0.001074\n","Train Epoch: 99 [92/107 (86%)]\tTrain Loss: 0.000827\n","Train Epoch: 99 [96/107 (90%)]\tTrain Loss: 0.004930\n","Train Epoch: 99 [100/107 (93%)]\tTrain Loss: 0.001502\n","Train Epoch: 99 [104/107 (97%)]\tTrain Loss: 0.007491\n","target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n","score [1.66408481e-05 8.64677370e-01 3.32779378e-01 9.99991059e-01\n"," 4.63679701e-01 4.21910809e-04 9.67722178e-01 9.99988914e-01\n"," 1.04520535e-02 1.04789054e-02 8.89542580e-01 2.15232670e-02\n"," 9.83256340e-01 1.06512848e-03 9.88141060e-01 1.97670019e-07\n"," 9.19018872e-04 9.13528919e-01 5.79665788e-02 8.06700205e-04\n"," 5.04492037e-03 9.59767520e-01 9.55554128e-01 9.97422695e-01\n"," 3.90184969e-01 9.34426486e-01 9.99927640e-01 1.92546904e-05\n"," 4.56417365e-05 5.39526009e-05 3.52659612e-04 5.09918493e-04\n"," 2.30890624e-02 1.29359168e-09 1.28004400e-12 3.53825236e-09\n"," 3.21704829e-05 9.85314846e-01 8.15053238e-04 1.85596374e-08\n"," 4.04790024e-09 1.63785174e-07 3.41219395e-01 2.52336287e-03\n"," 9.95975077e-01 3.94360395e-05 6.09449930e-02 1.08923381e-02\n"," 1.55921113e-02 5.24232686e-01 3.22988600e-01 3.12624441e-04\n"," 7.96563691e-05 3.03588982e-04 6.60825794e-09 5.34776211e-01\n"," 9.91354406e-01 6.41382780e-10 4.03039099e-04 4.16959883e-05\n"," 9.90542471e-01 9.92773235e-01 9.99550045e-01 9.99851704e-01\n"," 7.83578455e-01 9.99965072e-01 9.99995232e-01 9.99994397e-01\n"," 9.93532658e-01 9.97975647e-01 3.62678915e-01 5.14982082e-02\n"," 9.87531841e-01 9.99965549e-01 7.09443465e-02 9.83542442e-01\n"," 9.77850854e-01 9.40212846e-01 9.99044478e-01 9.97281790e-01\n"," 9.96665418e-01 7.57803857e-01 9.96351361e-01 9.99909639e-01\n"," 9.96990323e-01 9.71359253e-01 7.85972536e-01 6.63119793e-01\n"," 4.31615174e-01 9.78379846e-01 9.99076366e-01 9.99998331e-01\n"," 9.99806702e-01 9.99963045e-01 9.99808609e-01 9.99971747e-01\n"," 9.57761943e-01 1.17671281e-01 1.07524693e-01 9.86559391e-01\n"," 1.02472892e-02 9.98080611e-01 8.55984926e-01 6.21848047e-01\n"," 1.85929298e-01 9.74324644e-01 9.89060163e-01 1.42800212e-02\n"," 5.90957236e-04 9.88623127e-04 4.68803555e-01 5.23557127e-01\n"," 9.99985456e-01 9.99787390e-01 8.64659011e-01 9.05166328e-01\n"," 1.00000000e+00 1.00000000e+00]\n","predict [0. 1. 0. 1. 0. 0. 1. 1. 0. 0. 1. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 1. 1. 1.\n"," 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n"," 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0.\n"," 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 0. 0. 1. 0. 1. 1. 1. 0. 1. 1. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1.]\n","Train Epoch: 100 [0/107 (0%)]\tTrain Loss: 0.001059\n","Train Epoch: 100 [4/107 (4%)]\tTrain Loss: 0.000329\n","Train Epoch: 100 [8/107 (7%)]\tTrain Loss: 0.000076\n","Train Epoch: 100 [12/107 (11%)]\tTrain Loss: 0.001737\n","Train Epoch: 100 [16/107 (15%)]\tTrain Loss: 0.000138\n","Train Epoch: 100 [20/107 (19%)]\tTrain Loss: 0.089463\n","Train Epoch: 100 [24/107 (22%)]\tTrain Loss: 0.001041\n","Train Epoch: 100 [28/107 (26%)]\tTrain Loss: 0.000772\n","Train Epoch: 100 [32/107 (30%)]\tTrain Loss: 0.010570\n","Train Epoch: 100 [36/107 (34%)]\tTrain Loss: 0.020877\n","Train Epoch: 100 [40/107 (37%)]\tTrain Loss: 0.001172\n","Train Epoch: 100 [44/107 (41%)]\tTrain Loss: 0.001180\n","Train Epoch: 100 [48/107 (45%)]\tTrain Loss: 0.040534\n","Train Epoch: 100 [52/107 (49%)]\tTrain Loss: 0.020641\n","Train Epoch: 100 [56/107 (52%)]\tTrain Loss: 0.044110\n","Train Epoch: 100 [60/107 (56%)]\tTrain Loss: 0.000195\n","Train Epoch: 100 [64/107 (60%)]\tTrain Loss: 0.112721\n","Train Epoch: 100 [68/107 (64%)]\tTrain Loss: 0.039035\n","Train Epoch: 100 [72/107 (67%)]\tTrain Loss: 0.003832\n","Train Epoch: 100 [76/107 (71%)]\tTrain Loss: 0.001036\n","Train Epoch: 100 [80/107 (75%)]\tTrain Loss: 0.005249\n","Train Epoch: 100 [84/107 (79%)]\tTrain Loss: 0.032206\n","Train Epoch: 100 [88/107 (82%)]\tTrain Loss: 0.000303\n","Train Epoch: 100 [92/107 (86%)]\tTrain Loss: 0.001153\n","Train Epoch: 100 [96/107 (90%)]\tTrain Loss: 0.054740\n","Train Epoch: 100 [100/107 (93%)]\tTrain Loss: 0.001441\n","Train Epoch: 100 [104/107 (97%)]\tTrain Loss: 0.130386\n","target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n","score [6.45585265e-03 9.99287188e-01 9.38578665e-01 9.72431540e-01\n"," 4.07272950e-02 7.20741926e-04 9.98548090e-01 8.52868497e-01\n"," 1.44807971e-04 6.56881690e-01 7.83917427e-01 4.32507068e-01\n"," 7.67402709e-01 9.81069684e-01 9.89237249e-01 9.13456827e-03\n"," 4.52292711e-03 9.36977327e-01 3.77392257e-03 3.88991237e-02\n"," 1.10355029e-02 9.97451484e-01 8.93787026e-01 9.97191608e-01\n"," 9.82657731e-01 9.65712667e-01 9.99392033e-01 8.22749571e-04\n"," 7.68272381e-04 1.48608979e-05 2.43818872e-02 1.53133899e-01\n"," 3.58239800e-01 6.54264877e-05 5.33642997e-05 1.64671429e-02\n"," 1.78793103e-01 9.96733665e-01 9.36568677e-01 1.42980918e-01\n"," 1.55210495e-01 3.71531934e-01 9.55126762e-01 7.44457245e-01\n"," 8.65112722e-01 4.27403778e-01 5.15216649e-01 7.03722239e-01\n"," 7.88844228e-01 7.44710922e-01 9.84520078e-01 2.29803959e-06\n"," 1.30994551e-04 7.92920007e-04 1.51654529e-08 1.05896709e-03\n"," 9.92706954e-01 3.78324889e-12 2.30400031e-03 2.03305579e-04\n"," 9.99937415e-01 9.99487400e-01 9.99981642e-01 9.99918461e-01\n"," 1.19561829e-01 9.89381075e-01 9.78726625e-01 9.99959826e-01\n"," 9.78157341e-01 9.99018788e-01 9.96453404e-01 9.92950320e-01\n"," 9.99438465e-01 9.99982476e-01 8.64091218e-01 9.60947216e-01\n"," 9.99706447e-01 9.99592006e-01 9.99845266e-01 9.98217642e-01\n"," 9.98911142e-01 9.74888444e-01 9.98262227e-01 9.99938726e-01\n"," 9.98187840e-01 9.97969449e-01 9.98917103e-01 9.98751163e-01\n"," 1.91685129e-02 9.47855592e-01 9.56452727e-01 9.99404788e-01\n"," 9.67907012e-01 9.90641415e-01 9.94578362e-01 9.77681041e-01\n"," 6.72205836e-02 9.97355461e-01 8.63008618e-01 9.82513070e-01\n"," 9.87354457e-01 9.99987125e-01 9.99943972e-01 6.84481919e-01\n"," 1.93871409e-01 8.94320846e-01 9.23006117e-01 2.51889676e-01\n"," 7.23213032e-02 2.02651750e-02 2.19924778e-01 9.93787646e-01\n"," 2.16095001e-01 9.56793785e-01 9.93058443e-01 9.97258306e-01\n"," 1.00000000e+00 1.00000000e+00]\n","predict [0. 1. 1. 1. 0. 0. 1. 1. 0. 1. 1. 0. 1. 1. 1. 0. 0. 1. 0. 0. 0. 1. 1. 1.\n"," 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 1. 1.\n"," 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1.\n"," 0. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 1. 1. 1. 1. 1.]\n","vote_pred [0. 1. 0. 1. 0. 0. 1. 1. 0. 1. 1. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1.\n"," 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0.\n"," 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1.]\n","targetlist [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n","TP= 52 TN= 37 FN= 6 FP= 23\n","TP+FP 75\n","precision 0.6933333333333334\n","recall 0.896551724137931\n","F1 0.7819548872180451\n","acc 0.7542372881355932\n","AUCp 0.7566091954022989\n","AUC 0.8454022988505748\n","\n"," The epoch is 100, average recall: 0.8966, average precision: 0.6933,average F1: 0.7820, average accuracy: 0.7542, average AUC: 0.8454\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"X2UC-oMXHBav","colab_type":"code","outputId":"5f984ffc-5073-419a-8633-a8c959015f2d","executionInfo":{"status":"ok","timestamp":1592194080939,"user_tz":420,"elapsed":97341,"user":{"displayName":"HELEN HUANG","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgPwfdcR39k2-MsYXu8TmO-R41dE5DWynTaQ89_=s64","userId":"15309384205789813260"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["# test\n","bs = 10\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","epoch = 1\n","r_list = []\n","p_list = []\n","acc_list = []\n","AUC_list = []\n","# TP = 0\n","# TN = 0\n","# FN = 0\n","# FP = 0\n","vote_pred = np.zeros(testset.__len__())\n","vote_score = np.zeros(testset.__len__())\n","\n","\n","targetlist, scorelist, predlist = test(epoch)\n","print('target',targetlist)\n","print('score',scorelist)\n","print('predict',predlist)\n","vote_pred = vote_pred + predlist \n","vote_score = vote_score + scorelist \n","\n","TP = ((predlist == 1) & (targetlist == 1)).sum()\n","\n","TN = ((predlist == 0) & (targetlist == 0)).sum()\n","FN = ((predlist == 0) & (targetlist == 1)).sum()\n","FP = ((predlist == 1) & (targetlist == 0)).sum()\n","\n","print('TP=',TP,'TN=',TN,'FN=',FN,'FP=',FP)\n","print('TP+FP',TP+FP)\n","p = TP / (TP + FP)\n","print('precision',p)\n","p = TP / (TP + FP)\n","r = TP / (TP + FN)\n","print('recall',r)\n","F1 = 2 * r * p / (r + p)\n","acc = (TP + TN) / (TP + TN + FP + FN)\n","print('F1',F1)\n","print('acc',acc)\n","AUC = roc_auc_score(targetlist, vote_score)\n","print('AUC', AUC)\n","\n","# f = open(f'model_result/medical_transfer/test_{modelname}_{alpha_name}_LUNA_moco_CT_moco.txt', 'a+')\n","# f.write('\\n The epoch is {}, average recall: {:.4f}, average precision: {:.4f},\\\n","# average F1: {:.4f}, average accuracy: {:.4f}, average AUC: {:.4f}'.format(\n","# epoch, r, p, F1, acc, AUC))\n","# f.close()\n","# torch.save(model.state_dict(), \"model_backup/medical_transfer/{}_{}_covid_moco_covid.pt\".format(modelname,alpha_name))\n"],"execution_count":11,"outputs":[{"output_type":"stream","text":["target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n","score [1.03023287e-03 9.99982238e-01 8.58332554e-04 1.10003851e-01\n"," 8.56930128e-05 6.64671461e-05 2.01913848e-04 9.88670707e-01\n"," 1.35146677e-02 2.94235088e-02 9.66134965e-01 6.28570855e-01\n"," 3.01560009e-04 9.83061135e-01 9.72241342e-01 9.94482875e-01\n"," 9.08752629e-09 2.90310215e-02 1.96311221e-01 8.11822154e-03\n"," 1.05120484e-02 1.13069024e-02 2.88408026e-02 1.17322919e-03\n"," 2.45850049e-02 2.33325735e-02 5.61494045e-02 7.03587294e-01\n"," 1.49265900e-01 8.10889959e-01 8.62737149e-02 9.08358097e-01\n"," 9.99160528e-01 8.82145204e-03 7.95725675e-04 5.42042295e-23\n"," 2.53990687e-18 7.39059806e-01 2.98062354e-01 7.89107662e-03\n"," 1.39437173e-03 2.61314344e-02 3.26542221e-02 2.31757597e-03\n"," 1.82763543e-02 3.66546819e-03 9.26752448e-01 6.06066227e-01\n"," 5.12404561e-01 4.61191423e-02 3.69889021e-01 2.24201620e-01\n"," 6.50072753e-01 2.93797761e-01 3.85249481e-02 1.51168201e-02\n"," 1.82052795e-03 4.74835560e-03 9.92062747e-01 9.59260225e-01\n"," 4.23687481e-04 9.80472982e-01 4.29370105e-02 1.66651294e-01\n"," 3.60479653e-02 5.64933980e-05 4.65183366e-05 3.10190080e-05\n"," 1.04562035e-02 9.99693990e-01 2.36326516e-01 1.47254527e-01\n"," 1.29198539e-04 1.85040431e-03 1.65563810e-03 4.87945508e-04\n"," 7.48498738e-01 9.92511272e-01 1.08674087e-01 5.87648094e-01\n"," 9.12180126e-01 2.55965412e-01 7.74091780e-01 8.99104476e-01\n"," 4.18937393e-02 1.28708750e-01 9.82173681e-01 1.24362305e-01\n"," 1.93460464e-01 1.99448728e-24 1.91631305e-28 9.88606036e-01\n"," 9.99596417e-01 9.11192060e-01 9.94370162e-01 2.05141738e-01\n"," 9.99879956e-01 9.93026018e-01 9.99812067e-01 9.97095704e-01\n"," 9.92989540e-01 9.99985337e-01 9.99945164e-01 6.87565804e-01\n"," 9.97239709e-01 9.99973774e-01 9.99922514e-01 9.56659675e-01\n"," 9.98884261e-01 9.97509718e-01 9.98621106e-01 9.99273360e-01\n"," 9.99429524e-01 9.99072671e-01 9.99953985e-01 9.99623060e-01\n"," 9.99990940e-01 9.99997735e-01 9.99997258e-01 9.99998689e-01\n"," 9.99995589e-01 9.99998808e-01 9.99995470e-01 9.99998808e-01\n"," 9.99997854e-01 9.99977112e-01 9.97982383e-01 9.99306440e-01\n"," 9.99654651e-01 9.99300361e-01 9.99416709e-01 9.99574006e-01\n"," 9.99462903e-01 9.99909878e-01 9.99976039e-01 9.99537826e-01\n"," 9.99558389e-01 9.99982595e-01 9.99987006e-01 9.99984384e-01\n"," 9.99759495e-01 9.99781191e-01 9.84287858e-01 9.99372780e-01\n"," 9.99786556e-01 7.04890966e-01 9.99951839e-01 7.72932172e-01\n"," 7.19267428e-01 9.97844577e-01 9.97546494e-01 9.99848247e-01\n"," 9.99952197e-01 9.96515274e-01 9.97663617e-01 9.99994516e-01\n"," 9.99985218e-01 2.07275315e-03 9.65281725e-01 5.10960221e-01\n"," 6.89551962e-05 9.97221589e-01 9.98570323e-01 9.99990344e-01\n"," 1.34476915e-01 9.99559224e-01 8.96107912e-01 1.83501631e-01\n"," 9.99629259e-01 1.82097927e-02 2.37803743e-03 7.36260489e-02\n"," 9.99149442e-01 9.99332607e-01 6.71145260e-01 9.97448862e-01\n"," 9.93490100e-01 9.99919057e-01 9.82694089e-01 9.69802678e-01\n"," 8.82431030e-01 3.92622277e-02 6.77262666e-03 9.43320453e-01\n"," 9.99669909e-01 9.90052998e-01 9.99481976e-01 9.99723494e-01\n"," 2.92551164e-02 4.51611519e-01 5.56134760e-01 8.12356830e-01\n"," 9.92330253e-01 9.18799877e-01 8.87440741e-01 9.49306011e-01\n"," 9.62741017e-01 9.74707007e-01 9.08130944e-01 1.03447817e-01\n"," 9.98235464e-01 6.91549718e-01 9.94307220e-01]\n","predict [0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.\n"," 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.\n"," 0. 0. 0. 0. 1. 1. 0. 1. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0. 0. 1. 1. 1. 1. 0.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 0. 1. 1. 0.\n"," 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 0. 0. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1.]\n","TP= 93 TN= 65 FN= 12 FP= 33\n","TP+FP 126\n","precision 0.7380952380952381\n","recall 0.8857142857142857\n","F1 0.8051948051948051\n","acc 0.7783251231527094\n","AUC 0.8622934888241011\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Nkqfq8kiHBax","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"GSelWhf-HBaz","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}